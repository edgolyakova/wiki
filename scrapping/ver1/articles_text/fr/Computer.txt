



Ordinateur â€” WikipÃ©dia


























Aller au contenu





Afficherâ€¯/â€¯masquer la barre latÃ©rale












Rechercher















CrÃ©er un compte





Outils personnels


 CrÃ©er un compte
 Se connecter


		Pages pour les contributeurs dÃ©connectÃ©s en savoir plus


DiscussionContributions










Navigation


AccueilPortails thÃ©matiquesArticle au hasardContact




Contribuer


DÃ©buter sur WikipÃ©diaAideCommunautÃ©Modifications rÃ©centesFaire un don




Outils


Pages liÃ©esSuivi des pages liÃ©esTÃ©lÃ©verser un fichierPages spÃ©cialesLien permanentInformations sur la pageCiter cette pageÃ‰lÃ©ment Wikidata




Imprimerâ€¯/â€¯exporter


CrÃ©er un livreTÃ©lÃ©charger comme PDFVersion imprimable




Dans dâ€™autres projets


Wikimedia CommonsWikiquote




Langues

Sur cette version linguistique de WikipÃ©dia, les liens interlangues sont placÃ©s en haut Ã  droite du titre de lâ€™article.Aller en haut.















				Sommaire
				dÃ©placer vers la barre latÃ©rale
masquer





DÃ©but





1Ã‰tymologie







2Histoire


					Afficherâ€¯/â€¯masquer la sous-section Histoire
				




2.1PremiÃ¨re apparition de l'ordinateur







2.2Concept initial et rÃ©alisation







2.3Calculatrices







2.4Ã‰lectromÃ©canique et mÃ©canographie







2.5AnnÃ©es 1930









3Ã‰volution


					Afficherâ€¯/â€¯masquer la sous-section Ã‰volution
				




3.1Premier ordinateur (1937-1946)







3.2Tubes Ã  vide et commutateurs (1946-1955)







3.3GÃ©nÃ©rations suivantes (1955-2000)







3.4GÃ©nÃ©ralitÃ©s









4Fonctionnement


					Afficherâ€¯/â€¯masquer la sous-section Fonctionnement
				




4.1UAL et UC







4.2MÃ©moire







4.3EntrÃ©es-Sorties







4.4Bus







4.5Architecture







4.6Instructions







4.7Logiciels









5Types d'ordinateurs


					Afficherâ€¯/â€¯masquer la sous-section Types d'ordinateurs
				




5.1Par type de phÃ©nomÃ¨ne physique





5.1.1MÃ©canique







5.1.2Ã‰lectromÃ©canique







5.1.3Ã‰lectronique







5.1.4Quantique







5.1.5Optique







5.1.6Chimique (ou biologique)









5.2Par type de traitement temporel







5.3Par domaine d'application







5.4Par taille







5.5Par architecture









6Notes et rÃ©fÃ©rences







7Voir aussi


					Afficherâ€¯/â€¯masquer la sous-section Voir aussi
				




7.1Bibliographie







7.2Articles connexes







7.3Liens externes














						Basculer la table des matiÃ¨res
					
Ordinateur



227Â langues


ĞÔ¥ÑÑˆÓ™Ğ°AcÃ¨hAfrikaansAlemannischáŠ áˆ›áˆ­áŠ›AragonÃ©sÃ†ngliscØ§Ù„Ø¹Ø±Ø¨ÙŠØ©ÜÜªÜ¡ÜÜØ§Ù„Ø¯Ø§Ø±Ø¬Ø©Ù…ØµØ±Ù‰à¦…à¦¸à¦®à§€à¦¯à¦¼à¦¾AsturianuAzÉ™rbaycancaØªÛ†Ø±Ú©Ø¬Ù‡Ğ‘Ğ°ÑˆÒ¡Ğ¾Ñ€Ñ‚ÑĞ°BoarischÅ½emaitÄ—Å¡kaBikol CentralĞ‘ĞµĞ»Ğ°Ñ€ÑƒÑĞºĞ°ÑĞ‘ĞµĞ»Ğ°Ñ€ÑƒÑĞºĞ°Ñ (Ñ‚Ğ°Ñ€Ğ°ÑˆĞºĞµĞ²Ñ–Ñ†Ğ°)Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸à¤­à¥‹à¤œà¤ªà¥à¤°à¥€Bislamaá€•á€¡á€­á€¯á€á€ºá‚á€˜á€¬á‚á€á€¬á‚à¦¬à¦¾à¦‚à¦²à¦¾à½–à½¼à½‘à¼‹à½¡à½²à½‚à¦¬à¦¿à¦·à§à¦£à§à¦ªà§à¦°à¦¿à¦¯à¦¼à¦¾ à¦®à¦£à¦¿à¦ªà§à¦°à§€BrezhonegBosanskiá¨…á¨” á¨•á¨˜á¨á¨—Ğ‘ÑƒÑ€ÑĞ°Ğ´CatalÃ MÃ¬ng-dÄ•Ì¤ng-ngá¹³Ì„ĞĞ¾Ñ…Ñ‡Ğ¸Ğ¹Ğ½CebuanoTsetsÃªhestÃ¢heseÚ©ÙˆØ±Ø¯ÛŒQÄ±rÄ±mtatarcaÄŒeÅ¡tinaKaszÃ«bscziĞ¡Ğ»Ğ¾Ğ²Ñ£Ğ½ÑŒÑĞºÑŠ / â°”â°â°‘â°‚â°¡â°â° â°”â°â°ŸĞ§Ó‘Ğ²Ğ°ÑˆĞ»Ğ°CymraegDanskDeutschThuÉ”Å‹jÃ¤Å‹Zazakià¤¡à¥‹à¤Ÿà¥‡à¤²à¥€Î•Î»Î»Î·Î½Î¹ÎºÎ¬EmiliÃ n e rumagnÃ²lEnglishEsperantoEspaÃ±olEestiEuskaraEstremeÃ±uÙØ§Ø±Ø³ÛŒSuomiVÃµroNa Vosa VakavitiFÃ¸roysktNordfriiskFurlanGaeilgeè´›èªKriyÃ²l gwiyannenGÃ idhligGalegoAvaÃ±e'áº½ğŒ²ğŒ¿ğ„ğŒ¹ğƒğŒºàª—à«àªœàª°àª¾àª¤à«€GaelgHausaå®¢å®¶èª/Hak-kÃ¢-ngÃ®×¢×‘×¨×™×ªà¤¹à¤¿à¤¨à¥à¤¦à¥€Fiji HindiHrvatskiKreyÃ²l ayisyenMagyarÕ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶Ô±Ö€Õ¥Ö‚Õ´Õ¿Õ¡Õ°Õ¡ÕµÕ¥Ö€Õ§Õ¶InterlinguaBahasa IndonesiaInterlingueIgboIÃ±upiatunIlokanoIdoÃslenskaItalianoáƒá“„á’ƒá‘á‘á‘¦/inuktitutæ—¥æœ¬èªPatoisLa .lojban.Jawaáƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜QaraqalpaqshaKabÉ©yÉ›KongoGÄ©kÅ©yÅ©ÒšĞ°Ğ·Ğ°Ò›ÑˆĞ°á—á¶áŸá¶ááŸ’á˜áŸ‚ášà²•à²¨à³à²¨à²¡í•œêµ­ì–´ĞšÑŠĞ°Ñ€Ğ°Ñ‡Ğ°Ğ¹-Ğ¼Ğ°Ğ»ĞºÑŠĞ°Ñ€à¤•à¥‰à¤¶à¥à¤° / Ú©Ù²Ø´ÙØ±KurdÃ®ĞšĞ¾Ğ¼Ğ¸KernowekĞšÑ‹Ñ€Ğ³Ñ‹Ğ·Ñ‡Ğ°LatinaLadinoLÃ«tzebuergeschĞ›ĞµĞ·Ğ³Ğ¸Lingua Franca NovaLimburgsLigureLombardLingÃ¡laàº¥àº²àº§LietuviÅ³LatvieÅ¡uà¤®à¥ˆà¤¥à¤¿à¤²à¥€Basa BanyumasanMalagasyĞĞ»Ñ‹Ğº Ğ¼Ğ°Ñ€Ğ¸Ğ¹MinangkabauĞœĞ°ĞºĞµĞ´Ğ¾Ğ½ÑĞºĞ¸à´®à´²à´¯à´¾à´³à´‚ĞœĞ¾Ğ½Ğ³Ğ¾Ğ»á€˜á€¬á€á€¬ á€™á€”á€ºà¤®à¤°à¤¾à¤ à¥€Bahasa MelayuMaltiMirandÃ©sá€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬Ğ­Ñ€Ğ·ÑĞ½ÑŒÙ…Ø§Ø²ÙØ±ÙˆÙ†ÛŒNÄhuatlNapulitanoPlattdÃ¼Ã¼tschNedersaksiesà¤¨à¥‡à¤ªà¤¾à¤²à¥€à¤¨à¥‡à¤ªà¤¾à¤² à¤­à¤¾à¤·à¤¾NederlandsNorsk nynorskNorsk bokmÃ¥lß’ßßOccitanLivvinkarjalaOromooà¬“à¬¡à¬¼à¬¿à¬†Ğ˜Ñ€Ğ¾Ğ½à¨ªà©°à¨œà¨¾à¨¬à©€KapampanganPolskiPiemontÃ¨isÙ¾Ù†Ø¬Ø§Ø¨ÛŒÎ Î¿Î½Ï„Î¹Î±ÎºÎ¬Ù¾ÚšØªÙˆPortuguÃªsRuna SimiRomani ÄhibRomÃ¢nÄƒĞ ÑƒÑÑĞºĞ¸Ğ¹Ğ ÑƒÑĞ¸Ğ½ÑŒÑĞºÑ‹Ğ¹à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤®à¥Ğ¡Ğ°Ñ…Ğ° Ñ‚Ñ‹Ğ»Ğ°á±¥á±Ÿá±±á±›á±Ÿá±²á±¤SarduSicilianuScotsØ³Ù†ÚŒÙŠSrpskohrvatski / ÑÑ€Ğ¿ÑĞºĞ¾Ñ…Ñ€Ğ²Ğ°Ñ‚ÑĞºĞ¸Taclá¸¥ità·ƒà·’à¶‚à·„à¶½Simple EnglishSlovenÄinaØ³Ø±Ø§Ø¦ÛŒÚ©ÛŒSlovenÅ¡ÄinaChiShonaSoomaaligaShqipĞ¡Ñ€Ğ¿ÑĞºĞ¸ / srpskiSesothoSeelterskSundaSvenskaKiswahiliÅšlÅ¯nskiSakizayaà®¤à®®à®¿à®´à¯Tayalà²¤à³à²³à³à°¤à±†à°²à±à°—à±Ğ¢Ğ¾Ò·Ğ¸ĞºÓ£à¹„à¸—à¸¢TÃ¼rkmenÃ§eTagalogTÃ¼rkÃ§eXitsongaĞ¢Ğ°Ñ‚Ğ°Ñ€Ñ‡Ğ°/tatarÃ§aØ¦Û‡ÙŠØºÛ‡Ø±Ú†Û• / UyghurcheĞ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°Ø§Ø±Ø¯ÙˆOÊ»zbekcha/ÑĞ·Ğ±ĞµĞºÑ‡Ğ°VÃ¨netoVepsÃ¤n kelâ€™Tiáº¿ng Viá»‡tWest-VlamsVolapÃ¼kWalonWinarayWolofå´è¯­IsiXhosaáƒ›áƒáƒ áƒ’áƒáƒšáƒ£áƒ áƒ˜×™×™Ö´×“×™×©YorÃ¹bÃ¡Vahcuenghä¸­æ–‡æ–‡è¨€BÃ¢n-lÃ¢m-gÃºç²µèªIsiZulu
Modifier les liens









ArticleDiscussion





franÃ§ais











LireVoir le texte sourceVoir lâ€™historique







Plus


LireVoir le texte sourceVoir lâ€™historique











Un article de WikipÃ©dia, l'encyclopÃ©die libre.





Ne doit pas Ãªtre confondu avec Appareil informatique ou SystÃ¨me de traitement de l'information.


Un ordinateur est un systÃ¨me de traitement de l'information programmable tel que dÃ©fini par Alan Turing et qui fonctionne par la lecture sÃ©quentielle d'un ensemble d'instructions, organisÃ©es en programmes, qui lui font exÃ©cuter des opÃ©rations logiques et arithmÃ©tiques. Sa structure physique actuelle fait que toutes les opÃ©rations reposent sur la logique binaire et sur des nombres formÃ©s Ã  partir de chiffres binaires. DÃ¨s sa mise sous tension, un ordinateur exÃ©cute, l'une aprÃ¨s l'autre, des instructions qui lui font lire, manipuler, puis rÃ©Ã©crire un ensemble de donnÃ©es dÃ©terminÃ©es par une mÃ©moire morte d'amorÃ§age. Des tests et des sauts conditionnels permettent de passer Ã  l'instruction suivante et donc d'agir diffÃ©remment en fonction des donnÃ©es ou des nÃ©cessitÃ©s du moment ou de l'environnement.
Les donnÃ©es Ã  manipuler sont acquises soit par la lecture de mÃ©moires, soit en provenance de pÃ©riphÃ©riques internes ou externes (dÃ©placement d'une souris, touche appuyÃ©e sur un clavier, dÃ©placement d'un stylet sur une tablette, tempÃ©rature et autres mesures physiquesâ€¦). Une fois utilisÃ©s, ou manipulÃ©s, les rÃ©sultats sont Ã©crits soit dans des mÃ©moires, soit dans des composants qui peuvent transformer une valeur binaire en une action physique (Ã©criture sur une imprimante ou sur un moniteur, accÃ©lÃ©ration ou freinage d'un vÃ©hicule, changement de tempÃ©rature d'un fourâ€¦). L'ordinateur peut aussi rÃ©pondre Ã  des interruptions qui lui permettent dâ€™exÃ©cuter des programmes de rÃ©ponses spÃ©cifiques Ã  chacune, puis de reprendre lâ€™exÃ©cution sÃ©quentielle du programme interrompu.
De 1834 Ã  1837, Charles Babbage conÃ§oit une machine Ã  calculer programmable en associant un des descendants de la Pascaline (premiÃ¨re machine Ã  calculer mÃ©canique inventÃ©e par Blaise Pascal) avec des instructions Ã©crites sur le mÃªme type de cartes perforÃ©es que celles inventÃ©es par Joseph Marie Jacquard pour ses mÃ©tiers Ã  tisser[1]. C'est durant cette pÃ©riode qu'il imagine la plupart des caractÃ©ristiques de l'ordinateur moderne[2]. Babbage passe le reste de sa vie Ã  essayer de construire sa machine analytique, mais sans succÃ¨s. Nombre de personnes essayent de dÃ©velopper cette machine[3], mais c'est cent ans plus tard, en 1937, qu'IBM inaugure l'Ã¨re de l'informatique en commenÃ§ant le dÃ©veloppement de l'ASCC/Mark I, une machine construite sur l'architecture de Babbage qui, une fois rÃ©alisÃ©e, est considÃ©rÃ©e comme l'achÃ¨vement de son rÃªve[4].
La technique actuelle des ordinateurs date du milieu du xxeÂ siÃ¨cle. Les ordinateurs peuvent Ãªtre classÃ©s selon plusieurs critÃ¨res tels que le domaine d'application, la taille ou l'architecture.




DiffÃ©rents modÃ¨les d'ordinateurs(cliquez pour agrandir).




Ã‰tymologie
Le mot Â«Â ordinateurÂ Â» fut introduit par IBM France en 1955[5],[6] aprÃ¨s que FranÃ§ois Girard, alors responsable du service publicitÃ© de l'entreprise, eut l'idÃ©e de consulter son ancien professeur de lettres Ã  Paris, Jacques Perret. Avec Christian de Waldner, alors prÃ©sident d'IBM France, ils demandÃ¨rent au professeur Perret, de suggÃ©rer un Â«Â nom franÃ§ais pour sa nouvelle machine Ã©lectronique destinÃ©e au traitement de l'information (IBM 650), en Ã©vitant d'utiliser la traduction littÃ©rale du mot anglais computer (Â«Â calculateurÂ Â» ou Â«Â calculatriceÂ Â»), qui Ã©tait Ã  cette Ã©poque plutÃ´t rÃ©servÃ© aux machines scientifiquesÂ Â»[7].
En 1911, une description de la machine analytique de Babbage utilisait le mot ordonnateur pour en dÃ©crire son organe moteur: Â«Â Pour aller prendre et reporter les nombresâ€¦ et pour les soumettre Ã  lâ€™opÃ©ration demandÃ©e, il faut qu'il y ait dans la machine un organe spÃ©cial et variableÂ : c'est l'ordonnateur. Cet ordonnateur est constituÃ© simplement par des feuilles de carton ajourÃ©es, analogues Ã  celle des mÃ©tiers Jacquardâ€¦Â Â»[8].
Le professeur proposa un mot composÃ© centrÃ© autour d'ordonnateurÂ : celui qui met en ordre[9] et qui avait aussi la notion d'ordre ecclÃ©siastique dans l'Ã©glise catholique (ordinant)[10]. Il suggÃ©ra plus prÃ©cisÃ©ment Â«Â ordinatrice Ã©lectroniqueÂ Â», le fÃ©minin ayant pu permettre, selon lui, de mieux distinguer l'usage religieux de l'usage comptable du mot[11].
Â«Â IBM France retint le mot ordinateur et chercha au dÃ©but Ã  protÃ©ger ce nom comme une marque. Mais le mot fut facilement et rapidement adoptÃ© par les utilisateurs et IBM France dÃ©cida au bout de quelques mois de le laisser dans le domaine public.Â Â»[7]

Histoire
 L'image la plus cÃ©lÃ¨bre du dÃ©but de l'histoire de l'informatique[12].Ce portrait de Jacquard, tissÃ© en soie sur un mÃ©tier Jacquard, demandait la lecture de 24Â 000 cartes de plus de 1Â 000 trous chacune (1839). Il n'Ã©tait produit que sur demande. Charles Babbage l'utilisa souvent pour expliquer ses idÃ©es sur ce que fut la premiÃ¨re esquisse d'un ordinateur, sa machine analytique, qui utilisait des cartes Jacquard pour ses commandes et ses donnÃ©es[13].
PremiÃ¨re apparition de l'ordinateur
Article dÃ©taillÃ©Â : Histoire des ordinateurs.
Selon Bernard Cohen, auteur de l'ouvrage intitulÃ© Howard Aiken: Portrait of a computer pioneer, Â«Â les historiens des technologies et les informaticiens intÃ©ressÃ©s en histoire, ont adoptÃ© un certain nombre de caractÃ©ristiques qui dÃ©finissent un ordinateur. C'est ainsi que la question de savoir si le Mark I Ã©tait ou n'Ã©tait pas un ordinateur ne dÃ©pend pas d'une opinion majoritaire mais plutÃ´t de la dÃ©finition utilisÃ©e. Souvent, quelques-unes des caractÃ©ristiques fondamentales nÃ©cessaires pour Ãªtre considÃ©rÃ©es comme un ordinateur sontÂ :

Qu'il soit Ã©lectroniqueÂ ;
NumÃ©rique (au lieu d'analogique)Â ;
Qu'il soit programmableÂ ;
Qu'il puisse exÃ©cuter les quatre opÃ©rations Ã©lÃ©mentaires (addition, soustraction, multiplication, division) et â€” souvent â€” qu'il puisse extraire une racine carrÃ©e ou adresser une table qui en contientÂ ;
Qu'il puisse exÃ©cuter des programmes enregistrÃ©s en mÃ©moire.
Une machine n'est gÃ©nÃ©ralement pas classifiÃ©e comme un ordinateur Ã  moins qu'elle n'ait des caractÃ©ristiques supplÃ©mentaires comme la possibilitÃ© dâ€™exÃ©cuter des opÃ©rations spÃ©cifiques automatiquement et ceci d'une faÃ§on contrÃ´lÃ©e et dans une sÃ©quence prÃ©dÃ©terminÃ©e. Pour d'autres historiens et informaticiens, il faut aussi que la machine ait Ã©tÃ© vraiment construite et qu'elle ait Ã©tÃ© complÃ¨tement opÃ©rationnelle[14].Â Â»

Concept initial et rÃ©alisation
Sans une dÃ©finition stricte il est impossible d'identifier la machine qui devint le premier ordinateur, mais il faut remarquer certaines des Ã©tapes fondamentales qui vont du dÃ©veloppement du concept de la machine Ã  calculer programmable par Charles Babbage en 1837 au premier dÃ©veloppement de l'Ã¨re de l'informatique cent ans plus tard.
En 1834, Charles Babbage commence Ã  dÃ©velopper une machine Ã  calculer programmable, sa machine analytique. Il pense la programmer grÃ¢ce Ã  un cylindre Ã  picots comme dans les automates de Vaucanson, mais, deux ans plus tard, il remplace ce cylindre par la lecture de cartes Jacquard, et ainsi crÃ©e une machine Ã  calculer infiniment programmable[15].
En 1843, Ada Lovelace Ã©crit le premier programme informatique pour calculer les nombres de Bernoulli, pour la machine analytique qui ne sera jamais construite.
Henry Babbage construit une version extrÃªmement simplifiÃ©e de l'unitÃ© centrale de la Â«Â machine analytiqueÂ Â» de son pÃ¨re et l'utilise en 1906, pour calculer et imprimer automatiquement les quarante premiers multiples du nombre Pi avec une prÃ©cision de vingt-neuf dÃ©cimales[16], dÃ©montrant sans ambiguÃ¯tÃ© que le principe de la machine analytique Ã©tait viable et rÃ©alisable. En 1886, sa plus grande contribution fut de donner un ensemble mÃ©canique de dÃ©monstration d'une des machines de son pÃ¨re Ã  l'universitÃ© Harvard[17]. C'est cinquante ans plus tard, aprÃ¨s avoir entendu la prÃ©sentation de Howard Aiken sur son super calculateur, qu'un technicien de Harvard, Carmello Lanza, lui fit savoir qu'une machine similaire avait dÃ©jÃ  Ã©tÃ© dÃ©veloppÃ©e et qu'il lui montra l'ensemble mÃ©canique de dÃ©monstration donnÃ© par Henry Babbage qui se trouvait dans un des greniers de l'universitÃ©Â ; c'est ainsi qu'il dÃ©couvrit les travaux de Babbage et qu'il les incorpora dans la machine qu'il prÃ©senta Ã  IBM en 1937[18]. C'Ã©tait la troisiÃ¨me fois qu'il essayait de trouver un sponsor pour le dÃ©veloppement de sa machine car son projet avait dÃ©jÃ  Ã©tÃ© rejetÃ© deux fois avant l'intÃ©gration des travaux de Babbage dans l'architecture de sa machine (une fois par la Monroe Calculating Company[19] et une fois par l'universitÃ© Harvard[18]).
Leonardo Torres Quevedo remplaÃ§a toutes les fonctions mÃ©caniques de Babbage par des fonctions Ã©lectromÃ©caniques (addition, soustraction, multiplication et division mais aussi la lecture de cartes et les mÃ©moires). En 1914 et en 1920, Il construisit deux machines analytiques, non programmable, extrÃªmement simplifiÃ©es[20] mais qui montraient que des relais Ã©lectromÃ©caniques pouvaient Ãªtre utilisÃ©s dans une machine Ã  calculer qu'elle soit programmable ou non. Sa machine de 1914 avait une petite mÃ©moire Ã©lectromÃ©canique et son arithmomÃ¨tre de 1920, qu'il dÃ©veloppa pour cÃ©lÃ©brer le centiÃ¨me anniversaire de l'invention de l'arithmomÃ¨tre, Ã©tait commandÃ© par une machine Ã  Ã©crire qui Ã©tait aussi utilisÃ©e pour imprimer ses rÃ©sultats.
Percy Ludgate amÃ©liora et simplifia les fonctions mÃ©caniques de Babbage mais ne construisit pas de machine. Et enfin, Louis Couffignal essaya au dÃ©but des annÃ©es 1930[21], de construire une machine analytique Â«Â purement mÃ©canique, comme celle de Babbage, mais sensiblement plus simpleÂ Â», mais sans succÃ¨s. C'est cent ans aprÃ¨s la conceptualisation de l'ordinateur par Charles Babbage que le premier projet basÃ© sur l'architecture de sa machine analytique aboutira. En effet, c'est en 1937 qu'Howard Aiken prÃ©senta Ã  IBM un projet de machine Ã  calculer programmable qui sera le premier projet qui finira par une machine qui puisse Ãªtre, et qui sera utilisÃ©e, et dont les caractÃ©ristiques en font presque un ordinateur moderne. Et donc, bien que le premier ordinateur ne sera jamais dÃ©terminÃ© Ã  lâ€™unanimitÃ©, le dÃ©but de l'Ã¨re de l'informatique moderne peut Ãªtre considÃ©rÃ© comme la prÃ©sentation d'Aiken Ã  IBM, en 1937, qui aboutira par l'ASCC.

Calculatrices
Les machines Ã  calculer jouÃ¨rent un rÃ´le primordial dans le dÃ©veloppement des ordinateurs pour deux raisons tout Ã  fait indÃ©pendantes. D'une part, pour leurs originesÂ : c'est pendant le dÃ©veloppement d'une machine Ã  calculer automatique Ã  imprimante qu'en 1834 Charles Babbage commenÃ§a Ã  imaginer sa machine analytique, lâ€™ancÃªtre des ordinateurs. Câ€™Ã©tait une machine Ã  calculer programmÃ©e par la lecture de cartes perforÃ©es (inspirÃ©es du MÃ©tier Jacquard), avec un lecteur de cartes pour les donnÃ©es et un pour les programmes, avec des mÃ©moires, un calculateur central et des imprimantes et qui inspirera le dÃ©veloppement des premiers ordinateurs Ã  partir de 1937Â ; ce qui nous amÃ¨nera aux mainframes des annÃ©es 1960.
D'autre part, leur propagation se fit grÃ¢ce Ã  la commercialisation en 1971 du premier microprocesseur, l'Intel 4004, qui fut inventÃ© pendant le dÃ©veloppement d'une machine Ã  calculer Ã©lectronique pour la compagnie japonaise Busicom, qui est Ã  l'origine de l'explosion de la micro-informatique Ã  partir de 1975[22] et qui rÃ©side au cÅ“ur de tous les ordinateurs actuels quelles que soient leurs tailles ou fonctions (bien que seulement 2Â % des microprocesseurs produits chaque annÃ©e soient utilisÃ©s comme unitÃ©s centrales d'ordinateur, les 98Â % restant sont utilisÃ©s dans la construction de voitures, de robots mÃ©nagers, de montres, de camÃ©ras de surveillance[23]â€¦).

Ã‰lectromÃ©canique et mÃ©canographie
Outre les avancÃ©es observÃ©es dans l'industrie du textile et celles de l'Ã©lectronique, les avancÃ©es de la mÃ©canographie Ã  la fin du XIXeÂ siÃ¨cle, pour achever les recensements aux Ã‰tats-Unis, la mÃ©canisation de la cryptographie au dÃ©but du XXeÂ siÃ¨cle, pour chiffrer puis dÃ©chiffrer automatiquement des messages, le dÃ©veloppement des rÃ©seaux tÃ©lÃ©phoniques (Ã  base de relais Ã©lectromÃ©caniques), sont aussi Ã  prendre en compte pour comprendre l'avÃ¨nement de ce nouveau genre de machine qui ne calculent pas (comme font/faisaient les calculatrices), mais lisent et interprÃ¨tent des programmes qui -eux- calculent. Pour le monde des idÃ©es, avant l'invention de ces nouvelles machines, l'Ã©lÃ©ment fondateur de la science informatique est en 1936, la publication de l'article On Computable Numbers with an Application to the Entscheidungsproblem[24] par Alan Turing qui allait dÃ©placer le centre de prÃ©occupation de certains scientifiques (mathÃ©maticiens et logiciens) de l'Ã©poque, du sujet de la calculabilitÃ© (ou dÃ©cidabilitÃ©) ouvert par Hilbert, malmenÃ© par GodÃ«l, Ã©clairci par Church, vers le sujet de la mÃ©canisation du calcul (ou calculabilitÃ© effective). Dans ce texte de 36 pages, Turing expose une machine thÃ©orique capable d'effectuer tout calculÂ ; il dÃ©montre que cette machine est aussi puissante, au niveau du calcul, que tout Ãªtre humain. Autrement dit, un problÃ¨me mathÃ©matique possÃ¨de une solution, si et seulement si, il existe une machine de Turing capable de rÃ©soudre ce problÃ¨me. Par la suite, il expose une machine de Turing universelle apte Ã  reproduire toute machine de Turing, il s'agit des concepts d'ordinateur, de programmation et de programme. Il termine en dÃ©montrant qu'il existe au moins un problÃ¨me mathÃ©matique formellement insoluble, le problÃ¨me de l'arrÃªt.
Peu avant la Seconde Guerre mondiale, apparurent les premiÃ¨res calculatrices Ã©lectromÃ©caniques, construites selon les idÃ©es d'Alan Turing. Les machines furent vite supplantÃ©es par les premiers calculateurs Ã©lectroniques, nettement plus performants.

AnnÃ©es 1930
La fin des annÃ©es 1930 virent, pour la premiÃ¨re fois dans l'histoire de l'informatique, le dÃ©but de la construction de deux machines Ã  calculer programmables. Elles utilisaient des relais et Ã©taient programmÃ©es par la lecture de rouleaux perforÃ©s et donc, pour certains, Ã©taient dÃ©jÃ  des ordinateurs. Elles ne furent mises en service qu'au dÃ©but des annÃ©es 1940, faisant ainsi de 1940 la premiÃ¨re dÃ©cennie dans laquelle on trouve des ordinateurs et des machines Ã  calculer programmables totalement fonctionnels. C'est d'abord en 1937 que Howard Aiken, qui avait rÃ©alisÃ© que la machine analytique de Babbage Ã©tait le type de machine Ã  calculer qu'il voulait dÃ©velopper[25], proposa Ã  IBM de la crÃ©er et de la construireÂ ; aprÃ¨s une Ã©tude de faisabilitÃ©, Thomas J. Watson accepta de la construire en 1939Â ; elle fut testÃ©e en 1943 dans les locaux d'IBM et fut donnÃ©e et dÃ©mÃ©nagÃ©e Ã  l'universitÃ© Harvard en 1944, changeant son nom de ASCC Ã  Harvard Mark I ou Mark I.
Mais c'est aussi Konrad Zuse qui commenÃ§a le dÃ©veloppement de son Zuse 3, en secret, en 1939, et qui le finira en 1941. Parce que le Zuse 3 resta inconnu du grand public jusquâ€™aprÃ¨s la fin de la Seconde Guerre mondiale (sauf des services secrets amÃ©ricains qui le dÃ©truisirent dans un bombardement en 1943), ses solutions trÃ¨s inventives ne furent pas utilisÃ©es dans les efforts communs mondiaux de dÃ©veloppement de lâ€™ordinateur.

Ã‰volution
Premier ordinateur (1937-1946)
Six machines furent construites durant ces 9Â ans. Elles furent toutes dÃ©crites, au moins une fois, dans la multitude de livres de l'histoire de l'informatique, comme Ã©tant le premier ordinateurÂ ; aucune autre machine, construite ultÃ©rieurement, ne fut dÃ©crite comme telle. Ces six prÃ©curseurs peuvent Ãªtre divisÃ©es en trois groupes bien spÃ©cifiquesÂ :

d'une part, deux machines Ã  calculer. Ces deux machines n'Ã©taient pas programmables, l'une Ã©tait Ã©lectromÃ©canique, l'autre Ã©lectroniqueÂ :
1937Â : l'ABC qui pouvait rÃ©soudre des Ã©quations linÃ©aires et reconnu comme le premier ordinateur numÃ©rique,
1939Â : le Complex Number Calculator de George Stibitz, conÃ§u pour faire des opÃ©rations sur des nombres complexesÂ ;
d'autre part, deux machines Ã©lectromÃ©caniques programmables, programmÃ©es par la lecture de rouleaux perforÃ©s, mais qui ne possÃ©daient pas d'instruction de branchement conditionnel, et donc ne pouvaient aller d'une partie d'un programme Ã  une autreÂ :
1941Â : le Zuse 3,
1944Â : l'ASCC/Mark I d'IBMÂ ;

Â«Â Sans un branchement conditionnel, et donc lâ€™implÃ©mentation mÃ©canique du mot SI, le plus grand des calculateurs ne serait qu'une super machine Ã  calculer. Il pourrait Ãªtre comparÃ© Ã  une ligne d'assemblage, tout Ã©tant organisÃ© du dÃ©but Ã  la fin, avec aucune possibilitÃ© de changement une fois que la machine est mise en marche[26].Â Â»

â€”Â Andrew Hodges,  Alan Turing: the enigma, 1983.
et enfin, deux machines Ã©lectroniques spÃ©cialisÃ©es. Initialement ces machines ne pouvaient faire que cela, et Ã©taient programmÃ©es par le changement de fils et d'interrupteursÂ :
le Colossus, conÃ§u pour dÃ©chiffrer des messages secrets allemands,
1946Â : l'ENIAC[27], conÃ§u pour calculer des trajectoires balistiques.

Â«Â L'ENIAC et le Colosse Ã©taient comme deux kits Ã  assembler, desquelles beaucoup de machines similaires, mais diffÃ©rentes, pouvaient Ãªtre construites. Aucun nâ€™essaya dâ€™implÃ©menter l'universalitÃ© de la Â«Â machine de BabbageÂ Â» dans laquelle la machine n'est jamais modifiÃ©e, et oÃ¹ seulement les instructions sont rÃ©Ã©crites sur des cartes perforÃ©es[28].Â Â»

â€”Â Andrew Hodges,  Alan Turing: the enigma, 1983.
De ces six machines, seulement quatre furent connues de leurs contemporains, les deux autres, le Colosse et le Z3, utilisÃ©es dans l'effort de guerre, ne furent dÃ©couvertes qu'aprÃ¨s la fin de la Seconde Guerre mondiale, et donc ne participÃ¨rent pas au dÃ©veloppement communautaire mondial des ordinateurs. Seulement deux de ces machines furent utilisÃ©es dans les annÃ©es 1950, l'ASCC/Mark I et l'ENIAC, et chacune fut Ã©ventuellement modifiÃ©e pour en faire une machine Turing-complet. En juin 1945 est publiÃ© un article fondateur de John von Neumann[29] donnant les bases de l'architecture utilisÃ©e dans la quasi-totalitÃ© des ordinateurs depuis lors. Dans cet article, von Neumann veut concevoir un programme enregistrÃ© et programmÃ© dans la machine. La premiÃ¨re machine correspondant Ã  cette architecture, dite depuis architecture de von Neumann est une machine expÃ©rimentale la Small-Scale Experimental Machine (SSEM ou baby) construite Ã  Manchester en juillet 1948. En aoÃ»t 1949 la premiÃ¨re machine fonctionnelle, fondÃ©e sur les bases de von Neumann fut l'EDVAC.

Tubes Ã  vide et commutateurs (1946-1955)
 VÃ©rification Ã  l'oscilloscope de l'UNIVAC du Bureau du Recensement amÃ©ricain.
Cette chronologie[30] demande qu'un ordinateur soit Ã©lectronique et donc elle commence, en 1946, avec l'ENIAC qui, au dÃ©part, Ã©tait programmÃ© avec des interrupteurs et par le positionnement de fils sur un commutateur, comme sur un ancien standard tÃ©lÃ©phonique. Les ordinateurs de cette pÃ©riode sont Ã©normes avec des dizaines de milliers de tubes Ã  vide. L'ENIAC faisait 30Â m de long, 2,40Â m de haut et pesait 30Â tonnes. Ces machines nâ€™Ã©taient pas du tout fiables, par exemple, en 1952, dix-neuf mille tubes furent remplacÃ©s sur l'ENIAC, soit plus de tubes qu'il n'en contient[31].
Â«Â L'ENIAC prouva, sans ambiguÃ¯tÃ©, que les principes de base de l'Ã©lectronique Ã©tait bien fondÃ©s. Il Ã©tait vraiment inÃ©vitable que d'autres machines Ã  calculer de ce type seraient perfectionnÃ©es grÃ¢ce aux connaissances et Ã  lâ€™expÃ©rience acquises sur cette premiÃ¨re[32].Â Â»
De nouveau, le titre de premier ordinateur commercialisÃ© dÃ©pend de la dÃ©finition utilisÃ©eÂ ; trois ordinateurs sont souvent citÃ©s. En premier, le BINAC[33], conÃ§u par la Eckertâ€“Mauchly Computer Corporation et livrÃ© Ã  la Northrop Corporation en 1949 qui, aprÃ¨s sa livraison, ne fut jamais fonctionnel[34],[35]. En deuxiÃ¨me, le Ferranti Mark I, dont le prototype avait Ã©tÃ© dÃ©veloppÃ© par l'universitÃ© de Manchester, fut amÃ©liorÃ© et construit en un exemplaire par la sociÃ©tÃ© Ferranti et revendu Ã  l'universitÃ© de Manchester en fÃ©vrier 1951[36]. Et en dernier, UNIVAC I[33], conÃ§u par la Â«Â Eckertâ€“Mauchly Computer CorporationÂ Â», dont le premier fut vendu Ã  l'United States Census Bureau le 30 mars 1951. Une vingtaine de machines furent produites et vendues entre 1951 et 1954[37].

GÃ©nÃ©rations suivantes (1955-2000)
 Un mini-ordinateur PDP-8.
Â«Â L'utilisation de transistors au milieu des annÃ©es 1950 changea le jeu complÃ¨tement. Les ordinateurs devinrent assez fiables pour Ãªtre vendus Ã  des clients payants sachant qu'ils fonctionneraient assez longtemps pour faire du bon travail[38].Â Â» Les circuits intÃ©grÃ©s rÃ©duisirent la taille et le prix des ordinateurs considÃ©rablement. Les moyennes entreprises pouvaient maintenant acheter ce genre de machines.
Les circuits intÃ©grÃ©s permettent de concevoir une informatique plus dÃ©centralisÃ©e les constructeurs souhaitant concurrencer le gÃ©ant IBM. Le microprocesseur fut inventÃ© en 1969 par Ted Hoff d'Intel pendant le dÃ©veloppement d'une calculatrice pour la firme japonaise Busicom. Intel commercialisera le 4004 fin 1971. Ted Hoff avait copiÃ© l'architecture du PDP-8, le premier mini-ordinateur, et c'est grÃ¢ce Ã  la technologie de circuits intÃ©grÃ©s LSI (large scale integration), qui permettait de mettre quelques milliers de transistors sur une puce[39] qu'il put miniaturiser les fonctions d'un ordinateur en un seul circuit intÃ©grÃ©. La fonction premiÃ¨re du microprocesseur Ã©tait de contrÃ´ler son environnement. Il lisait des interrupteurs, les touches d'un clavier et il agissait en exÃ©cutant les opÃ©rations requises (addition, multiplication, etc.) et en affichant les rÃ©sultats. Le premier ordinateur personnel fut dÃ©crit dans le livre d'Edmund Berkeley, Giant brain, or machines that think, en 1949, et sa construction fut dÃ©crite dans une sÃ©rie d'articles du magazine Radio-Electronics Ã  partir du numÃ©ro d'octobre 1950. En 1972, une sociÃ©tÃ© franÃ§aise dÃ©veloppe le Micral, premier micro-ordinateur Ã  Ãªtre basÃ© sur le microprocesseur 8008. Mais lâ€™ordinateur qui crÃ©a l'industrie de l'ordinateur personnel est l'Altair 8800[40],[41] qui fut dÃ©crit pour la premiÃ¨re fois dans le magazine Radio-Electronics de janvier 1975. Bill Gates, Paul Allen, Steve Wozniak et Steve Jobs (ordre chronologique) firent tous leurs dÃ©buts dans la micro-informatique sur ce produit moins de six mois aprÃ¨s son introduction.

GÃ©nÃ©ralitÃ©s
 Le superordinateur Cray-1, lancÃ© en 1976.
Les ordinateurs furent d'abord utilisÃ©s pour le calcul (en nombres entiers d'abord, puis flottants). On ne peut cependant les assimiler Ã  de simples calculateurs, du fait de la possibilitÃ© quasi infinie de lancer d'autres programmes en fonction du rÃ©sultat de calculs, ou de capteurs internes ou externes (tempÃ©rature, inclinaison, orientation,Â etc.), ou de toute action de l'opÃ©rateur ou de son environnement.

Dans l'architecture de von Neumann, les donnÃ©es sont banalisÃ©es et peuvent Ãªtre interprÃ©tÃ©es indiffÃ©remment comme des nombres, des instructions, des valeurs logiques ou tout symbole dÃ©fini arbitrairement (exempleÂ : lettres de lâ€™alphabet).
Le calcul reprÃ©sente une des applications possibles. Dans ce cas, les donnÃ©es sont traitÃ©es comme des nombres.
Lâ€™ordinateur est utilisÃ© aussi pour ses possibilitÃ©s d'organisation de lâ€™information, entre autres sur des pÃ©riphÃ©riques de stockage magnÃ©tique. On a calculÃ© Ã  la fin des annÃ©es 1980 que sans les ordinateurs il faudrait toute la population franÃ§aise juste pour faire dans ce pays le seul travail des banquesÂ :
cette capacitÃ© dâ€™organiser les informations a gÃ©nÃ©ralisÃ© lâ€™usage du traitement de texte dans le grand publicÂ ;
la gestion des bases de donnÃ©es relationnelles permet Ã©galement de retrouver et de consolider des informations rÃ©parties vues par l'utilisateur comme plusieurs tables indÃ©pendantes.
Cette crÃ©ation d'un nÃ©ologisme fut Ã  l'origine de traductions multiples des expressions supercomputer, superordinateur ou supercalculateur.
L'expÃ©rience a appris Ã  distinguer dans un ordinateur deux aspects, dont le second avait Ã©tÃ© au dÃ©part sous-estimÃ©Â :

l'architecture physique, matÃ©rielle (alias hardware ou hard)Â ;
l'architecture logicielle (alias software ou soft).
Un ordinateur trÃ¨s avancÃ© techniquement pour son Ã©poque comme le Gamma 60 de la compagnie Bull n'eut pas le succÃ¨s attendu, pour la simple raison qu'il existait peu de moyens de mettre en Å“uvre commodÃ©ment ses possibilitÃ©s techniques[rÃ©f.Â nÃ©cessaire].
Le logiciel â€” et son complÃ©ment les services (formation, maintenanceâ€¦) â€” forme depuis le milieu des annÃ©es 1980 lâ€™essentiel des coÃ»ts d'Ã©quipement informatique, le matÃ©riel nâ€™y ayant qu'une part minoritaire.
Les ordinateurs peuvent Ãªtre sensibles aux bombes IEM.[rÃ©f.Â nÃ©cessaire]

Fonctionnement
 Vue d'ensemble des diffÃ©rents organes d'un ordinateur personnel.
 Ã‰clatÃ© d'un ordinateur personnelÂ :1Â : Ã‰cranÂ ;2Â : Carte mÃ¨reÂ ;3Â : ProcesseurÂ ;4Â : ParallÃ¨le ATAÂ ;5Â : MÃ©moire vive (RAM)Â ;6Â : Connecteurs d'extensionsÂ : Carte Graphique, Carte Son, Carte rÃ©seau,Â etc.Â ;7Â : Alimentation Ã©lectriqueÂ ;8Â : Lecteur de disque optiqueÂ ;9Â : Disque dur, disque Ã©lectroniqueÂ ;10Â : ClavierÂ ;11Â : Souris.
Parmi toutes les machines inventÃ©es par l'Homme, l'ordinateur est celle qui se rapproche le plus du concept anthropologique suivantÂ : Organe d'entrÃ©e, organe de traitement de l'information et organe de sortie. Chez l'humain, les organes d'entrÃ©e sont les organes sensoriels, l'organe de traitement est le cerveau dont les logiciels sont l'apprentissage avec des mises Ã  jour constantes en cours de vie, puis les organes de sortie sont les muscles. Pour les ordinateurs modernes, les organes d'entrÃ©e sont le clavier et la souris et les organes de sortie, l'Ã©cran, l'imprimante, le graveur de DVD,Â etc. Les techniques utilisÃ©es pour fabriquer ces machines ont Ã©normÃ©ment changÃ© depuis les annÃ©es 1940 et sont devenues une technologie (câ€™est-Ã -dire un ensemble industriel organisÃ© autour de techniques) Ã  part entiÃ¨re depuis les annÃ©es 1970. Beaucoup utilisent encore les concepts dÃ©finis par John von Neumann, bien que cette architecture soit en rÃ©gressionÂ : les programmes ne se modifient plus guÃ¨re eux-mÃªmes (ce qui serait considÃ©rÃ© comme une mauvaise pratique de programmation), et le matÃ©riel prend en compte cette nouvelle donne en sÃ©parant aujourd'hui nettement le stockage des instructions et des donnÃ©es, y compris dans les caches.
Lâ€™architecture de von Neumann dÃ©composait lâ€™ordinateur en quatre parties distinctesÂ :

Lâ€™unitÃ© arithmÃ©tique et logique (UAL) ou unitÃ© de traitementÂ : son rÃ´le est dâ€™effectuer les opÃ©rations de base, un peu comme le ferait une calculatriceÂ ;
Lâ€™unitÃ© de contrÃ´le. Câ€™est lâ€™Ã©quivalent des doigts qui actionneraient la calculatriceÂ ;
La mÃ©moire qui contient Ã  la fois les donnÃ©es et le programme qui dira Ã  lâ€™unitÃ© de contrÃ´le quels calculs faire sur ces donnÃ©es. La mÃ©moire se divise entre mÃ©moire vive (programmes et donnÃ©es en cours de fonctionnement) et mÃ©moire permanente (programmes et donnÃ©es de base de la machine)Â ;
Les entrÃ©es-sortiesÂ : dispositifs qui permettent de communiquer avec le monde extÃ©rieur.
UAL et UC
Lâ€™unitÃ© arithmÃ©tique et logique ou UAL est lâ€™Ã©lÃ©ment qui rÃ©alise les opÃ©rations Ã©lÃ©mentaires (additions, soustractionsâ€¦), les opÃ©rateurs logiques (ET, OU, NI, etc.) et les opÃ©rations de comparaison (par exemple la comparaison dâ€™Ã©galitÃ© entre deux zones de mÃ©moire). Câ€™est lâ€™UAL qui effectue les calculs de lâ€™ordinateur. Lâ€™unitÃ© de contrÃ´le prend ses instructions dans la mÃ©moire. Celles-ci lui indiquent ce quâ€™elle doit ordonner Ã  lâ€™UAL et, comment elle devra Ã©ventuellement agir selon les rÃ©sultats que celle-ci lui fournira. Une fois lâ€™opÃ©ration terminÃ©e, lâ€™unitÃ© de contrÃ´le passe soit Ã  lâ€™instruction suivante, soit Ã  une autre instruction Ã  laquelle le programme lui ordonne de se brancher.
L'unitÃ© de contrÃ´le facilite la communication entre l'unitÃ© arithmÃ©tique et logique, la mÃ©moire ainsi que les pÃ©riphÃ©riques. Elle gÃ¨re la plupart des exÃ©cutions des instructions dans l'ordinateur.

MÃ©moire
Au sein du systÃ¨me, la mÃ©moire peut Ãªtre dÃ©crite comme une suite de cellules numÃ©rotÃ©es contenant chacune une petite quantitÃ© dâ€™informations. Cette information peut servir Ã  indiquer Ã  lâ€™ordinateur ce quâ€™il doit faire (instructions) ou contenir des donnÃ©es Ã  traiter. Dans la plupart des architectures, c'est la mÃªme mÃ©moire qui est utilisÃ©e pour les deux fonctions. Dans les calculateurs massivement parallÃ¨les, on admet mÃªme que des instructions de programmes soient substituÃ©es Ã  dâ€™autres en cours dâ€™opÃ©ration lorsque cela se traduit par une plus grande efficacitÃ©. Cette pratique Ã©tait jadis courante, mais les impÃ©ratifs de lisibilitÃ© du gÃ©nie logiciel l'ont fait rÃ©gresser, hormis dans ce cas particulier, depuis plusieurs dÃ©cennies. Cette mÃ©moire peut Ãªtre rÃ©Ã©crite autant de fois que nÃ©cessaire. La taille de chacun des blocs de mÃ©moire ainsi que la technologie utilisÃ©e ont variÃ© selon les coÃ»ts et les besoinsÂ : 8 bits pour les tÃ©lÃ©communications, 12 bits pour lâ€™instrumentation (DEC) et 60 bits pour de gros calculateurs scientifiques (Control Data). Un consensus a fini par Ãªtre trouvÃ© autour de lâ€™octet comme unitÃ© adressable et dâ€™instructions sur format de 4 ou 8Â octets.
Dans tous les cas de figure, l'octet reste adressable, ce qui simplifie l'Ã©criture des programmes. Les techniques utilisÃ©es pour la rÃ©alisation des mÃ©moires ont compris des relais Ã©lectromÃ©caniques, des tubes au mercure au sein desquels Ã©taient gÃ©nÃ©rÃ©es des ondes acoustiques, des transistors individuels, des tores de ferrite et enfin des circuits intÃ©grÃ©s incluant des millions de transistors.

EntrÃ©es-Sorties
Les dispositifs dâ€™entrÃ©e/sortie permettent Ã  lâ€™ordinateur de communiquer avec lâ€™extÃ©rieur. Ces dispositifs sont trÃ¨s importants, du clavier Ã  lâ€™Ã©cran. La carte rÃ©seau permet par exemple de relier les ordinateurs en rÃ©seau informatique, dont le plus grand est Internet. Le point commun entre tous les pÃ©riphÃ©riques dâ€™entrÃ©e est quâ€™ils convertissent lâ€™information quâ€™ils rÃ©cupÃ¨rent de lâ€™extÃ©rieur en donnÃ©es comprÃ©hensibles par lâ€™ordinateur. Ã€ lâ€™inverse, les pÃ©riphÃ©riques de sortie dÃ©codent lâ€™information fournie par lâ€™ordinateur afin de la rendre comprÃ©hensible par lâ€™utilisateur.

Bus
Article dÃ©taillÃ©Â : bus (informatique).
Ces diffÃ©rentes parties sont reliÃ©es par trois bus, le bus d'adresse, le bus de donnÃ©es et le bus de contrÃ´le. Un bus est un groupement d'un certain nombre de fils Ã©lectriques rÃ©alisant une liaison pour transporter des informations binaires codÃ©es sur plusieurs bits. Le bus d'adresse transporte les adresses gÃ©nÃ©rÃ©es par l'UCT (UnitÃ© Centrale de Traitement) pour sÃ©lectionner une case mÃ©moire ou un registre interne de l'un des blocs. Le nombre de bits vÃ©hiculÃ©s par ce bus dÃ©pend de la quantitÃ© de mÃ©moire qui doit Ãªtre adressÃ©e. Le bus de donnÃ©es transporte les donnÃ©es Ã©changÃ©es entre les diffÃ©rents Ã©lÃ©ments du systÃ¨me. Le bus de contrÃ´le transporte les diffÃ©rents signaux de synchronisation nÃ©cessaires au fonctionnement du systÃ¨meÂ : signal de lecture (RD), signal d'Ã©criture (WR), signal de sÃ©lection (CSÂ : Chip Select).

Architecture
La miniaturisation permet dâ€™intÃ©grer lâ€™UAL et lâ€™unitÃ© de contrÃ´le au sein dâ€™un mÃªme circuit intÃ©grÃ© connu sous le nom de microprocesseur. Typiquement, la mÃ©moire est situÃ©e sur des circuits intÃ©grÃ©s proches du processeur, une partie de cette mÃ©moire, la mÃ©moire cache, pouvant Ãªtre situÃ©e sur le mÃªme circuit intÃ©grÃ© que lâ€™UAL.
Lâ€™ensemble est, sur la plupart des architectures, complÃ©tÃ© dâ€™une horloge qui cadence le processeur. Bien sÃ»r, on souhaite qu'elle soit le plus rapide possible, mais on ne peut pas augmenter sans limites sa vitesse pour deux raisonsÂ :

plus lâ€™horloge est rapide et plus le processeur dÃ©gage de la chaleur (selon le carrÃ© de la frÃ©quence). Une trop grande tempÃ©rature peut dÃ©tÃ©riorer le processeurÂ ;
il existe une cadence oÃ¹ le processeur devient instableÂ ; il gÃ©nÃ¨re des erreurs qui mÃ¨nent le plus souvent Ã  un plantage.
 Ã‰volution du nombre de transistors sur un circuit intÃ©grÃ© selon la loi de Moore.
La tendance a Ã©tÃ© Ã  partir de 2004 de regrouper plusieurs UAL dans le mÃªme processeur, voire plusieurs processeurs dans la mÃªme puce. En effet, la miniaturisation progressive (voir Loi de Moore) le permet sans grand changement de coÃ»t. Une autre tendance, depuis 2006 chez ARM, est aux microprocesseurs sans horlogeÂ : la moitiÃ© de la dissipation thermique est en effet due aux signaux d'horloge quand le microprocesseur fonctionneÂ ; de plus, un microprocesseur sans horloge a une consommation presque nulle quand il ne fonctionne pasÂ : le seul signal d'horloge nÃ©cessaire est alors celui destinÃ© au rafraÃ®chissement des mÃ©moires. Cet atout est important pour les modÃ¨les portables.
Le principal Ã©cart fonctionnel aujourdâ€™hui par rapport au modÃ¨le de von Neumann est la prÃ©sence sur certaines architectures de deux antÃ©mÃ©moires diffÃ©rentesÂ : une pour les instructions et une pour les donnÃ©es (alors que le modÃ¨le de von Neumann spÃ©cifiait une mÃ©moire commune pour les deux). La raison de cet Ã©cart est que la modification par un programme de ses propres instructions est aujourdâ€™hui considÃ©rÃ©e (sauf sur les machines hautement parallÃ¨les) comme une pratique Ã  proscrire. DÃ¨s lors, si le contenu du cache de donnÃ©es doit Ãªtre rÃ©crit en mÃ©moire principale quand il est modifiÃ©, on sait que celui du cache dâ€™instructions nâ€™aura jamais Ã  lâ€™Ãªtre, dâ€™oÃ¹ simplification des circuits et gain de performance.

Instructions
Les instructions que lâ€™ordinateur peut comprendre ne sont pas celles du langage humain. Le matÃ©riel sait juste exÃ©cuter un nombre limitÃ© dâ€™instructions bien dÃ©finies. Des instructions typiques comprises par un ordinateur sont par exempleÂ :

Copier le contenu de la cellule 123 et le placer dans la cellule 456Â ;
Ajouter le contenu de la cellule 321 Ã  celui de la cellule 654Â ;
Placer le rÃ©sultat dans la cellule 777Â ;
Si le contenu de la cellule 999 vaut 0, exÃ©cuter lâ€™instruction Ã  la cellule 345.
La plupart des instructions se composent de deux zonesÂ : lâ€™une indiquant quoi faire, nommÃ©e code opÃ©ration, et lâ€™autre indiquant oÃ¹ le faire, nommÃ©e opÃ©rande.
Au sein de lâ€™ordinateur, les instructions correspondent Ã  des codes â€” le code pour une copie Ã©tant par exemple 001. Lâ€™ensemble dâ€™instructions quâ€™un ordinateur supporte se nomme son langage machine, langage qui est une succession de chiffres binaires, car les instructions et donnÃ©es qui sont comprises par le processeur (CPU) sont constituÃ©es uniquement de 0 (zÃ©ro) et de 1 (un)Â :

0 = le courant Ã©lectrique ne passe pasÂ ;
1 = le courant Ã©lectrique passe.
En gÃ©nÃ©ral, ce type de langage n'est pas utilisÃ© car on lui prÃ©fÃ¨re ce que lâ€™on appelle un langage de haut niveau qui est ensuite transformÃ© en langage binaire par un programme spÃ©cial (interprÃ©teur ou compilateur selon les besoins). Les programmes ainsi obtenus sont des programmes compilÃ©s comprÃ©hensibles par l'ordinateur dans son langage natif. Certains langages de programmation, comme lâ€™assembleur sont dits langages de bas niveau car les instructions quâ€™ils utilisent sont trÃ¨s proches de celles de lâ€™ordinateur. Les programmes Ã©crits dans ces langages sont ainsi trÃ¨s dÃ©pendants de la plate-forme pour laquelle ils ont Ã©tÃ© dÃ©veloppÃ©s. Le langage C, beaucoup plus facile Ã  relire que lâ€™assembleur, permet de produire plus facilement des programmes. Pour cette raison, on lâ€™a vu de plus en plus utilisÃ© Ã  mesure que les coÃ»ts du matÃ©riel diminuaient et que les salaires horaires des programmeurs augmentaient[rÃ©f.Â nÃ©cessaire].

Logiciels
Les logiciels informatiques sont des listes (gÃ©nÃ©ralement longues) dâ€™instructions exÃ©cutables par un ordinateur. De nombreux programmes contiennent des millions dâ€™instructions, effectuÃ©es pour certaines de maniÃ¨re rÃ©pÃ©titive. De nos jours, un ordinateur personnel exÃ©cute plusieurs milliards dâ€™instructions par seconde. Depuis le milieu des annÃ©es 1960, des ordinateurs exÃ©cutent plusieurs programmes simultanÃ©ment. Cette possibilitÃ© est appelÃ©e multitÃ¢che. Câ€™est le cas de tous les ordinateurs modernes. En rÃ©alitÃ©, chaque cÅ“ur de processeur nâ€™exÃ©cute quâ€™un programme Ã  la fois, passant dâ€™un programme Ã  lâ€™autre chaque fois que nÃ©cessaire. Si la rapiditÃ© du processeur est suffisamment grande par rapport au nombre de tÃ¢ches Ã  exÃ©cuter, lâ€™utilisateur aura lâ€™impression dâ€™une exÃ©cution simultanÃ©e des programmes. Les prioritÃ©s associÃ©es aux diffÃ©rents programmes sont, en gÃ©nÃ©ral, gÃ©rÃ©es par le systÃ¨me d'exploitation.
Le systÃ¨me dâ€™exploitation est le programme central qui contient les programmes de base nÃ©cessaires au bon fonctionnement des applications de lâ€™ordinateur. Le systÃ¨me dâ€™exploitation alloue les ressources physiques de lâ€™ordinateur (temps processeur, mÃ©moireâ€¦) aux diffÃ©rents programmes en cours dâ€™exÃ©cution. Il fournit aussi des outils aux logiciels (comme les pilotes) afin de leur faciliter lâ€™utilisation des diffÃ©rents pÃ©riphÃ©riques sans avoir Ã  en connaÃ®tre les dÃ©tails physiques.

Types d'ordinateurs

DiffÃ©rents types d'ordinateurs



IBM 370 (1972).






HP 2116 (1974).






Serveur VAX (1975).






Bull-Micral p.Â 2 franÃ§ais en 1981.






IBM PC 5150 en 1983.






Superordinateur Columbia de la NASA en 2004.






Acer Aspire 8920 (2012).




Par type de phÃ©nomÃ¨ne physique
MÃ©canique
L'ordinateur mÃ©canique se base sur des composants mÃ©canique pour effectuer les calculs (engrenages,Â etc.)

La machine analytique de Charles Babbage
Ã‰lectromÃ©canique
Les ordinateurs Ã©lectromÃ©canique utilisent Ã  la fois du courant Ã©lectrique et des mÃ©canismes mÃ©caniques pour le calcul (relais Ã©lectromÃ©caniques)

Zuse 3 et l'ASCC/Mark I d'IBM
Ã‰lectronique
Les ordinateurs Ã©lectroniques utilisent des Ã©lectrons pour rÃ©aliser les diffÃ©rentes fonctions de l'architecture d'un ordinateur.
C'est le phÃ©nomÃ¨ne physique sous-jacent de nos ordinateurs actuels.

Quantique
Les ordinateurs quantiques utilisent les propriÃ©tÃ©s quantiques de la matiÃ¨re.

Optique
Les ordinateurs optiques utilisent des photons pour le traitement des informations.

Chimique (ou biologique)
Ordinateur Ã  ADN
Ordinateur neuronal
Par type de traitement temporel
Synchrone
Autosynchrone
Asynchrone
Par domaine d'application
Ordinateur d'entrepriseÂ : Mainframe (exemplesÂ : IBM 360 et 370, DEC PDP-10,Â etc.), Mini-ordinateur, (exemplesÂ : IBM AS/400-ISeries, RS/6000, HP9000,Â etc.), Superordinateur (exemplesÂ : Riken, Cray,Â etc.)
Ordinateur personnel (exemplesÂ : PC, Macintosh,Â etc.)
SystÃ¨me embarquÃ©
Ordinateur de bord
Par taille
Ordinateur de pocheÂ : Assistant numÃ©rique personnel, Smartphone, Smartwatch
Ordinateur portableÂ : Ultraportable, Tablette tactile, Ordinateur portable
Ordinateur de bureauÂ : Mini PC, Ordinateur de bureau, Station de travail
Ordinateur intermÃ©diaireÂ : Mini-ordinateur
Ordinateur gÃ©antÂ : Mainframe, Superordinateur
Par architecture
Amiga
Atari ST
Compatible PC
Macintosh
stations SPARC
â€¦
Notes et rÃ©fÃ©rences

â†‘ Blaise Pascal, PensÃ©es, PensÃ©es sur Gallica. 
Â«Â La machine d'arithmÃ©tique fait des effets qui approchent plus de la pensÃ©e que tout ce que font les animauxÂ ; mais elle ne fait rien qui puisse faire dire qu'elle a de la volontÃ©, comme les animaux.Â Â»

.

â†‘ (en) Anthony Hyman, Charles Babbage, pioneer of the computer, 1982. Â«Â â€¦in less than two years he had sketched out many of the salient features of the modern computer. A crucial step was the adoption of a punched card system derived from the Jacquard loom.Â Â».

â†‘ (en) Â«Â From Analytical Engine to Electronic Digital Computer: The Contributions of Ludgate, Torres, and BushÂ Â» (consultÃ© le 24 mars 2013).

â†‘ CitÃ© dansÂ : Randell 1973, p.Â 187.

â†‘ IBM France, 1955Â : le terme Â«Â OrdinateurÂ Â» est inventÃ© par Jacques Perret, Ã  la demande d'IBM France, Centenaire d'IBM, 16 avril 2014.

â†‘ 16 avril 1955Â : "Que diriez-vous d'ordinateurÂ ?", Le Monde, 2005.

â†‘ a et b Pierre Guiraud, ProblÃ¨mes et mÃ©thodes de la statistique linguistique, Springer - 1959, (ISBNÂ 9789027700254).

â†‘ L. Jacob, p.Â 189 (1911).

â†‘ NapolÃ©on Landais, Dictionnaire gÃ©nÃ©ral et grammatical des dictionnaires franÃ§ais, Didier - 1849.

â†‘ Â«Â Ã‰tymologie du mot ORDINATEURÂ : ordinateur (ancien franÃ§ais)Â Â», sur presse-francophone.org (consultÃ© le 22 fÃ©vrier 2008).

â†‘ Â«Â Histoire de la crÃ©ation du mot OrdinateurÂ : la lettre in extenso de J. Perret et son contexte expliquÃ© par Gilles ZemorÂ Â», 23 aoÃ»t 1996 (consultÃ© le 12 mars 2008).

â†‘ The Most Famous Image in the Early History of Computing From cave paintings to the internet HistoryofScience.com.

â†‘ Anthony Hyman, ed., Science and Reform: Selected Works of Charles Babbage (Cambridge, England: Cambridge University Press, 1989), page 298. Une copie de ce tableau est dans la collection du Science Museum de Londres. (Delve (2007), page 99).

â†‘ Bernard Cohen, p. 297 (2000)Â ; traduit de lâ€™amÃ©ricainÂ : Â«Â Historians of technology and computer scientists interested in history have adopted a number of qualifications that define a computer. As a result, the question of whether Mark I was or was not a computer depends not on a general consensus but rather on the particular definition that is adopted. Often, some primary defining characteristics of a computer are that it must (1) be electronic, (2) be digital (rather than analog), (3) be programmed, (4) be able to perform the four elementary operations (addition, subtraction, multiplication, and division) and -often- extract roots or obtain information from built-in tables, and (5) incorporate the principle of the stored program. A machine does not generally qualify as a computer unless it has some further properties, for example the ability to perform certain specified operations automatically in a controlled and predetermined sequence. For some historians and computer scientists, a machine must also have been actually constructed and then become fully operational.Â Â».

â†‘ (en) "The introduction of punched cards into the new engine was important not only as a more convenient form of control than the drums, or because programs could now be of unlimited extent, and could be stored and repeated without the danger of introducing errors in setting the machine by hand; it was important also because it served to crystalize Babbage's feeling that he had invented something really new, something much more than a sophisticated calculating machine." Bruce Collier, 1970.

â†‘ Robert LigonniÃ¨re, p.Â 109 (1987).

â†‘ (en) fragment of Babbage's first difference engine (page consultÃ©e le 18-10-2013).

â†‘ a et b Bernard Cohen, p.Â 66 (2000).

â†‘ Bernard Cohen, p.Â 44 (2000).

â†‘ "â€¦mais ces machines semblent Ãªtre restÃ©es des appareils de dÃ©monstration." Louis Couffignal, p.Â 53 (1933).

â†‘ "â€¦sa machine est aujourd'hui en voie de construction.", citÃ© dans la prÃ©face par Maurice d'Ocagne, page VII (1933).

â†‘ l'Altair 8800 qui eut Bill Gates et Paul Allen comme premiers programmeurs.

â†‘ (en) Jim Turley, The essential guide to semiconductors, Prentice Hall, New Jersey, 2003, p.Â 123.

â†‘ (en) Alan Turing, On Computable Numbers, with an Application to the EntscheidungsproblemÂ : Proceedings of the London Mathematical Society, London Mathematical Society, 1937 (DOIÂ 10.1112/PLMS/S2-42.1.230, lire en ligne) et Â«Â [idem]Â : A CorrectionÂ Â», Proc. London Math. Soc., 2e sÃ©rie, vol.Â 43,â€ 1938, p.Â 544-546 (DOIÂ 10.1112/plms/s2-43.6.544, lire en ligne).

â†‘ Bernard Cohen, p.Â 66-67 (2000)Â : "Carmello Lanzaâ€¦couldn't see why in the world I (Howard Aiken) wanted to do anything like this in the Physics laboratory, because we already had such a machine and nobody used itâ€¦ Lanza led him up into the atticâ€¦ There, sure enoughâ€¦ were the wheels that Aiken later put on display in the lobby of the Computer Laboratory. With them was a letter from Henry Prevost Babbage describing these wheels as part of his father's proposed calculating engine. This was the first time Aiken ever heard of Babbage he said, and it was this experience that led him to look up Babbage in the library and to come across his autobiography."

â†‘ Andrew Hodges p. 298 (1983)Â : (en) Without conditional branching, the ability to mechanize the word IF, the grandest calculator would be no more than a glorified adding machine. It might be thought of as assembly line, everything being laid down from start to finish, and there being no possibility of interference in the process once started.

â†‘ Les premiers ordinateurs, sur online.fr, consultÃ© le 14 octobre 2018.

â†‘ Andrew Hodges p. 302 (1983)Â : (en) Both ENIAC and Colossus were like kits out of which many slightly different machines could be made. Neither sought to embody the true universality of Babbage's conception, in which the machinery would be entirely unchanged, and only the instruction cards rewritten.

â†‘ (en) [PDF] First Draft of a Report on the EDVAC, sur le site archive.org.

â†‘ Andrew S Tanenbaum page 5-13 (1987). Le livre utilise 1945 mais l'ENIAC fut inaugurÃ© en fÃ©vrier 1946.

â†‘ Encyclopedia of Computer Science p.Â 541 (1976).

â†‘ Encyclopedia of Computer Science p.Â 541 (1976). Traduit de "the ENIAC established the fact that the basic principles of electronic engineering are sound. It was indeed inevitable that future computing machines of this type would be improved through the knowledge and experience gained on this first one."

â†‘ a et b plaque commÃ©morant la premiÃ¨re commercialisation d'un ordinateur pour le BINAC et pour l'UNIVAC I (dans ce cas premiÃ¨re commercialisation en vente libre).

â†‘ (en) interview avec Isaac Auerbach Ã  propos du BINAC et de l'UNIVAC.

â†‘ (en) The BINAC: A case study in the history of technology, IEEE.

â†‘ Une deuxiÃ¨me machine, dont la commande par l'Ã©tablissement de recherche atomique d'Harwell fut annulÃ©e au milieu de sa construction, ne fut jamais finie par Ferranti.

â†‘ Voir le tableau rÃ©capitulatif dans l'articleÂ : (en) UNIVAC installations, 1951â€“1954.

â†‘ (en) Andrew S Tanenbaum page 6 (1987) "The introduction of the transistor in the mid-1950s changed the picture radically. Computers became reliable enough that they could be sold to paying customers with the expectation that they would continue to function long enough to get some useful work done."

â†‘ Andrew S Tanenbaum page 11 (1987).

â†‘ (en) How the Altair 8800 started the PC revolution (Part 1) consultÃ© le 12-05-2013.

â†‘ (en) Bill Gates talks about Microsoft and the Altair 8800 (1994) consultÃ© le 12-05-2013.



Voir aussi

Sur les autres projets WikimediaÂ :

Ordinateur, sur Wikimedia Commonsordinateur, sur le Wiktionnaire (thÃ©saurus)Ordinateur, sur Wikiquote


Bibliographie
L. Jacob, EncyclopÃ©die Scientifique, Le Calcul MÃ©canique, Paris, Octave Doin et fils, 1911
Robert LigonniÃ¨re, PrÃ©histoire et Histoire des ordinateurs, Paris, Robert Laffont, 1987, 356Â p. (ISBNÂ 978-2-221-05261-7)
Louis Couffignal, Les machines Ã  calculerÂ : leurs principes, leur Ã©volution, Paris, Gauthier-Villars, 1933
(en) Bernard Cohen, Howard AikenÂ : Portrait of a computer pioneer, Cambridge, Massachusetts, The MIT press, 2000, 329Â p. (ISBNÂ 978-0-262-53179-5, lire en ligne)
(en) Bruce Collier, The little engine that could'veÂ : The calculating machines of Charles Babbage, Garland Publishing Inc, 1970, 319Â p. (ISBNÂ 0-8240-0043-9, lire en ligne)
(en) Andrew Hodges, Alan TuringÂ : the enigma, Londres, Burnett books, 1983 (ISBNÂ 0-8240-0043-9, lire en ligne)
(en) Brian Randell, The origins of Digital computers, Selected Papers, New York, Springer-Verlag, 1973, 464Â p. (ISBNÂ 3-540-06169-X)
(en) Andrew S. Tanenbaum, Operating Systems, Design and implementation, Amsterdam, The Netherlands, Prentice-Hall International Editions, 1987 (ISBNÂ 0-13-637331-3)
(en) Ralston & Chester-Editors, Encyclopedia of Computer Science, New York, Petrocelli/Charter, 1976 (ISBNÂ 0-88405-321-0)
(en) Nancy Stern, Â«Â The BINAC:A case study in the history of technologyÂ Â», Annals of the History of Computing, IEEE, vol.Â 1, noÂ 1,â€ juillet 1979, p.Â 9â€“20 (ISSNÂ 1058-6180)
(en) Nancy Stern, Â«Â An Interview with ISAAC L. AUERBACHÂ Â», Charles Babbage Institute,â€ avril 1978 (lire en ligne [PDF])
Que diriez-vous dâ€™Â«Â ordinateurÂ Â»Â ?, lettre de 1955 de J. Perret proposant ce mot, et son analyse 2015 par L. Depecker, en ligne sur BibNum.
Articles connexes

Liste d'ordinateurs du passÃ©
Minitel
Micro-informatique
Programmation informatique
Programme informatique
Intelligence artificielle
Superordinateur
Internet
Intranet
DÃ©bit
RÃ©seau informatique
Serveur informatique
DonnÃ©es informatiques
SystÃ¨me informatique
SystÃ¨me d'exploitation
SystÃ¨me embarquÃ©
PÃ©riphÃ©rique informatique
Informatique
Histoire de l'informatique
Histoire des ordinateurs
Bus de donnÃ©es
Bus de contrÃ´le
Ordinateur portable
Ordinateur quantique
Ordinateur Ã  ADN
Ordinateur neuronal
Ordinateur optique
Ordinateur de bord
Ordinateur de bureau
Ordinateur personnel
Mini-ordinateur
Ultraportable
Mini PC
Smartphone
Tablette tactile
Ã‰cran d'ordinateur
Souris d'ordinateur
Clavier d'ordinateur
PavÃ© numÃ©rique
Logiciel
Carte mÃ¨re
Carte d'extension
Carte rÃ©seau
MÃ©moire vive
MÃ©moire cache
Disque dur
Processeur
Microprocesseur
Circuit intÃ©grÃ©
Bug

Liens externes



Ressources relatives Ã  la santÃ©Â : (en)Â Medical Subject Headings (csÂ +Â sk)Â WikiSkripta 
Ressource relative Ã  la littÃ©ratureÂ : (en)Â The Encyclopedia of Science Fiction 
Notices dans des dictionnaires ou encyclopÃ©dies gÃ©nÃ©ralistesÂ : Brockhaus EnzyklopÃ¤die EncyclopÃ¦dia Britannica EncyclopÃ¦dia Universalis EncyclopÃ©die Treccani Gran EnciclopÃ¨dia Catalana Swedish Nationalencyklopedin Store norske leksikon 
Notices d'autoritÃ©Â : BibliothÃ¨que nationale de France (donnÃ©es) BibliothÃ¨que du CongrÃ¨s Gemeinsame Normdatei BibliothÃ¨que nationale de la DiÃ¨te BibliothÃ¨que nationale dâ€™Espagne BibliothÃ¨que nationale dâ€™IsraÃ«l BibliothÃ¨que nationale tchÃ¨que 
Â«Â OrdinateurÂ : demandez le programmeÂ !Â Â», EurÃªkaÂ ! , France Culture, 18 aoÃ»t 2022.


vÂ Â· mDomaines de l'informatique
 
RemarqueÂ : cette liste s'inspire du systÃ¨me de classification informatique de l'ACM Ã©ditÃ© en 2012
 
MatÃ©riel

Circuit imprimÃ©
PÃ©riphÃ©rique
Circuit intÃ©grÃ©
IntÃ©gration Ã  trÃ¨s grande Ã©chelle
Informatique durable
Conception assistÃ©e par ordinateur pour l'Ã©lectronique

 
Appareil et organisationd'un systÃ¨me

Architecture matÃ©rielle
Machine Ã  calculer
MÃ©canographie
Calculateur analogique
Calculatrice
Calculateur quantique
Ordinateur
SystÃ¨me embarquÃ©
SystÃ¨me temps rÃ©el
SÃ»retÃ© de fonctionnement

 
RÃ©seau

Architecture de rÃ©seau
Protocole de communication
Ã‰quipement d'interconnexion de rÃ©seau informatique
Planificateur de rÃ©seauÂ (en)
Rendement du rÃ©seauÂ (en)
Service rÃ©seau

 
Organisation du logiciel

InterprÃ¨te
Middleware
Machine virtuelle
SystÃ¨me d'exploitation
QualitÃ© logicielle

 
ThÃ©orieÂ (en) et outilÂ (en)de programmation

Paradigme de programmation
Langage de programmation
Compilateur
Langage dÃ©diÃ©
Langage de modÃ©lisation
Cadriciel
Environnement de dÃ©veloppement
Gestion de configuration logicielle
BibliothÃ¨que logicielle
DÃ©pÃ´t

 
DÃ©veloppement de logiciel

Processus de dÃ©veloppement de logicielÂ (en)
Analyse des exigences
Conception de logiciel
Assemblage de logicielÂ (en)
DÃ©ploiement de logicielÂ (en)
Maintenance du logiciel
Ã‰quipe de programmationÂ (en)
Open source

 
ThÃ©orie du calculÂ (en)

ModÃ¨le de calcul
Langage formel
ThÃ©orie des automates
ThÃ©orie de la complexitÃ©
LogiqueÂ (en)
SÃ©mantique

 
Algorithmique

Algorithme
Conception d'algorithmeÂ (en)
Analyse de la complexitÃ© des algorithmes
Algorithme probabiliste
GÃ©omÃ©trie algorithmique

 
MathÃ©matiquesde l'informatique

MathÃ©matiques discrÃ¨tes
ProbabilitÃ©
Statistique
Logiciel mathÃ©matiqueÂ (en)
ThÃ©orie de l'information
Analyse
Analyse numÃ©rique

 
SystÃ¨me d'information

Base de donnÃ©es
MÃ©moire (informatique)
Progiciel
Logiciel social
SystÃ¨me d'information gÃ©ographique
SystÃ¨me d'aide Ã  la dÃ©cision
Supervision
Base de donnÃ©es multimÃ©dia
Exploration de donnÃ©es
BibliothÃ¨que numÃ©rique
Plateforme
Marketing Ã©lectronique
World Wide Web
Recherche d'information

 
SÃ©curitÃ©

Cryptographie
MÃ©thode formelle
Service de sÃ©curitÃ©Â (en)
SystÃ¨me de dÃ©tection d'intrusion
SÃ©curitÃ© matÃ©rielleÂ (en)
SÃ©curitÃ© du rÃ©seau
SÃ©curitÃ© de l'information
SÃ©curitÃ© de l'applicationÂ (en)

 
Interactions homme-machine

Design numÃ©rique
Informatique socialeÂ (en)
Informatique ubiquitaire
VisualisationÂ (en)
AccessibilitÃ© numÃ©rique

 
ConcurrenceÂ (en)

Programmation concurrente
ParallÃ©lisme
Calcul distribuÃ©
Multithreading
Multiprocesseur

 
Intelligence artificielle

Traitement automatique des langues
ReprÃ©sentation des connaissances
Vision par ordinateur
Planification
Optimisation
Philosophie de l'intelligence artificielle
Intelligence artificielle distribuÃ©e

 
Apprentissage automatique

Apprentissage supervisÃ©
Apprentissage non supervisÃ©
Apprentissage par renforcement
Apprentissage multi-tÃ¢chesÂ (en)
Validation croisÃ©e

 
Infographie

Animation par ordinateur
2D numÃ©rique
Animation 3D
Rendu photorÃ©aliste
Retouche d'image
Processeur graphique
RÃ©alitÃ© mixte
RÃ©alitÃ© virtuelle
Compression d'image
Conception paramÃ©trique

 
Informatique appliquÃ©e

Commerce en ligne
Logiciel d'entreprise
MathÃ©matiques computationnelles
Physique numÃ©rique
Chimie numÃ©rique
Biologie numÃ©rique
Sciences sociales numÃ©riqueÂ (en)
IngÃ©nierie numÃ©rique
Informatique mÃ©dicale
Art numÃ©rique
Ã‰dition Ã©lectronique
Cyberguerre
Vote Ã©lectronique
Jeu vidÃ©o
Traitement de texte
Recherche opÃ©rationnelle
Technologies de l'Ã©ducation
Gestion Ã©lectronique des documents

 

vÂ Â· mÃ‰lectronique
 
Analogique

Alimentation Ã©lectrique
Amplificateur
Mesure
OpÃ©rationnel
Bobine
Capteur
Circuit intÃ©grÃ©
Composants analogiques programmables (FPAA)
Commutateur
Condensateur
Diode
Filtre
PotentiomÃ¨tre
Radiocommunication
RÃ©sistance
Thyristor
Transistor
Triac
Tube

 
NumÃ©rique

ASIC
Circuit logique programmable (CPLD/EPLD/FPGA/PAL/PLA/PLD)
Convertisseur analogique-numÃ©rique
Convertisseur numÃ©rique-analogique
DSP
ÂµContrÃ´leur
ÂµProcesseur
Ordinateur
Porte logique

 
Opto-Ã©lectronique

Cellule photoÃ©lectrique
Cellule photovoltaÃ¯que
LED
Diode laser
OLED
Photocoupleur
Photodiode

 
MicroÃ©lectronique
MEMS
 

Automatique
Ã‰lectricitÃ©
Ã‰lectrochimie
Ã‰lectromagnÃ©tisme
Ã‰lectrotechnique
Robotique
Traitement du signal


vÂ Â· mPersonnalitÃ© de l'annÃ©e selon Time Magazine
 
1927-1950

Charles Lindbergh (1927)
Walter Chrysler (1928)
Owen D. Young (1929)
Mahatma Gandhi (1930)
Pierre Laval (1931)
Franklin D. Roosevelt (1932)
Hugh S. Johnson (1933)
Franklin D. Roosevelt (1934)
HaÃ¯lÃ© SÃ©lassiÃ© Ier (1935)
Wallis Simpson (1936)
Tchang KaÃ¯-chek
Soong May-ling (1937)
Adolf Hitler (1938)
Joseph Staline (1939)
Winston Churchill (1940)
Franklin D. Roosevelt (1941)
Joseph Staline (1942)
George Marshall (1943)
Dwight D. Eisenhower (1944)
Harry S. Truman (1945)
James F. Byrnes (1946)
George Marshall (1947)
Harry S. Truman (1948)
Winston Churchill (1949)
Les soldats amÃ©ricains (1950)

 
1951-1975

Mohammad Mossadegh (1951)
Ã‰lisabeth II (1952)
Konrad Adenauer (1953)
John Foster Dulles (1954)
Harlow Curtice (1955)
Les rÃ©voltÃ©s hongrois (1956)
Nikita Khrouchtchev (1957)
Charles de Gaulle (1958)
Dwight D. Eisenhower (1959)
Les scientifiques amÃ©ricains (1960)
John F. Kennedy (1961)
Le pape Jean XXIII (1962)
Martin Luther King (1963)
Lyndon B. Johnson (1964)
William Westmoreland (1965)
La gÃ©nÃ©ration Baby boom et leurs cadets (1966)
Lyndon B. Johnson (1967)
Les astronautes d'Apollo 8
1968, William Anders
Frank Borman
James Lovell
La classe moyenne amÃ©ricaine (1969)
Willy Brandt (1970)
Richard Nixon (1971)
Henry Kissinger
Richard Nixon (1972)
John Sirica (1973)
Le roi FayÃ§al (1974)
Les AmÃ©ricaines (1975)

 
1976-2000

Jimmy Carter (1976)
Anouar el-Sadate (1977)
Deng Xiaoping (1978)
L'ayatollah Khomeini (1979)
Ronald Reagan (1980)
Lech WaÅ‚Ä™sa (1981)
L'ordinateur (1982)
Ronald Reagan
Iouri Andropov (1983)
Peter Ueberroth (1984)
Deng Xiaoping (1985)
Corazon Aquino (1986)
MikhaÃ¯l Gorbatchev (1987)
La Terre en danger (1988)
MikhaÃ¯l Gorbatchev (1989)
George H. W. Bush (1990)
Ted Turner (1991)
Bill Clinton (1992)
Les Faiseurs de paix (1993)
Le pape Jean-Paul II (1994)
Newt Gingrich (1995)
David Ho (1996)
Andrew Grove (1997)
Bill Clinton
Kenneth Starr (1998)
Jeff Bezos (1999)
George W. Bush (2000)

 
Depuis 2001

Rudy Giuliani (2001)
Les whistleblowers (2002)
Le soldat amÃ©ricain (2003)
George W. Bush (2004)
Les bons Samaritains (Bill Gates, Melinda Gates et Bono) (2005)
Â«Â VousÂ Â» (2006)
Vladimir Poutine (2007)
Barack Obama (2008)
Ben Bernanke (2009)
Mark Zuckerberg (2010)
Le protestataire (2011)
Barack Obama (2012)
Le pape FranÃ§ois (2013)
Les combattants d'Ebola (2014)
Angela Merkel (2015)
Donald Trump (2016)
Â«Â Silence BreakersÂ Â» (2017)
Les GardiensÂ : Jamal Khashoggi / Maria Ressa / Wa Lone et Kyaw Soe Oo / Personnel de The Capital (2018)
Greta Thunberg (2019)
Joe Biden et Kamala Harris (2020)
Elon Musk (2021)

 

 Portail de lâ€™informatique   Portail de lâ€™Ã©lectricitÃ© et de lâ€™Ã©lectronique   Portail des technologies   Portail de lâ€™Ã©dition numÃ©rique  




Ce document provient de Â«Â https://fr.wikipedia.org/w/index.php?title=Ordinateur&oldid=196221292Â Â».
CatÃ©goriesâ€¯: MatÃ©riel informatiqueMachine Ã  calculerPersonnalitÃ© de l'annÃ©e selon Time MagazineCatÃ©gories cachÃ©esâ€¯: Page en semi-protection longuePage utilisant le modÃ¨le Citation avec un retour ligneArticle Ã  rÃ©fÃ©rence nÃ©cessaireCatÃ©gorie Commons avec lien local identique sur WikidataPage utilisant P486Page utilisant P3471Page pointant vers des bases externesPage pointant vers des bases relatives Ã  la santÃ©Page utilisant P5357Page pointant vers des bases relatives Ã  la littÃ©raturePage utilisant P5019Page utilisant P1417Page utilisant P3219Page utilisant P3365Page utilisant P1296Page utilisant P3222Page utilisant P4342Page pointant vers des dictionnaires ou encyclopÃ©dies gÃ©nÃ©ralistesArticle de WikipÃ©dia avec notice d'autoritÃ©Article contenant un appel Ã  traduction en anglaisPortail:Informatique/Articles liÃ©sPortail:Technologies/Articles liÃ©sPortail:Sciences/Articles liÃ©sPortail:Ã‰lectricitÃ© et Ã©lectronique/Articles liÃ©sPortail:Ã‰dition numÃ©rique/Articles liÃ©sPortail:Ã‰dition/Articles liÃ©sPortail:Ã‰conomie/Articles liÃ©sArticle de qualitÃ© dans une autre langueArticle de qualitÃ© en bosnienBon article en tatarArticle de qualitÃ© en mongol






 La derniÃ¨re modification de cette page a Ã©tÃ© faite le 18 aoÃ»t 2022 Ã  16:07.
Droit d'auteur : les textes sont disponibles sous licence Creative Commons attribution, partage dans les mÃªmes conditions ; dâ€™autres conditions peuvent sâ€™appliquer. Voyez les conditions dâ€™utilisation pour plus de dÃ©tails, ainsi que les crÃ©dits graphiques. En cas de rÃ©utilisation des textes de cette page, voyez comment citer les auteurs et mentionner la licence.
WikipediaÂ® est une marque dÃ©posÃ©e de la Wikimedia Foundation, Inc., organisation de bienfaisance rÃ©gie par le paragraphe 501(c)(3) du code fiscal des Ã‰tats-Unis.


Politique de confidentialitÃ©
Ã€ propos de WikipÃ©dia
Avertissements
Contact
Version mobile
DÃ©veloppeurs
Statistiques
DÃ©claration sur les tÃ©moins (cookies)













