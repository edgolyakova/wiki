Ordinateur
Étymologie
Histoire
Évolution
Fonctionnement
Types d'ordinateurs
Notes et références
Voir aussi
Première apparition de l'ordinateur
Concept initial et réalisation
Calculatrices
Électromécanique et mécanographie
Années 1930
Premier ordinateur (1937-1946)
Tubes à vide et commutateurs (1946-1955)
Générations suivantes (1955-2000)
Généralités
UAL et UC
Mémoire
Entrées-Sorties
Bus
Architecture
Instructions
Logiciels
Par type de phénomène physique
Par type de traitement temporel
Par domaine d'application
Par taille
Par architecture
Bibliographie
Articles connexes
Liens externes
Mécanique
Électromécanique
Électronique
Quantique
Optique
Chimique (ou biologique)
Pages pour les contributeurs déconnectés en savoir plus
Sommaire
déplacer vers la barre latérale
masquer
Ne doit pas être confondu avec Appareil informatique ou Système de traitement de l'information.
Un ordinateur est un système de traitement de l'information programmable tel que défini par Alan Turing et qui fonctionne par la lecture séquentielle d'un ensemble d'instructions, organisées en programmes, qui lui font exécuter des opérations logiques et arithmétiques.
Sa structure physique actuelle fait que toutes les opérations reposent sur la logique binaire et sur des nombres formés à partir de chiffres binaires.
Dès sa mise sous tension, un ordinateur exécute, l'une après l'autre, des instructions qui lui font lire, manipuler, puis réécrire un ensemble de données déterminées par une mémoire morte d'amorçage.
Des tests et des sauts conditionnels permettent de passer à l'instruction suivante et donc d'agir différemment en fonction des données ou des nécessités du moment ou de l'environnement.
Les données à manipuler sont acquises soit par la lecture de mémoires, soit en provenance de périphériques internes ou externes (déplacement d'une souris, touche appuyée sur un clavier, déplacement d'un stylet sur une tablette, température et autres mesures physiques…). Une fois utilisés, ou manipulés, les résultats sont écrits soit dans des mémoires, soit dans des composants qui peuvent transformer une valeur binaire en une action physique (écriture sur une imprimante ou sur un moniteur, accélération ou freinage d'un véhicule, changement de température d'un four…). L'ordinateur peut aussi répondre à des interruptions qui lui permettent d’exécuter des programmes de réponses spécifiques à chacune, puis de reprendre l’exécution séquentielle du programme interrompu.
De 1834 à 1837, Charles Babbage conçoit une machine à calculer programmable en associant un des descendants de la Pascaline (première machine à calculer mécanique inventée par Blaise Pascal) avec des instructions écrites sur le même type de cartes perforées que celles inventées par Joseph Marie Jacquard pour ses métiers à tisser. C'est durant cette période qu'il imagine la plupart des caractéristiques de l'ordinateur moderne.
Babbage passe le reste de sa vie à essayer de construire sa machine analytique, mais sans succès. Nombre de personnes essayent de développer cette machine, mais c'est cent ans plus tard, en 1937, qu'IBM inaugure l'ère de l'informatique en commençant le développement de l'ASCC/Mark I, une machine construite sur l'architecture de Babbage qui, une fois réalisée, est considérée comme l'achèvement de son rêve.
La technique actuelle des ordinateurs date du milieu du xxe siècle.
Les ordinateurs peuvent être classés selon plusieurs critères tels que le domaine d'application, la taille ou l'architecture.
Le mot « ordinateur » fut introduit par IBM France en 1955, après que François Girard, alors responsable du service publicité de l'entreprise, eut l'idée de consulter son ancien professeur de lettres à Paris, Jacques Perret.
Avec Christian de Waldner, alors président d'IBM France, ils demandèrent au professeur Perret, de suggérer un « nom français pour sa nouvelle machine électronique destinée au traitement de l'information (IBM 650), en évitant d'utiliser la traduction littérale du mot anglais computer (« calculateur » ou « calculatrice »), qui était à cette époque plutôt réservé aux machines scientifiques ».
En 1911, une description de la machine analytique de Babbage utilisait le mot ordonnateur pour en décrire son organe moteur: « Pour aller prendre et reporter les nombres… et pour les soumettre à l’opération demandée, il faut qu'il y ait dans la machine un organe spécial et variable : c'est l'ordonnateur.
Cet ordonnateur est constitué simplement par des feuilles de carton ajourées, analogues à celle des métiers Jacquard… ».
Le professeur proposa un mot composé centré autour d'ordonnateur : celui qui met en ordre et qui avait aussi la notion d'ordre ecclésiastique dans l'église catholique (ordinant).
Il suggéra plus précisément « ordinatrice électronique », le féminin ayant pu permettre, selon lui, de mieux distinguer l'usage religieux de l'usage comptable du mot.
« IBM France retint le mot ordinateur et chercha au début à protéger ce nom comme une marque.
Mais le mot fut facilement et rapidement adopté par les utilisateurs et IBM France décida au bout de quelques mois de le laisser dans le domaine public. »
Selon Bernard Cohen, auteur de l'ouvrage intitulé Howard Aiken: Portrait of a computer pioneer, « les historiens des technologies et les informaticiens intéressés en histoire, ont adopté un certain nombre de caractéristiques qui définissent un ordinateur. C'est ainsi que la question de savoir si le Mark I était ou n'était pas un ordinateur ne dépend pas d'une opinion majoritaire mais plutôt de la définition utilisée. Souvent, quelques-unes des caractéristiques fondamentales nécessaires pour être considérées comme un ordinateur sont :
Une machine n'est généralement pas classifiée comme un ordinateur à moins qu'elle n'ait des caractéristiques supplémentaires comme la possibilité d’exécuter des opérations spécifiques automatiquement et ceci d'une façon contrôlée et dans une séquence prédéterminée. Pour d'autres historiens et informaticiens, il faut aussi que la machine ait été vraiment construite et qu'elle ait été complètement opérationnelle. »
Sans une définition stricte il est impossible d'identifier la machine qui devint le premier ordinateur, mais il faut remarquer certaines des étapes fondamentales qui vont du développement du concept de la machine à calculer programmable par Charles Babbage en 1837 au premier développement de l'ère de l'informatique cent ans plus tard.
En 1834, Charles Babbage commence à développer une machine à calculer programmable, sa machine analytique.
Il pense la programmer grâce à un cylindre à picots comme dans les automates de Vaucanson, mais, deux ans plus tard, il remplace ce cylindre par la lecture de cartes Jacquard, et ainsi crée une machine à calculer infiniment programmable.
En 1843, Ada Lovelace écrit le premier programme informatique pour calculer les nombres de Bernoulli, pour la machine analytique qui ne sera jamais construite.
Henry Babbage construit une version extrêmement simplifiée de l'unité centrale de la « machine analytique » de son père et l'utilise en 1906, pour calculer et imprimer automatiquement les quarante premiers multiples du nombre Pi avec une précision de vingt-neuf décimales, démontrant sans ambiguïté que le principe de la machine analytique était viable et réalisable.
En 1886, sa plus grande contribution fut de donner un ensemble mécanique de démonstration d'une des machines de son père à l'université Harvard. C'est cinquante ans plus tard, après avoir entendu la présentation de Howard Aiken sur son super calculateur, qu'un technicien de Harvard, Carmello Lanza, lui fit savoir qu'une machine similaire avait déjà été développée et qu'il lui montra l'ensemble mécanique de démonstration donné par Henry Babbage qui se trouvait dans un des greniers de l'université ; c'est ainsi qu'il découvrit les travaux de Babbage et qu'il les incorpora dans la machine qu'il présenta à IBM en 1937. C'était la troisième fois qu'il essayait de trouver un sponsor pour le développement de sa machine car son projet avait déjà été rejeté deux fois avant l'intégration des travaux de Babbage dans l'architecture de sa machine (une fois par la Monroe Calculating Company et une fois par l'université Harvard).
Leonardo Torres Quevedo remplaça toutes les fonctions mécaniques de Babbage par des fonctions électromécaniques (addition, soustraction, multiplication et division mais aussi la lecture de cartes et les mémoires).
En 1914 et en 1920, Il construisit deux machines analytiques, non programmable, extrêmement simplifiées mais qui montraient que des relais électromécaniques pouvaient être utilisés dans une machine à calculer qu'elle soit programmable ou non.
Sa machine de 1914 avait une petite mémoire électromécanique et son arithmomètre de 1920, qu'il développa pour célébrer le centième anniversaire de l'invention de l'arithmomètre, était commandé par une machine à écrire qui était aussi utilisée pour imprimer ses résultats.
Percy Ludgate améliora et simplifia les fonctions mécaniques de Babbage mais ne construisit pas de machine.
Et enfin, Louis Couffignal essaya au début des années 1930, de construire une machine analytique « purement mécanique, comme celle de Babbage, mais sensiblement plus simple », mais sans succès. C'est cent ans après la conceptualisation de l'ordinateur par Charles Babbage que le premier projet basé sur l'architecture de sa machine analytique aboutira.
En effet, c'est en 1937 qu'Howard Aiken présenta à IBM un projet de machine à calculer programmable qui sera le premier projet qui finira par une machine qui puisse être, et qui sera utilisée, et dont les caractéristiques en font presque un ordinateur moderne.
Et donc, bien que le premier ordinateur ne sera jamais déterminé à l’unanimité, le début de l'ère de l'informatique moderne peut être considéré comme la présentation d'Aiken à IBM, en 1937, qui aboutira par l'ASCC.
Les machines à calculer jouèrent un rôle primordial dans le développement des ordinateurs pour deux raisons tout à fait indépendantes. D'une part, pour leurs origines : c'est pendant le développement d'une machine à calculer automatique à imprimante qu'en 1834 Charles Babbage commença à imaginer sa machine analytique, l’ancêtre des ordinateurs. C’était une machine à calculer programmée par la lecture de cartes perforées (inspirées du Métier Jacquard), avec un lecteur de cartes pour les données et un pour les programmes, avec des mémoires, un calculateur central et des imprimantes et qui inspirera le développement des premiers ordinateurs à partir de 1937 ; ce qui nous amènera aux mainframes des années 1960.
D'autre part, leur propagation se fit grâce à la commercialisation en 1971 du premier microprocesseur, l'Intel 4004, qui fut inventé pendant le développement d'une machine à calculer électronique pour la compagnie japonaise Busicom, qui est à l'origine de l'explosion de la micro-informatique à partir de 1975 et qui réside au cœur de tous les ordinateurs actuels quelles que soient leurs tailles ou fonctions (bien que seulement 2 % des microprocesseurs produits chaque année soient utilisés comme unités centrales d'ordinateur, les 98 % restant sont utilisés dans la construction de voitures, de robots ménagers, de montres, de caméras de surveillance…).
Outre les avancées observées dans l'industrie du textile et celles de l'électronique, les avancées de la mécanographie à la fin du XIXe siècle, pour achever les recensements aux États-Unis, la mécanisation de la cryptographie au début du XXe siècle, pour chiffrer puis déchiffrer automatiquement des messages, le développement des réseaux téléphoniques (à base de relais électromécaniques), sont aussi à prendre en compte pour comprendre l'avènement de ce nouveau genre de machine qui ne calculent pas (comme font/faisaient les calculatrices), mais lisent et interprètent des programmes qui -eux- calculent.
Pour le monde des idées, avant l'invention de ces nouvelles machines, l'élément fondateur de la science informatique est en 1936, la publication de l'article On Computable Numbers with an Application to the Entscheidungsproblem par Alan Turing qui allait déplacer le centre de préoccupation de certains scientifiques (mathématiciens et logiciens) de l'époque, du sujet de la calculabilité (ou décidabilité) ouvert par Hilbert, malmené par Godël, éclairci par Church, vers le sujet de la mécanisation du calcul (ou calculabilité effective).
Dans ce texte de 36 pages, Turing expose une machine théorique capable d'effectuer tout calcul ; il démontre que cette machine est aussi puissante, au niveau du calcul, que tout être humain.
Autrement dit, un problème mathématique possède une solution, si et seulement si, il existe une machine de Turing capable de résoudre ce problème.
Par la suite, il expose une machine de Turing universelle apte à reproduire toute machine de Turing, il s'agit des concepts d'ordinateur, de programmation et de programme.
Il termine en démontrant qu'il existe au moins un problème mathématique formellement insoluble, le problème de l'arrêt.
Peu avant la Seconde Guerre mondiale, apparurent les premières calculatrices électromécaniques, construites selon les idées d'Alan Turing.
Les machines furent vite supplantées par les premiers calculateurs électroniques, nettement plus performants.
La fin des années 1930 virent, pour la première fois dans l'histoire de l'informatique, le début de la construction de deux machines à calculer programmables.
Elles utilisaient des relais et étaient programmées par la lecture de rouleaux perforés et donc, pour certains, étaient déjà des ordinateurs.
Elles ne furent mises en service qu'au début des années 1940, faisant ainsi de 1940 la première décennie dans laquelle on trouve des ordinateurs et des machines à calculer programmables totalement fonctionnels. C'est d'abord en 1937 que Howard Aiken, qui avait réalisé que la machine analytique de Babbage était le type de machine à calculer qu'il voulait développer, proposa à IBM de la créer et de la construire ; après une étude de faisabilité, Thomas J. Watson accepta de la construire en 1939 ; elle fut testée en 1943 dans les locaux d'IBM et fut donnée et déménagée à l'université Harvard en 1944, changeant son nom de ASCC à Harvard Mark I ou Mark I.
Mais c'est aussi Konrad Zuse qui commença le développement de son Zuse 3, en secret, en 1939, et qui le finira en 1941. Parce que le Zuse 3 resta inconnu du grand public jusqu’après la fin de la Seconde Guerre mondiale (sauf des services secrets américains qui le détruisirent dans un bombardement en 1943), ses solutions très inventives ne furent pas utilisées dans les efforts communs mondiaux de développement de l’ordinateur.
Six machines furent construites durant ces 9 ans.
Elles furent toutes décrites, au moins une fois, dans la multitude de livres de l'histoire de l'informatique, comme étant le premier ordinateur ; aucune autre machine, construite ultérieurement, ne fut décrite comme telle.
Ces six précurseurs peuvent être divisées en trois groupes bien spécifiques :
« Sans un branchement conditionnel, et donc l’implémentation mécanique du mot SI, le plus grand des calculateurs ne serait qu'une super machine à calculer.
Il pourrait être comparé à une ligne d'assemblage, tout étant organisé du début à la fin, avec aucune possibilité de changement une fois que la machine est mise en marche. »
— Andrew Hodges, Alan Turing: the enigma, 1983.
« L'ENIAC et le Colosse étaient comme deux kits à assembler, desquelles beaucoup de machines similaires, mais différentes, pouvaient être construites.
Aucun n’essaya d’implémenter l'universalité de la « machine de Babbage » dans laquelle la machine n'est jamais modifiée, et où seulement les instructions sont réécrites sur des cartes perforées. »
— Andrew Hodges, Alan Turing: the enigma, 1983.
De ces six machines, seulement quatre furent connues de leurs contemporains, les deux autres, le Colosse et le Z3, utilisées dans l'effort de guerre, ne furent découvertes qu'après la fin de la Seconde Guerre mondiale, et donc ne participèrent pas au développement communautaire mondial des ordinateurs.
Seulement deux de ces machines furent utilisées dans les années 1950, l'ASCC/Mark I et l'ENIAC, et chacune fut éventuellement modifiée pour en faire une machine Turing-complet.
En juin 1945 est publié un article fondateur de John von Neumann donnant les bases de l'architecture utilisée dans la quasi-totalité des ordinateurs depuis lors.
Dans cet article, von Neumann veut concevoir un programme enregistré et programmé dans la machine.
La première machine correspondant à cette architecture, dite depuis architecture de von Neumann est une machine expérimentale la Small-Scale Experimental Machine (SSEM ou baby) construite à Manchester en juillet 1948. En août 1949 la première machine fonctionnelle, fondée sur les bases de von Neumann fut l'EDVAC.
Cette chronologie demande qu'un ordinateur soit électronique et donc elle commence, en 1946, avec l'ENIAC qui, au départ, était programmé avec des interrupteurs et par le positionnement de fils sur un commutateur, comme sur un ancien standard téléphonique.
Les ordinateurs de cette période sont énormes avec des dizaines de milliers de tubes à vide. L'ENIAC faisait 30 m de long, 2,40 m de haut et pesait 30 tonnes.
Ces machines n’étaient pas du tout fiables, par exemple, en 1952, dix-neuf mille tubes furent remplacés sur l'ENIAC, soit plus de tubes qu'il n'en contient.
« L'ENIAC prouva, sans ambiguïté, que les principes de base de l'électronique était bien fondés. Il était vraiment inévitable que d'autres machines à calculer de ce type seraient perfectionnées grâce aux connaissances et à l’expérience acquises sur cette première. »
De nouveau, le titre de premier ordinateur commercialisé dépend de la définition utilisée ; trois ordinateurs sont souvent cités. En premier, le BINAC, conçu par la Eckert–Mauchly Computer Corporation et livré à la Northrop Corporation en 1949 qui, après sa livraison, ne fut jamais fonctionnel,. En deuxième, le Ferranti Mark I, dont le prototype avait été développé par l'université de Manchester, fut amélioré et construit en un exemplaire par la société Ferranti et revendu à l'université de Manchester en février 1951. Et en dernier, UNIVAC I, conçu par la « Eckert–Mauchly Computer Corporation », dont le premier fut vendu à l'United States Census Bureau le 30 mars 1951. Une vingtaine de machines furent produites et vendues entre 1951 et 1954.
« L'utilisation de transistors au milieu des années 1950 changea le jeu complètement.
Les ordinateurs devinrent assez fiables pour être vendus à des clients payants sachant qu'ils fonctionneraient assez longtemps pour faire du bon travail. » Les circuits intégrés réduisirent la taille et le prix des ordinateurs considérablement.
Les moyennes entreprises pouvaient maintenant acheter ce genre de machines.
Les circuits intégrés permettent de concevoir une informatique plus décentralisée les constructeurs souhaitant concurrencer le géant IBM.
Le microprocesseur fut inventé en 1969 par Ted Hoff d'Intel pendant le développement d'une calculatrice pour la firme japonaise Busicom.
Intel commercialisera le 4004 fin 1971. Ted Hoff avait copié l'architecture du PDP-8, le premier mini-ordinateur, et c'est grâce à la technologie de circuits intégrés LSI (large scale integration), qui permettait de mettre quelques milliers de transistors sur une puce qu'il put miniaturiser les fonctions d'un ordinateur en un seul circuit intégré. La fonction première du microprocesseur était de contrôler son environnement.
Il lisait des interrupteurs, les touches d'un clavier et il agissait en exécutant les opérations requises (addition, multiplication, etc.) et en affichant les résultats.
Le premier ordinateur personnel fut décrit dans le livre d'Edmund Berkeley, Giant brain, or machines that think, en 1949, et sa construction fut décrite dans une série d'articles du magazine Radio-Electronics à partir du numéro d'octobre 1950. En 1972, une société française développe le Micral, premier micro-ordinateur à être basé sur le microprocesseur 8008. Mais l’ordinateur qui créa l'industrie de l'ordinateur personnel est l'Altair 8800, qui fut décrit pour la première fois dans le magazine Radio-Electronics de janvier 1975. Bill Gates, Paul Allen, Steve Wozniak et Steve Jobs (ordre chronologique) firent tous leurs débuts dans la micro-informatique sur ce produit moins de six mois après son introduction.
Les ordinateurs furent d'abord utilisés pour le calcul (en nombres entiers d'abord, puis flottants).
On ne peut cependant les assimiler à de simples calculateurs, du fait de la possibilité quasi infinie de lancer d'autres programmes en fonction du résultat de calculs, ou de capteurs internes ou externes (température, inclinaison, orientation, etc.), ou de toute action de l'opérateur ou de son environnement.
Cette création d'un néologisme fut à l'origine de traductions multiples des expressions supercomputer, superordinateur ou supercalculateur.
L'expérience a appris à distinguer dans un ordinateur deux aspects, dont le second avait été au départ sous-estimé :
Un ordinateur très avancé techniquement pour son époque comme le Gamma 60 de la compagnie Bull n'eut pas le succès attendu, pour la simple raison qu'il existait peu de moyens de mettre en œuvre commodément ses possibilités techniques[réf. nécessaire].
Le logiciel — et son complément les services (formation, maintenance…) — forme depuis le milieu des années 1980 l’essentiel des coûts d'équipement informatique, le matériel n’y ayant qu'une part minoritaire.
Les ordinateurs peuvent être sensibles aux bombes IEM.[réf. nécessaire]
Parmi toutes les machines inventées par l'Homme, l'ordinateur est celle qui se rapproche le plus du concept anthropologique suivant : Organe d'entrée, organe de traitement de l'information et organe de sortie.
Chez l'humain, les organes d'entrée sont les organes sensoriels, l'organe de traitement est le cerveau dont les logiciels sont l'apprentissage avec des mises à jour constantes en cours de vie, puis les organes de sortie sont les muscles.
Pour les ordinateurs modernes, les organes d'entrée sont le clavier et la souris et les organes de sortie, l'écran, l'imprimante, le graveur de DVD, etc.
Les techniques utilisées pour fabriquer ces machines ont énormément changé depuis les années 1940 et sont devenues une technologie (c’est-à-dire un ensemble industriel organisé autour de techniques) à part entière depuis les années 1970. Beaucoup utilisent encore les concepts définis par John von Neumann, bien que cette architecture soit en régression : les programmes ne se modifient plus guère eux-mêmes (ce qui serait considéré comme une mauvaise pratique de programmation), et le matériel prend en compte cette nouvelle donne en séparant aujourd'hui nettement le stockage des instructions et des données, y compris dans les caches.
L’architecture de von Neumann décomposait l’ordinateur en quatre parties distinctes :
L’unité arithmétique et logique ou UAL est l’élément qui réalise les opérations élémentaires (additions, soustractions…), les opérateurs logiques (ET, OU, NI, etc.) et les opérations de comparaison (par exemple la comparaison d’égalité entre deux zones de mémoire). C’est l’UAL qui effectue les calculs de l’ordinateur. L’unité de contrôle prend ses instructions dans la mémoire.
Celles-ci lui indiquent ce qu’elle doit ordonner à l’UAL et, comment elle devra éventuellement agir selon les résultats que celle-ci lui fournira.
Une fois l’opération terminée, l’unité de contrôle passe soit à l’instruction suivante, soit à une autre instruction à laquelle le programme lui ordonne de se brancher.
L'unité de contrôle facilite la communication entre l'unité arithmétique et logique, la mémoire ainsi que les périphériques.
Elle gère la plupart des exécutions des instructions dans l'ordinateur.
Au sein du système, la mémoire peut être décrite comme une suite de cellules numérotées contenant chacune une petite quantité d’informations.
Cette information peut servir à indiquer à l’ordinateur ce qu’il doit faire (instructions) ou contenir des données à traiter.
Dans la plupart des architectures, c'est la même mémoire qui est utilisée pour les deux fonctions.
Dans les calculateurs massivement parallèles, on admet même que des instructions de programmes soient substituées à d’autres en cours d’opération lorsque cela se traduit par une plus grande efficacité. Cette pratique était jadis courante, mais les impératifs de lisibilité du génie logiciel l'ont fait régresser, hormis dans ce cas particulier, depuis plusieurs décennies.
Cette mémoire peut être réécrite autant de fois que nécessaire.
La taille de chacun des blocs de mémoire ainsi que la technologie utilisée ont varié selon les coûts et les besoins : 8 bits pour les télécommunications, 12 bits pour l’instrumentation (DEC) et 60 bits pour de gros calculateurs scientifiques (Control Data).
Un consensus a fini par être trouvé autour de l’octet comme unité adressable et d’instructions sur format de 4 ou 8 octets.
Dans tous les cas de figure, l'octet reste adressable, ce qui simplifie l'écriture des programmes.
Les techniques utilisées pour la réalisation des mémoires ont compris des relais électromécaniques, des tubes au mercure au sein desquels étaient générées des ondes acoustiques, des transistors individuels, des tores de ferrite et enfin des circuits intégrés incluant des millions de transistors.
Les dispositifs d’entrée/sortie permettent à l’ordinateur de communiquer avec l’extérieur.
Ces dispositifs sont très importants, du clavier à l’écran.
La carte réseau permet par exemple de relier les ordinateurs en réseau informatique, dont le plus grand est Internet.
Le point commun entre tous les périphériques d’entrée est qu’ils convertissent l’information qu’ils récupèrent de l’extérieur en données compréhensibles par l’ordinateur. À l’inverse, les périphériques de sortie décodent l’information fournie par l’ordinateur afin de la rendre compréhensible par l’utilisateur.
Ces différentes parties sont reliées par trois bus, le bus d'adresse, le bus de données et le bus de contrôle.
Un bus est un groupement d'un certain nombre de fils électriques réalisant une liaison pour transporter des informations binaires codées sur plusieurs bits.
Le bus d'adresse transporte les adresses générées par l'UCT (Unité Centrale de Traitement) pour sélectionner une case mémoire ou un registre interne de l'un des blocs.
Le nombre de bits véhiculés par ce bus dépend de la quantité de mémoire qui doit être adressée. Le bus de données transporte les données échangées entre les différents éléments du système.
Le bus de contrôle transporte les différents signaux de synchronisation nécessaires au fonctionnement du système : signal de lecture (RD), signal d'écriture (WR), signal de sélection (CS : Chip Select).
La miniaturisation permet d’intégrer l’UAL et l’unité de contrôle au sein d’un même circuit intégré connu sous le nom de microprocesseur.
Typiquement, la mémoire est située sur des circuits intégrés proches du processeur, une partie de cette mémoire, la mémoire cache, pouvant être située sur le même circuit intégré que l’UAL.
L’ensemble est, sur la plupart des architectures, complété d’une horloge qui cadence le processeur.
Bien sûr, on souhaite qu'elle soit le plus rapide possible, mais on ne peut pas augmenter sans limites sa vitesse pour deux raisons :
La tendance a été à partir de 2004 de regrouper plusieurs UAL dans le même processeur, voire plusieurs processeurs dans la même puce.
En effet, la miniaturisation progressive (voir Loi de Moore) le permet sans grand changement de coût. Une autre tendance, depuis 2006 chez ARM, est aux microprocesseurs sans horloge : la moitié de la dissipation thermique est en effet due aux signaux d'horloge quand le microprocesseur fonctionne ; de plus, un microprocesseur sans horloge a une consommation presque nulle quand il ne fonctionne pas : le seul signal d'horloge nécessaire est alors celui destiné au rafraîchissement des mémoires.
Cet atout est important pour les modèles portables.
Le principal écart fonctionnel aujourd’hui par rapport au modèle de von Neumann est la présence sur certaines architectures de deux antémémoires différentes : une pour les instructions et une pour les données (alors que le modèle de von Neumann spécifiait une mémoire commune pour les deux).
La raison de cet écart est que la modification par un programme de ses propres instructions est aujourd’hui considérée (sauf sur les machines hautement parallèles) comme une pratique à proscrire.
Dès lors, si le contenu du cache de données doit être récrit en mémoire principale quand il est modifié, on sait que celui du cache d’instructions n’aura jamais à l’être, d’où simplification des circuits et gain de performance.
Les instructions que l’ordinateur peut comprendre ne sont pas celles du langage humain.
Le matériel sait juste exécuter un nombre limité d’instructions bien définies.
Des instructions typiques comprises par un ordinateur sont par exemple :
La plupart des instructions se composent de deux zones : l’une indiquant quoi faire, nommée code opération, et l’autre indiquant où le faire, nommée opérande.
Au sein de l’ordinateur, les instructions correspondent à des codes — le code pour une copie étant par exemple 001. L’ensemble d’instructions qu’un ordinateur supporte se nomme son langage machine, langage qui est une succession de chiffres binaires, car les instructions et données qui sont comprises par le processeur (CPU) sont constituées uniquement de 0 (zéro) et de 1 (un) :
En général, ce type de langage n'est pas utilisé car on lui préfère ce que l’on appelle un langage de haut niveau qui est ensuite transformé en langage binaire par un programme spécial (interpréteur ou compilateur selon les besoins).
Les programmes ainsi obtenus sont des programmes compilés compréhensibles par l'ordinateur dans son langage natif.
Certains langages de programmation, comme l’assembleur sont dits langages de bas niveau car les instructions qu’ils utilisent sont très proches de celles de l’ordinateur.
Les programmes écrits dans ces langages sont ainsi très dépendants de la plate-forme pour laquelle ils ont été développés. Le langage C, beaucoup plus facile à relire que l’assembleur, permet de produire plus facilement des programmes.
Pour cette raison, on l’a vu de plus en plus utilisé à mesure que les coûts du matériel diminuaient et que les salaires horaires des programmeurs augmentaient[réf. nécessaire].
Les logiciels informatiques sont des listes (généralement longues) d’instructions exécutables par un ordinateur.
De nombreux programmes contiennent des millions d’instructions, effectuées pour certaines de manière répétitive.
De nos jours, un ordinateur personnel exécute plusieurs milliards d’instructions par seconde.
Depuis le milieu des années 1960, des ordinateurs exécutent plusieurs programmes simultanément.
Cette possibilité est appelée multitâche. C’est le cas de tous les ordinateurs modernes.
En réalité, chaque cœur de processeur n’exécute qu’un programme à la fois, passant d’un programme à l’autre chaque fois que nécessaire.
Si la rapidité du processeur est suffisamment grande par rapport au nombre de tâches à exécuter, l’utilisateur aura l’impression d’une exécution simultanée des programmes.
Les priorités associées aux différents programmes sont, en général, gérées par le système d'exploitation.
Le système d’exploitation est le programme central qui contient les programmes de base nécessaires au bon fonctionnement des applications de l’ordinateur.
Le système d’exploitation alloue les ressources physiques de l’ordinateur (temps processeur, mémoire…) aux différents programmes en cours d’exécution.
Il fournit aussi des outils aux logiciels (comme les pilotes) afin de leur faciliter l’utilisation des différents périphériques sans avoir à en connaître les détails physiques.
IBM 370 .
HP 2116 .
Serveur VAX .
Bull-Micral p. 2 français en 1981.
IBM PC 5150 en 1983.
Superordinateur Columbia de la NASA en 2004.
Acer Aspire 8920 .
L'ordinateur mécanique se base sur des composants mécanique pour effectuer les calculs (engrenages, etc.)
Les ordinateurs électromécanique utilisent à la fois du courant électrique et des mécanismes mécaniques pour le calcul (relais électromécaniques)
Les ordinateurs électroniques utilisent des électrons pour réaliser les différentes fonctions de l'architecture d'un ordinateur.
C'est le phénomène physique sous-jacent de nos ordinateurs actuels.
Les ordinateurs quantiques utilisent les propriétés quantiques de la matière.
Les ordinateurs optiques utilisent des photons pour le traitement des informations.
« La machine d'arithmétique fait des effets qui approchent plus de la pensée que tout ce que font les animaux ; mais elle ne fait rien qui puisse faire dire qu'elle a de la volonté, comme les animaux. »
« …in less than two years he had sketched out many of the salient features of the modern computer.
A crucial step was the adoption of a punched card system derived from the Jacquard loom. »
Sur les autres projets Wikimedia :
