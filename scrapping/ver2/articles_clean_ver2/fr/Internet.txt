Internet
Terminologie
Historique
Gouvernance
Aspects juridiques
Technique
Considérations sociales
Arts et littérature
Notes et références
Voir aussi
En France
Au Québec
Années 1960
Années 1970
Années 1980
Années 1990
Depuis 2000
Au niveau européen
Neutralité du réseau
Libre circulation de l'information
Règles de droit applicables au réseau
Règlement général sur la protection des données
Connexions grand public
Centre de données
Infrastructures matérielles
Protocoles logiciels
Impact écologique de l'infrastructure
Statistiques
Un bouleversement social
Internet comme outil de mobilisation
Fracture numérique
Notes
Références
Filmographie
Bibliographie
Articles connexes
Liens externes
Pages pour les contributeurs déconnectés en savoir plus
Sommaire
déplacer vers la barre latérale
masquer
Ne doit pas être confondu avec World Wide Web.
Internet est un réseau informatique mondial accessible au public.
Il s'agit d'un réseau de réseaux, à commutation de paquets, sans centre névralgique, composé de millions de réseaux aussi bien publics que privés, universitaires, commerciaux et gouvernementaux, eux-mêmes regroupés en réseaux autonomes ; il en existe plus de 91 000 en 2019. L'information est transmise via Internet grâce à un ensemble standardisé de protocoles de transfert de données, qui permet des applications variées comme le courrier électronique, le World Wide Web, la messagerie instantanée, le partage de fichiers en pair-à-pair, le streaming, le podcasting, la téléconférence.
Dans les années 1990, l'apparition du Web contribue à rendre Internet accessible au grand public.
Depuis les années 2010, un nombre croissant de types d'objets divers y sont connectés, formant l'Internet des objets.
Un internaute est une personne qui utilise un accès à Internet.
Cet accès peut être obtenu grâce à un fournisseur d'accès à travers divers moyens de communication électronique : soit filaire (réseau téléphonique commuté à bas débit, ADSL, fibre optique jusqu'au domicile), soit sans fil (WiMAX, par satellite, GPRS, EDGE, 3G, 3G+, 4G, 4G+, 5G ou 5G+).
Le terme américain « Internet » est dérivé du concept d'internetting, en français : « interconnecter des réseaux », dont la première utilisation documentée remonte à octobre 1972 par Robert Elliot Kahn, dans le cadre de la première International Conference on Computer Communications (ICCC) à Washington.
Les origines exactes du terme restent à déterminer.
Le 1er janvier 1983, le nom « Internet », déjà en usage pour désigner l'ensemble d'ARPANET et de plusieurs réseaux informatiques, devient officiel.
En anglais, « un internet » (nom commun, sans majuscule) est un terme technique désignant un réseau constitué de l'interconnexion de plusieurs réseaux informatiques au moyen de routeurs. Écrire « Internet » avec une majuscule permet alors de distinguer le réseau global d'un quelconque autre réseau de réseaux. L'usage de la minuscule devient toutefois majoritaire vers 2015. En 2016, l'agence Associated Press adopte la minuscule dans son Stylebook, qui fait office de « bible orthotypographique » de la presse anglo-saxonne.
En français, une controverse porte sur l'usage ou non d'une majuscule (« Internet » ou « internet ») et sur l'usage d'un article défini (« l'Internet » ou « Internet »). Dans l'usage courant, l'article est très peu employé.
La Commission générale de terminologie et de néologie indique qu'il faut utiliser le mot « internet » comme un nom commun, c'est-à-dire sans majuscule.
Dans son dictionnaire, l'Académie française donne un exemple utilisant la forme « l'internet ».
L'Office québécois de la langue française recommande d'utiliser une majuscule, car le terme « est considéré comme un nom propre qui désigne une réalité unique ». De nombreux correcteurs orthographiques intégrés aux logiciels francophones utilisent la majuscule (Microsoft Office, Mozilla Firefox…).
Enfin, certains, comme Frédéric Martel, estiment qu'il faudrait aller plus loin et dire « les internets » (au pluriel et avec une minuscule) en raison du fait qu'Internet « est partout différent ».
Le débat se poursuit, en France comme dans d’autres pays.
En 1934, Paul Otlet décrit dans son Traité de documentation une vision prémonitoire de l'avènement d'Internet.
Au début des années 1960, J.C.R. Licklider du Massachusetts Institute of Technology (MIT) décrivit pour la première fois les interactions sociales possibles avec un réseau d'ordinateurs,.
En 1961, Leonard Kleinrock, également du MIT, publia le premier texte théorique sur la commutation de paquets.
En octobre 1962, Licklider fut le premier chef du programme de recherche en informatique de la Defense Advanced Research Projects Agency (DARPA), organisme de recherche pour la défense américaine.
Il persuada ses successeurs Ivan Sutherland, Bob Taylor et le chercheur du MIT Lawrence G. Roberts de l'intérêt des réseaux informatiques.
En 1964, Leonard Kleinrock publia le premier livre sur le sujet.
En 1965, Roberts testa avec Thomas Merrill la première connexion informatique à longue distance, entre le Massachusetts et la Californie.
Le résultat montra que des ordinateurs pouvaient travailler ensemble à distance, mais que le mode de communication par commutation de circuit du système téléphonique était inadapté. Le concept de communication par commutation de paquets de Kleinrock s'imposa.
En 1966, Roberts fut engagé par Taylor à la DARPA pour concevoir ARPANET.
Il publia les plans en 1967. En présentant ce texte, il découvrit deux autres groupes de chercheurs travaillant indépendamment sur le même sujet : un groupe du National Physical Laboratory (NPL) du Royaume-Uni avec Donald Davies et Roger Scantlebury, et un groupe de la RAND Corporation avec Paul Baran.
Entre 1962 et 1965, le groupe de la RAND avait étudié la transmission par paquets pour l'armée américaine.
Son but était de pouvoir maintenir les télécommunications en cas d'attaque (éventuellement nucléaire), ce que permet une transmission par paquets dans un réseau non centralisé. Il s'agissait d'un développement indépendant d'ARPANET : bien que probablement robuste face à une telle attaque, ARPANET n'avait pourtant été conçu que pour faciliter les télécommunications entre chercheurs.
Le rapport de Paul Baran resta purement théorique, et tomba rapidement dans l'oubli.
Mais le mythe de l'« ARPANET comme dernier rempart à une attaque atomique » trouve là son origine.
Pendant ce temps, au British National Physical Laboratory, l'équipe de Donald Davies, avait progressé : NPL Network, le premier réseau maillé fondé sur la transmission de datagrammes (packets) était fonctionnel. L'histoire d'Internet ne retint pourtant pas une origine européenne : ARPANET serait désormais l'origine officielle du réseau.
En août 1968, la DARPA accepta de financer le développement du matériel de routage des paquets d'ARPANET.
Ce développement fut confié en décembre à un groupe de la firme Bolt, Beranek and Newman (BBN) de Boston.
Ce dernier travailla avec Bob Kahn sur l'architecture du réseau.
Roberts améliorait les aspects topologiques et économiques du réseau.
Kleinrock préparait des systèmes de mesures du réseau.
Le 20 septembre 1969, BBN installa le premier équipement à l'université de Californie à Los Angeles (UCLA), où travaillait Kleinrock.
Le second nœud du réseau fut installé au Stanford Research Institute (SRI) où travaillait Douglas Engelbart sur un projet d'hypertexte.
Deux nœuds supplémentaires furent ajoutés avec l'université de Santa Barbara et l'université d'Utah.
Fin 1969, ARPANET comptait donc quatre nœuds.
Le Network Working Group (NWG) conduit par Steve Crocker finit le protocole de communication poste-à-poste NCP en décembre 1970. Ce protocole fut adopté entre 1971 et 1972 par les sites branchés à ARPANET.
Ceci permit le développement d'applications par les utilisateurs du réseau.
La perspective d'une informatique plus décentralisée commence à intéresser les constructeurs souhaitant rivaliser avec le géant IBM.
En 1972, Ray Tomlinson mit au point la première application importante : le courrier électronique.
En octobre 1972, Kahn organisa la première démonstration à grande échelle d'ARPANET à l'International Computer Communication Conference (ICCC). C'était la première démonstration publique.
Le concept d'Internet est né d'ARPANET. L'idée était de permettre la connexion entre des réseaux divers : ARPANET, des communications avec les satellites, des communications par radio.
Cette idée fut introduite par Kahn en 1972 sous le nom de Internetting.
Le protocole NCP d'ARPANET ne permettait pas d'adresser des hôtes hors d'ARPANET ni de corriger d'éventuelles erreurs de transmission.
Kahn décida donc de développer un nouveau protocole, qui devint finalement TCP/IP.
En parallèle, un projet inspiré par ARPANET était dirigé en France par Louis Pouzin : le projet Cyclades.
De nombreuses propriétés de TCP/IP ont ainsi été développées, plus tôt, pour Cyclades.
Pouzin et Kahn indiquent que TCP/IP a été inspiré par le réseau Cyclades français, poussé par la CII et sa distributed system architecture : on commence à parler de calcul distribué. Aux États-Unis, IBM et DEC créent les architectures SNA et DECnet, en profitant de la numérisation du réseau d'AT&T (Réseau téléphonique commuté).
En 1973, Kahn demanda à Vint Cerf (parfois appelé le « père d'Internet ») de travailler avec lui, car Cerf connaissait les détails de mise en œuvre de NCP.
Le premier document faisant référence à TCP fut écrit en 1973 par Cerf : A Partial Specification of an International Transmission Protocol.
La première spécification formelle de TCP, rédigée en décembre 1974, fut le RFC 675.
La version initiale de TCP ne permettait que la communication en établissant un circuit virtuel.
Cela fonctionnait bien pour le transfert de fichiers ou le travail à distance, mais n'était pas adapté à des applications comme la téléphonie par Internet.
TCP fut donc séparé de IP et UDP proposé pour les transmissions sans établissement d'un circuit.
En septembre 1981, le protocole de communication TCP/IP version 4 est décrit dans les Request for comments (RFC) 791, 792 et 793. Il est installé en 1983 sur ARPANET.
En 1983, les spécifications du système de noms de domaine sont publiées dans les RFC 882 et 883. En 1986, la National Science Foundation (NSF) crée NSFNET avec une dorsale Internet de 56 kilobits par seconde, qui passe à 1,5 mégabits par seconde en 1988.
Dans les années 1980 la guerre des protocoles[Quoi ?] n'est pas éteinte et l'Europe préfère le modèle OSI, ce qui freine l'adoption de TCP/IP pour les connexions distantes.
Le CERN participe à l'introduction des techniques liées à Internet en Europe, par la mise en service de deux routeurs Cisco en 1987, qui sont vraisemblablement les premiers introduits sur le continent européen.
Le CERN n'est connecté à Internet qu'en 1989.
Le début des années 1990 marque la naissance de l'application la plus connue d'Internet aujourd'hui : le World Wide Web, un ensemble de pages en HTML mélangeant du texte, des hyperliens, des images, adressables via une URL (ou adresse web) et accessibles via le protocole HTTP.
Ces standards, développés au CERN par Tim Berners-Lee et Robert Cailliau, deviennent rapidement populaires grâce au développement au NCSA par Marc Andreessen et Eric Bina du premier navigateur multimédia Mosaic.
L'interconnexion de réseaux d'opérateurs indépendants qui utilisent les standards d'Internet s'organise à partir de 1991. La coentreprise Commercial Internet Exchange (CIX) est créée à cet effet, initialement pour les réseaux PSINet, CERFNet et Alternet.
Ceci marque le début de l'Internet commercial mondial et ouvert à tous.
Le premier site web est mis en service en 1991 et le 30 avril 1993 marque le passage officiel du World Wide Web dans le domaine public.
En janvier 1992, l’Internet Society (ISOC) voit le jour, avec pour objectif de promouvoir et de coordonner les développements sur Internet. L’année 1993 voit l’apparition du premier navigateur web (browser), mêlant texte et image.
Cette même année, la National Science Foundation (NSF) mandate une compagnie pour enregistrer les noms de domaine. À la fin des années 1990, des sociétés pionnières comme Yahoo!, Amazon, eBay, Netscape, et AOL voient leur valeur en bourse monter en flèche, ce qui se finit par le krach boursier de 2001-2002.
En septembre 2014, l'espace Web connecté à Internet dépasse un milliard de sites en ligne, pour près de trois milliards d'internautes.
Le nombre de sites, d'internautes, de courriels envoyés, de recherches effectuées sur le moteur de recherche Google, est en augmentation permanente.
Les années 2010 voient le développement d'une informatique quantique à l'état de prototype, notamment d'un Internet quantique,. Selon Michel de Pracontal, il permettrait de « créer un réseau planétaire d’ordinateurs surpuissants fonctionnant selon les principes de la théorie des quanta, et connectés par des lignes de télécommunication spéciales permettant de transporter à distance les états quantiques.
Potentiellement, un tel système serait beaucoup plus rapide que l’Internet classique et mettrait à disposition des utilisateurs une puissance de calcul très supérieure.
Il aurait aussi l’immense avantage de garantir le secret des communications avec un niveau de protection inégalable par les moyens actuels ». Les États-Unis, l'Union européenne et la Chine cherchent à le développer.
La technologie intéresse tout particulièrement les organisations cherchant à optimiser la sécurité de leurs communications, telles que les banques et les armées.
Selon la définition du groupe de travail sur la gouvernance d'Internet, il faut entendre par « gouvernance de l’internet » l’élaboration et l’application par les États, le secteur privé et la société civile, dans le cadre de leurs rôles respectifs, de principes, normes, règles, procédures de prise de décisions et programmes communs propres à modeler l’évolution et l’usage de l’Internet.
Les registres de métadonnées sont importants dans l'établissement de règles d'accès aux ressources web qui utilisent les Uniform Resource Identifiers (qui peuvent être les URL qui s'affichent sur la barre de navigation de l'ordinateur personnel).
Un certain nombre d'organismes sont chargés de la gestion d'internet, avec des attributions spécifiques.
Ils participent à l'élaboration des standards techniques, l'attribution des noms de domaines, des adresses IP, etc. :
Dans un but de maintenir ou d'élargir la neutralité des réseaux, mais aussi d'engager les diverses parties globales dans un dialogue sur le sujet de la gouvernance, les Nations unies ont convoqué :
La gestion des ressources numériques essentielles au fonctionnement d'internet est confiée à l'Internet Assigned Numbers Authority (IANA), celle-ci délègue l'assignation des blocs d'adresses IP et de numéros d'Autonomous System aux registres Internet régionaux.
Dans l'Union européenne :
Voir Utilisation de l'URI pour l'accès aux ressources informatiques dans l'Union européenne
La neutralité du Net ou la neutralité du réseau est un principe fondateur d'internet qui exclut toute discrimination à l'égard de la source, de la destination ou du contenu de l'information transmise sur le réseau.
Mais de récents développements technologiques tendent à mettre fin à cette neutralité.[réf. nécessaire]
C'est aujourd'hui un grand enjeu technico-économique et socio-éthique.
Conscient de cette situation, le Conseil des droits de l'homme des Nations unies, prend position le 1er juillet 2016, en adoptant la résolution (A/HRC/32/L.20), non contraignante, visant à condamner les restrictions de l'accès à l'information sur Internet.
Le Conseil des droits de l'homme condamne sans équivoque les mesures qui visent à empêcher ou à perturber délibérément l'accès à l'information ou la diffusion d'informations en ligne, en violation du droit international des droits de l'homme, et invite tous les États à s'abstenir de telles pratiques et à les faire cesser,.
Sur le plan privé, l'association « accessnow.org » promeut et observe le libre accès à internet à travers le monde.
Internet trouve son fondement juridique dans l'existence d'un principe de libre-circulation de l'information qui remonte au XIXe siècle, lors de l'émergence du télégraphe.
Depuis, ce principe a émergé graduellement de la rencontre progressive puis de la symbiose entre la libre-circulation internationale des services et la liberté d'expression.
Tout au long du XXe siècle, ce qui était à l'origine une problématique technique encadrée par l'Union internationale des télécommunications (UIT) a été progressivement captée, reprise et amplifiée par l'accord général sur les tarifs douaniers et le commerce (GATT) puis l'Organisation mondiale du commerce (OMC), dans le cadre de la libéralisation internationale du commerce des services.
Cette problématique a alors été nommée progressivement « libre-circulation de l'information ». Mais cette origine technique et économique n'est pas son seul fondement.
Elle se base également sur le principe international de liberté d'expression.
Aujourd'hui, que l'on observe la jurisprudence du Conseil constitutionnel français ou des Cours européennes, le principe de libre circulation de l'information est consacré dans sa triple dimension : technique (en tant que support indissociable d'Internet), économique (en tant que préalable nécessaire à la libre-circulation mondiale des services) et éthique (en tant qu'instrument venant compléter et élargir le traditionnel principe de liberté d'expression).
Porter atteinte à ce principe revient dans le même temps (potentiellement) à porter atteinte à la liberté d'utiliser Internet, à la liberté d'expression et à la liberté de prestation de services.
Il n'existe pas de droit spécifique à Internet, mais plutôt une application du droit commun au réseau Internet.
Certaines législations nationales sont modifiées afin de prendre en compte ces particularités ; par exemple, en France, la Loi pour la confiance dans l'économie numérique (LCEN) du 21 juin 2004 et son article 6 sur la procédure de notification de contenu illicite sur internet.
Selon Benjamin Bayart, militant en faveur de la neutralité du réseau, la décision du Conseil constitutionnel rendue le 10 juin 2009 confirme qu'« Internet est essentiel à l’exercice de la liberté d'expression ».
L'application du droit sur internet est rendue difficile pour deux raisons principales :
La protection de données personnelles des pays membres de l’UE était, jusqu'au 25 mai 2018, communément basée sur la directive européenne du 24 octobre 1995, qui n’était plus en accord avec les avancées technologiques des 20 dernières années.
Cette dernière couvre la liberté de circulation des données personnelles en limitant les divergences entre les législations nationales.
En 2012, la Commission européenne a proposé le règlement général sur la protection des données (RGPD) qui révise cette loi afin d'y intégrer les échanges de données personnelles liés à la technologie, notamment à Internet et au Cloud.
Il aura fallu quatre ans de négociations législatives pour que ce projet de loi soit finalement promulgué le 14 avril 2016. Il est entré en application le 25 mai 2018. Dans ce contexte, le traitement des données personnelles par les entreprises est soumis à de nouvelles règles strictes.
Les données à caractère personnel sont concernées par le RGPD, elles concernent les informations basiques telles que les noms et prénoms, coordonnées postales et téléphoniques, mais aussi l’adresse IP, les informations sur la santé et les loisirs ainsi que les données personnelles sous pseudonyme, qui sont considérées comme des informations personnellement identifiables et soumises au nouveau règlement.
Internet est constitué de la multitude de réseaux répartis dans le monde entier et interconnectés. Chaque réseau est rattaché à une entité propre (université, fournisseur d'accès à Internet, armée) et est associé à un identifiant unique appelé Autonomous System (AS) utilisé par le protocole de routage BGP.
Afin de pouvoir communiquer entre eux, les réseaux s'échangent des données, soit en établissant une liaison directe, soit en se rattachant à un nœud d'échange (point de peering).
Ces échanges peuvent se limiter au trafic entre leurs utilisateurs respectifs (on parle alors de peering) ou bien inclure le trafic de tiers (il s'agit alors d'accord de transit).
Un opérateur qui fournit un service de transit Internet à d'autres fournisseurs d'accès est appelé carrier.
Ces accords d'échange de trafic sont libres, ils ne font pas l'objet d'une régulation par une autorité centrale.
Chaque réseau est connecté à un ou plusieurs autres réseaux.
Lorsque des données doivent être transmises d'un ordinateur vers un autre appartenant à un AS différent, il faut déterminer le chemin à effectuer parmi les réseaux.
Les routeurs chargés du trafic entre les AS disposent généralement d'une table de routage complète (Full routing table), de plus de 440 000 routes en 2013, et transmettent le trafic à un routeur voisin et plus proche de la destination après consultation de leur table de routage.
Des chercheurs israéliens de l'université Bar-Ilan ont déclaré, après avoir analysé les nœuds reliant l'ensemble des sites, qu'internet est un réseau méduse.
Ils la définissent comme ayant un cœur dense connecté à une multitude d'autres sites, qui ne sont reliés entre eux que par ce cœur, semblable à un maillage à structure fractale.
Cette zone permet à 70 % du réseau de rester connecté sans passer par le cœur.
Les chercheurs indiquent donc cette zone comme piste pour désengorger le trafic, en répartissant mieux les sites de cette zone.
En pratique, ces connexions sont réalisées par des infrastructures matérielles, et des protocoles informatiques.Ces connexions permettent notamment de relier des connexions grand public à des Centre de traitement de données.
L'accès à internet est souvent vendu sous la forme d'offre commerciale de services, avec un abonnement fixe ou un paiement aux données consommées.
Certaines organisations, notamment les universités européennes, disposent de leurs propres réseaux (ex. : Renater).
Pour accéder à Internet, il faut disposer d'un équipement IP ainsi que d'une connexion à un fournisseur d'accès. Pour cela, l'utilisateur emploie les matériel et logiciel suivants :
Des logiciels sont, eux, nécessaires pour exploiter Internet suivant les usages :
Les centres de données sont des lieux occupés par des serveurs.
Avant la bulle Internet, des millions de mètres carrés destinés à abriter de tels centres furent construits dans l'espoir de les voir occupés par des serveurs.
Depuis, la concentration des centres s'est poursuivie, avec le développement de centres spécialisés pour lesquels les défis les plus importants sont la maîtrise de la climatisation et surtout de la consommation électrique.
Ce mouvement a été intégré dans le green computing (informatique durable) et vise à aboutir à des centres de traitement de données dits écologiques pour lesquels sont apparus des outils spécialisés.
Internet repose sur la transmission d'informations d'un point à un autre.
Cette transmission se fait généralement au moyen d'ondes électromagnétiques.
Les différents points sont donc connectés soit physiquement, soit indirectement à travers d'autres points.
Ces ondes peuvent être transmises dans l'air (technologies sans fil), dans une fibre optique ou dans un câble métallique (technologies filaires).
Lorsque l'information doit passer d'une voie vers une autre, elle est aiguillée au moyen de matériels dédiés (switch, routeurs).
Les protocoles logiciels utilisés sur internet sont les conventions structurant les échanges d'informations nécessaires au transfert des contenus applicatifs pour l'usager final.
Ils permettent notamment d'identifier les interfaces (donc les machines), de s'assurer de la réception des données envoyées, et de l'interopérabilité.
Internet fonctionne suivant un modèle en couches, similaire au modèle OSI.
Les éléments appartenant aux mêmes couches utilisent un protocole de communication pour s'échanger des informations.
Un protocole est un ensemble de règles qui définissent un langage afin de faire communiquer plusieurs ordinateurs.
Ils sont définis par des normes ouvertes, les RFC (RFC 791, RFC 1000, RFC 1462 et RFC 1580).
Chaque protocole a des fonctions propres et, ensemble, ils fournissent un éventail de moyens permettant de répondre à la multiplicité et à la diversité des besoins sur internet.
Les principaux sont les suivants, classés selon leur couche (IP, TCP et UDP) ; couches applicatives :
Indépendamment du transfert entre deux points, les routeurs doivent pouvoir s'échanger des informations de routage.
Un IGP (Interior Gateway Protocol) et un EGP (Exterior gateway protocol) comme BGP (Border Gateway Protocol) satisfont ce besoin.
Comme produit essentiellement « dématérialisé », Internet peut paraître écologique, ou tout du moins avoir un impact limité sur l'environnement.
En accélérant les transferts d'informations et en facilitant les échanges de données, l'usage d'Internet a fréquemment été présenté comme vertueux de ce point de vue ; cet argument a par exemple été présenté lors de la mise en place de factures électroniques ou de la dématérialisation des marchés publics[réf. souhaitée].
Le fonctionnement du réseau implique pourtant de très fortes consommations énergétiques[réf. incomplète],,. Outre le coût énergétique engendré par la construction de l'infrastructure (dit énergie grise), le coût de fonctionnement des centres de données peut être mis en évidence et traduit en équivalent CO2[Combien ?].
Internet aurait une empreinte carbone extrêmement importante ; plus de 200 milliards d'emails seraient envoyés chaque jour,, sachant que chaque email est lié à l'émission de quelques grammes à quelques dizaines de grammes de CO2, en fonction du nombre de destinataires et de la taille des pièces jointes,,. Cet impact écologique est principalement dû à la fabrication des terminaux (ordinateur, smartphone, tablette, etc.) ayant servi à envoyer et recevoir l'email.
Si Internet était un pays, ce serait le troisième plus gros consommateur d'électricité en 2018, après la Chine et les États-Unis,. Selon plusieurs études, en 2018, Internet correspond à 6 à 10 % de la consommation mondiale d'électricité et pèserait près de 5 % des émissions de gaz à effet de serre mondiales, soit plus que le transport aérien.
Une heure d'échanges de courriels dans le monde correspond à 4 000 vols Paris-New York[réf. nécessaire]. La construction des centres de traitement de données des principaux acteurs d'Internet, Google, Apple et Facebook, dans l'État de Caroline du Nord aux États-Unis, est intimement liée au bas coût de l'énergie dans cet État.
Ce coût bas s'explique par le fonctionnement de centrales thermiques utilisant le charbon des Appalaches, dont l'exploitation à ciel ouvert détruit des montagnes entières.
En 2008, le monde compte 1,574 milliard d'internautes.
En juin 2012, 2,4 milliards d'internautes sont recensés.
En octobre 2020, plus de 60 % la population mondiale accède à Internet ; on compte 4,93 milliards d'internautes, soit un taux de pénétration de 63,2 %
Le développement du réseau internet entraîne un bouleversement sans précédent depuis l'apparition de l'imprimerie.
Comme l'ont fait l'écriture, le charbon et les télécommunications lors de leur apparition, Internet augmente la capacité des hommes à travailler ensemble de façon plus efficace et plus étendue.
Ce n'est pas une simple révolution technologique, mais un remaniement complet de la manière dont l'humanité appréhende le monde qui l'entoure. « C'est pourquoi la virtualité d'Internet n'est pas celle que l'on croit.
Elle ne s'oppose pas au réel, mais à l'actuel.
Elle se trouve dans chacune de nos actions.
Internet offre de nouvelles potentialités d'action et chacune des virtualités qui est ainsi actualisée, conjointement, change subrepticement le monde que nous vivons », affirme Boris Beaude.
Le philosophe Guillaume Cazeaux remarque, quant à lui, que la libération de la parole, permise par le Web 2.0, entraîne un effet inattendu : noyés dans la masse d’informations et de désinformations, les internautes développent des représentations du monde qui les divisent.
Comme l’imprimerie avait ébranlé la foi et provoqué la Réforme protestante, en favorisant la diffusion du savoir, Internet génère aussi des « schismes » qui menacent l’unité de nos sociétés. « Les questionnements vertigineux qui se posaient à l’homme de la Renaissance, à Montaigne par exemple, redeviennent ainsi étonnamment les nôtres », estime le philosophe.
La mise à disposition constante d'images et d'idées et leur transmission rapide ont des conséquences sur le développement psychologique, moral et social des personnes, la structure et le fonctionnement des sociétés, les échanges culturels, la perception des valeurs et les convictions religieuses.
La planète est devenue un réseau mondial, bourdonnant de transmissions électroniques, une planète « en conversation ». Tout cela n'est pas sans poser des questions éthiques sur le développement de la personne humaine et la chance que peuvent avoir les personnes et les peuples de percevoir une transcendance.
Internet est un espace paradoxal : il se détache de la conception spatiale ou matérialiste de l'espace que l'histoire a mise en place. « Internet est un espace qui fait gagner de l'espace-temps.
Il se révèle plus efficient que d'autres espaces dès lors que l'étendue est vaste, que le nombre de réalités considérées est important et que l'interaction n'exige pas de contact matériel », selon Boris Beaude.
Internet a bouleversé les rôles et les structures sociales jusqu'alors bien établis.
Alors que le géant Google a transformé l'accès à l'information de différentes façons (accessibilité, rapidité et réseautage), les réseaux sociaux sont devenus les principaux moyens de médiation et de relation entre les individus, pour ne nommer que ceux-là. Internet s'est donc immiscé dans l'ordre social pour le remanier. « La capacité d'Internet à créer du contact réticulaire en dépit de la distance territoriale offre aussi une opportunité considérable d'organisation, de production et de coordination », souligne Boris Beaude.
Autant Internet peut être une occasion d'enrichissement personnel et culturel, et contribuer à un développement humain authentique, autant il risque de constituer une menace pour le lien social, s'il en vient à dispenser les hommes de toute communication directe.
Le sociologue Philippe Breton met en garde contre une conception de la « société mondiale de l'information », où les liens sociaux seraient fondés sur la séparation des corps et la collectivisation des consciences.
Selon lui, cette vision du tout-internet découle de l'héritage de Teilhard de Chardin, du bouddhisme zen et des croyances New Age.
Internet a commencé à se développer dans le monde dans les années 1995-2000, au moment où la communauté des informaticiens se préparait au passage à l'an 2000 (noté « Y2K » dans le monde anglo-saxon).
Le consultant canadien Peter de Jager a largement contribué dans ces années à la mobilisation mondiale, grâce à son site web year2000.com, qui était à l'époque le site le plus interconnecté au monde[réf. nécessaire]. À l'occasion du 10e anniversaire du passage à l'an 2000, Peter de Jaeger a reçu le Lifeboat Foundation’s 2009 Guardian Award.
Eric Klien, président de la Lifeboat Foundation, a salué les efforts de Peter de Jaeger en ajoutant :
Let us learn from the Y2K success by applying its worldwide mobilization method to future problems and not mislearn from it that all future problems will just solve themselves somehow so we can ignore them.
« Tirons les leçons du succès du passage à l'an 2000 en appliquant sa méthode de mobilisation mondiale à des problèmes futurs, et sans en conclure que tous les problèmes futurs se résoudront d'eux-mêmes alors même qu'on les ignorerait. »
Alors même que certains experts dénoncent de mauvaises hypothèses sur le rôle des technologies de l'information et de la communication par rapport aux problèmes d'environnement, les mêmes experts soulignent qu'internet peut jouer un rôle très important pour la mobilisation des citoyens sur les questions de responsabilité sociale et de développement durable.
Internet est en effet un réseau de vigilance, alimenté par les associations, les ONG, et les gouvernements, accessible à tous les citoyens (au moins dans les pays les plus développés), et qui peuvent en outre servir de source d'information pour les médias.
La convention d'Aarhus, signée en 1998 par trente-neuf États, porte sur l'accès à l'information et la participation du public au processus décisionnel.
En France, elle a donné lieu au portail Toutsurlenvironnement.fr, qui publie de nombreuses informations environnementales.
Le web de deuxième génération (web 2.0), fournit des plateformes d'échange entre utilisateurs grâce à des services collaboratifs tels que les wikis. L'encyclopédie Wikipédia en est d'ailleurs un excellent exemple.
Internet est souvent employé comme outil de mobilisation par les organisations non gouvernementales et altermondialistes, comme Attac.
Par ailleurs, des groupuscules politiques utilisent Internet comme un canal de sensibilisation et de propagande[réf. nécessaire].
Un phénomène nouveau apparu dans les années 2000 est la collecte de signatures pour des pétitions en ligne[réf. nécessaire].
La tendance apparue depuis 2012 environ en France est à une articulation entre l'usage offensif d'Internet par le biais des réseaux sociaux et l'expression publique dans la rue.
Elle introduit des combinaisons innovantes entre les manifestations de rue et les techniques de prise de parole (sites internet, blogs, web social) ou les terminaux mobiles (SMS, prise d'images et de vidéos).
Ces formes de mobilisations peuvent avoir un effet indésirable : le slacktivisme (dit « militantisme de canapé ») peut sembler suffisant à ses participants, par conséquent cela peut diminuer le nombre de ceux qui ensuite passent au militantisme[réf. nécessaire].
La fracture numérique est la disparité d'accès aux technologies informatiques, mise en évidence par la disponibilité inégale du réseau Internet.
Elle recouvre parfois le clivage entre « les info-émetteurs et les info-récepteurs ».
Cette disparité est fortement marquée d'une part entre les pays riches et les pays pauvres, d'autre part entre les zones urbaines denses et les zones rurales.
Elle existe également à l'intérieur des zones moyennement denses.
La période 1971-1975, qui a vu les prémices d'Internet, est évoquée sur le ton de l'humour dans Comédies françaises, un roman d’Éric Reinhardt publié en 2020. Le livre interroge les liens entre culture, politique et technologie lors de l'arrêt du réseau Cyclades, reposant sur le datagramme et qui inspirera la conception d'Internet[réf. à confirmer]. L'auteur décrit, via l'enquête d'un jeune journaliste de l'Agence France-Presse, comment Ambroise Roux, patron de la CGE, a obtenu du président Valéry Giscard d'Estaing, au début des surfacturations aux PTT, l'abandon du plan Calcul, d'Unidata, de la Délégation générale à l'informatique et du réseau Cyclades.
« L'un des grands phénomènes sociaux actuels, que l'internet a généré, est en effet la création d'un gouffre, de plus en plus profond, entre deux types de populations : celles qui s'informent encore majoritairement sur les médias classiques, ou le web, mais dans son versant traditionnel (les grands sites d'information), et celles qui ont pris l'habitude de s'informer sur le web alternatif, et qui ont perdu presque toute confiance dans les médias dits "officiels". […] La base contextuelle à partir de laquelle nous forgeons notre représentation du réel se scinde, selon que l'on s'informe via les médias traditionnels ou le web, mais aussi - plus encore - selon les nébuleuses que l'on fréquente sur la Toile.
Des univers mentaux très différents se créent, qui séparent les hommes, autant que pouvaient l'être jadis les habitants de différentes régions du monde. »
Sur les autres projets Wikimedia :
: document utilisé comme source pour la rédaction de cet article.
