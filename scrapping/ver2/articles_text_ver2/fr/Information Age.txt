Ère de l'information
Aperçu des premiers développements[modifier | modifier le code]
Concept en trois étapes[modifier | modifier le code]
Économie[modifier | modifier le code]
Innovations[modifier | modifier le code]
Références[modifier | modifier le code]
Bibliographie[modifier | modifier le code]
Liens externes[modifier | modifier le code]
Expansion des bibliothèques et loi de Moore[modifier | modifier le code]
Stockage d'informations et loi de Kryder[modifier | modifier le code]
Transmission d'informations[modifier | modifier le code]
Calcul[modifier | modifier le code]
Emplois et répartition des revenus[modifier | modifier le code]
Automatisation, productivité et gain de travail[modifier | modifier le code]
Industrie à forte intensité d'information[modifier | modifier le code]
Transistors[modifier | modifier le code]
Calculateurs[modifier | modifier le code]
Données[modifier | modifier le code]
Optique[modifier | modifier le code]
Pages pour les contributeurs déconnectés en savoir plus
Sommaire
				déplacer vers la barre latérale
masquer
L'ère de l'information (aussi connue comme ère numérique ou ère informatique) est une période historique qui débute au milieu du XXe siècle et qui est caractérisée par un rapide passage de l'industrie traditionnelle établie par la révolution industrielle à une économie principalement basée sur la technologie de l'information[1],[2],[3],[4]. L'apparition de l'ère de l'information peut être associée au développement de la technologie des transistors, en particulier du MOSFET (transistor à effet de champ métal-oxyde-semi-conducteur )[5],[6] qui est devenu la pierre angulaire de l'électronique numérique  et a révolutionné la technologie moderne[7].
Selon le Département des affaires économiques et sociales des Nations Unies, l'ère de l'information est formée en capitalisant sur les progrès de la microminiaturisation informatique[8],[2].
L'expansion de la taille des bibliothèques est calculée en 1945 par Fremont Rider (en) pour doubler sa capacité tous les 16 ans si l'espace disponible est suffisant[9]. Il  préconise de remplacer les œuvres imprimées volumineuses et en décomposition par des photographies analogiques microformes miniaturisées, qui pourraient être dupliquées à la demande pour les usagers des bibliothèques et d'autres institutions.
Cependant, Rider ne prévoit alors pas l'arrivée de l'électronique numérique dans les décennies suivantes, grâce à laquelle de vastes augmentations de la rapidité de la croissance de l'information sont rendues possibles avec des technologies numériques automatisées et potentiellement sans perte. En conséquence, la loi de Moore, formulée vers 1965, postule que le nombre de transistors dans un circuit intégré dense double environ tous les deux ans[10],[11].
Au début des années 80, parallèlement à l'amélioration de la puissance de calcul, la prolifération des ordinateurs personnels plus petits et moins chers permet un accès immédiat à l'information et la capacité de partager et de stocker ces informations pour un nombre croissant de personnes. La connectivité entre les ordinateurs au sein des organisations permet aux employés à différents niveaux d'accéder à de plus grandes quantités d'informations.
La capacité technologique mondiale de stockage de données est passée de 2,6 exaoctets (EB) (compressés de manière optimale) en 1986 à 15,8 EB en 1993 puis plus de 54,5 EB en 2000 et enfin 295 EB en 2007[12],[13]. C'est l'équivalent informationnel de moins d'un CD-ROM de 730 mégaoctets (Mo) par personne en 1986 (539 Mo par personne), environ quatre CD-ROM par personne en 1993, douze CD-ROM par personne en 2000 et près de soixante et un CD-ROM par personne en 2007[14]. On estime que la capacité mondiale de stockage d'informations atteint 5 zettaoctets en 2014, l'équivalent de 4 500 piles de livres imprimés sur la distance Terre-Soleil [15].
La quantité de données numériques stockées semble augmenter exponentiellement, rappelant la loi de Moore. En tant que tel, la loi de Kryder prescrit que la quantité d'espace de stockage disponible semble croître de manière approximativement exponentielle [16],[17],[11].
La capacité technologique mondiale de reception d'informations via des réseaux de diffusion unidirectionnels est de 432 exaoctets d'informations  en 1986, 715 exaoctets en 1993, 1,2 zettaoctets en 2000 puis 1,9 zettaoctets en 2007, soit l'équivalent informationnel de 174 journaux par personne et par jour[14].
Dans les années 90, la propagation de l'utilisation d'Internet provoque un bond soudain de l'accès et de la capacité de partager des informations dans les entreprises et les foyers du monde entier. La technologie évolue si rapidement qu'un ordinateur coûtant 3 000 dollars en 1997 coûtait 2 000 dollars deux ans plus tard et 1 000 dollars l'année suivante.
La capacité technologique mondiale de calculer des informations avec des ordinateurs polyvalents est passée de 3,0×108 MIPS en 1986 à 4,4×109 MIPS en 1993, à 2,9×1011 MIPS en 2000 puis à 6,4×1012 MIPS en 2007[14] Un article publié dans la revue Trends in Ecology and Evolution rapporte que, désormais[15]:
[La technologie numérique] a largement dépassé la capacité cognitive de tout être humain et l'a fait une décennie plus tôt que prévu. En termes de capacité, il existe deux mesures d'importance : le nombre d'opérations qu'un système peut effectuer et la quantité d'informations pouvant être stockées. Le nombre d'opérations synaptiques par seconde dans un cerveau humain est estimé entre 10^15 et 10^17. Bien que ce nombre soit impressionnant, même en 2007, les ordinateurs à usage général de l'humanité étaient capables d'exécuter bien plus de 10^18 instructions par seconde. Les estimations suggèrent que la capacité de stockage d'un cerveau humain individuel est d'environ 10^12 octets. Par habitant, cela correspond au stockage numérique actuel [en 2016] (5x10^21 octets pour 7,2x10^9 personnes).
L'ère de l'information peut être définie comme l'ère de l'information primaire et l'ère de l'information secondaire. L'information à l'ère de l'information primaire est gérée par les journaux, la radio et la télévision. L'ère de l'information secondaire s'est développée par Internet, les télévisions par satellite et les téléphones portables. L'ère de l'information tertiaire est apparue lorsque les médias de l'ère de l'information primaire se sont interconnectés avec les médias de l'ère de l'information secondaire[18].
Finalement, les technologies de l'information et de la communication (TIC) - c'est-à-dire les ordinateurs, les machines informatisées, la fibre optique, les satellites de communication, Internet, etc. - sont devenues une partie importante de l'économie mondiale, car le développement des micro-ordinateurs a considérablement changé de nombreuses entreprises et industries[19],[20].
Nicholas Negroponte discute par exemple en 1995 dans son livre Being Digital des similitudes et des différences entre les produits faits d'atomes et les produits faits de bits[21]. Essentiellement, une copie d'un produit fait de bits peut être fabriquée à bas prix et rapidement, puis expédiée rapidement à travers le monde à un coût très bas.
L'ère de l'information affecte la main-d'œuvre de plusieurs manières, par exemple en obligeant les travailleurs à être compétitifs sur un marché du travail mondial. L'une des préoccupations les plus évidentes est le remplacement du travail humain par des ordinateurs capables de faire leur travail plus rapidement et plus efficacement, créant ainsi une situation dans laquelle les personnes qui exécutent des tâches facilement automatisables sont forcées de trouver un emploi là où leur travail n'est pas dispensable. Cela crée particulièrement des problèmes pour les habitants des villes industrielles, où les solutions impliquent généralement de réduire le temps de travail. Ainsi, les personnes qui perdent leur emploi peuvent être poussées à rejoindre les «travailleurs de l'esprit» (par exemple les ingénieurs, médecins, avocats, enseignants, professeurs, scientifiques, cadres, journalistes, consultants, etc.), qui reçoivent des salaires relativement élevés en comparaison.
Parallèlement à l'automatisation, les emplois traditionnellement associés à la classe moyenne (par exemple, sur chaîne de montage, de traitement des données ou de gestion) commencent également à disparaître du fait de l'externalisation[22]. Incapables de concurrencer avec les pays en développement, les travailleurs de la production et des services dans les sociétés post-industrielles perdent leur emploi à cause de la sous-traitance, acceptent des réductions de salaire ou se contentent d'emplois de services peu qualifiés et à bas salaires.
Dans le passé, le sort économique des individus était lié à celui de leur nation. Par exemple, les travailleurs aux États-Unis étaient autrefois bien payés par rapport à ceux d'autres pays. Avec l'avènement de l'ère de l'information et l'amélioration de la communication, ce n'est plus le cas, car les travailleurs doivent désormais être compétitifs sur un marché du travail mondial, où les salaires sont moins dépendants du succès ou de l'échec des économies individuelles.
En créant une main d'œuvre mondialisée, Internet accroît aussi les opportunités dans les pays en développement, permettant aux travailleurs de ces endroits de fournir des services en personne, donc en concurrence directe avec leurs homologues d'autres pays. Cet avantage compétitif se traduit par des opportunités accrues et des salaires plus élevés pour eux[23].
L'ère de l'information affecte aussi la main-d'œuvre dans la mesure où l'automatisation et l'informatisation aboutissent à une productivité plus élevée associée à une perte nette d'emplois dans le secteur manufacturier. Aux États-Unis, par exemple, de janvier 1972 à août 2010, le nombre de personnes employées dans les emplois manufacturiers est passé de 17,5 millions à 11,5 tandis que la valeur manufacturière augmentait de 270 %[24].
S'il est apparu au départ que la perte d'emplois dans le secteur industriel pouvait être partiellement compensée par la croissance rapide des emplois dans les technologies de l'information, la récession du début des années 2000 amorce le début d'une forte baisse du nombre d'emplois dans le secteur. Cette tendance se poursuit jusqu'en 2003[25] mais les données ultérieures montrent que, dans l'ensemble, la technologie crée plus d'emplois qu'elle n'en détruit, même à court terme[26].
L'industrie devient plus gourmande en informations, mais moins en main-d'œuvre et en capital. Cela laisse des implications importantes pour la main-d'œuvre car les travailleurs deviennent de plus en plus productifs à mesure que la valeur de leur travail diminue. Pour le système du capitalisme lui-même, non seulement la valeur du travail diminue, mais la valeur du capital est également diminuée.
Dans le modèle classique, les investissements en capitale humain et financier sont des prédicteurs importants de la performance d'une nouvelle entreprise[27]. Cependant, comme l'ont par exemple démontré Mark Zuckerberg et Facebook, il est désormais possible pour un groupe de personnes relativement inexpérimentées avec un capital limité de réussir à grande échelle[28].
L'ère de l'information est rendue possible par la technologie développée lors de la révolution numérique, qui a elle-même été rendue possible en s'appuyant sur les développements de la révolution technologique à la Belle Epoque.
L'apparition de l'ère de l'information peut être associée au développement de la technologie des transistors. Le concept d'un transistor à effet de champ est théorisé pour la première fois par Julius Edgar Lilienfeld en 1925. Le premier transistor pratique est le transistor à point de contact, inventé par les ingénieurs Walter Houser Brattain et John Bardeen aux Laboratoires Bells en 1947, jetant les bases de la technologie moderne[4]. L'équipe de recherche de Shockley invente également le transistor bipolaire en 1952[29],[30]. Cependant, les premiers sont des dispositifs relativement volumineux qui sont difficiles à fabriquer en série, les limitant à un nombre restreint d'applications spécialisées[31].
Le début de l'ère de l'information, avec l'ère du silicium, remonte à l'invention du transistor à effet de champ à grille métal-oxyde (MOSFET, ou transistor MOS)[32],  inventé par Mohamed M. Atalla et Dawon Kahng aux Laboratoires Bells en 1959[6],[29],[33]. Le MOSFET est le premier transistor vraiment compact qui peut être miniaturisé et produit en série pour une large gamme d'utilisations[31]. Avec sa grande évolutivité[34], sa consommation d'énergie beaucoup plus faible et sa densité plus élevée que les transistors bipolaires, le MOSFET permet de construire circuits intégres (IC) à très grande échelle[35]. Plus de 10 000 transistors peuvent être placés dans un petit circuit intégré[36] et plus tard des milliards de transistors dans un seul appareil[37].
L'adoption généralisée des MOSFET révolutionne l' industrie électronique[38], comme systèmes de régulation et les ordinateurs depuis les années 1970[39]. Le MOSFET permet notamment à un ordinateur d'exister sur quelques petites puces de circuits intégrés plutôt que de remplir une pièce[7] et rendant ensuite possible les smartphones. Dans les années 2010, des milliards de transistors MOS sont fabriqués chaque jour[29]. Il s'agit de la pierre angulaire de l'électronique numérique depuis la fin du XXe siècle, ouvrant la voie à l'ère numérique[6]. Le transistor MOS est ainsi crédité de la transformation de la société à travers le monde[37]  et décrit comme le "cheval de trait" de l'ère de l'information[5],[40].
Avant l'avènement de l'électronique, les calculateurs mécaniques, comme la machine analytique en 1837, sont conçus pour fournir des calculs mathématiques de routine et ont des capacités de prise de décision simples. Les besoins militaires pendant la Seconde Guerre mondiale conduisent au développement des premiers ordinateurs électroniques, basés sur des tubes électroniques, notamment le Zuse 3, l'ordinateur Atanasoff–Berry, l'ordinateur Colossus et l' ENIAC.
L'invention du transistor permet l'ère des ordinateurs centraux (années 1950-1970), caractérisée par les IBM 360 et 370. Ces grands ordinateurs de la taille d'une pièce permettent de calculer et de manipuler les données beaucoup plus rapidement qu'humainement, mais coûtent cher à acheter et à entretenir. Ils sont donc initialement limités à quelques institutions scientifiques, grandes entreprises et agences gouvernementales.
Le circuit intégré au germanium est inventé par Jack Kilby chez Texas Instruments en 1958. Le circuit intégré en silicium est ensuite conçu en 1959 par Robert Noyce chez Fairchild Semiconductor, en utilisant le processus planaire développé par Jean Hoerni, qui s'appuyait à son tour sur une méthode de passivation de Mohamed Atalla développée aux Laboratoires Bells en 1957[41],[42]. À la suite de l'invention du transistor MOS par Mohamed Atalla et Dawon Kahng en 1959[33] le circuit intégré MOS est développé par Fred Heiman et Steven Hofstein au RCA en 1962[43]. Le MOS IC  à grille de silicium est conçu par Federico Faggin chez Fairchild Semiconductor en 1968[44].
Avec l'avènement du transistor MOS et du MOS IC, la technologie des transistors s'améliore rapidement suet le rapport puissance de calcul / taille augmente considérablement suivant la loi de Moore, donnant un accès direct aux ordinateurs à des groupes de personnes de plus en plus divers.
Le circuit intégré MOS conduit à l'invention du microprocesseur. Le premier microprocesseur commercial à puce unique lancé en 1971, l'Intel 4004, est  développé par Federico Faggin à l'aide de sa technologie MOS IC à grille silicium, avec Marcian Hoff, Masatoshi Shima et Stanley Mazor[45],[46].
Parallèlement aux bornes d'arcade électroniques et aux consoles de jeux vidéo dans les années 1970, le développement d'ordinateurs personnels comme le Commodore PET et Apple II (tous deux en 1977) permet au plus grand nombre d'accéder à l'ordinateur. Mais le partage de données entre ordinateurs individuels est soit inexistant, soit largement manuel, utilisant d'abord des cartes perforées et des bandes magnétiques, puis des disquettes.
Les premiers développements en matière de stockage de données sont initialement basés sur des photographies, à commencer par la microphotographie en 1851 puis la microforme dans les années 1920, avec la possibilité de stocker des documents sur pellicule, ce qui les rend beaucoup plus compacts. Les premières théories de l'information et les codes de Hamming sont développés vers 1950, mais attendaient que les innovations techniques en matière de transmission et de stockage des données pour être pleinement utilisés.
La mémoire à tores magnétiques est développée à partir des recherches de Frederick W. Viehe en 1947 et d'An Wang à l'Université de Harvard en 1949[47],[48]. Avec l'avènement du transistor MOS, la mémoire à semi-conducteur MOS est développée par John Schmidt à Fairchild Semiconductor en 1964[49],[50]. En 1967, Dawon Kahng et Simon Sze conçoivent le MOSFET à grille flottante (FGMOS), qui, selon eux, pourrait être utilisé pour la mémoire morte reprogrammable (EPROM)[51], fournissant la base de la mémoire non volatile (NVM) comme la mémoire flash[52]. À la suite de l'invention de la mémoire flash par Fujio Masuoka chez Toshiba en 1980[53],[54], la firme  commercialise la mémoire flash NAND en 1987[55],[51].
Alors que les câbles transmettant des données numériques connectés aux terminaux et périphériques d'ordinateurs aux ordinateurs centraux sont courants et que des systèmes spéciaux de partage de messages menant au courrier électronique sont développés pour la première fois dans les années 1960, la mise en réseau indépendante entre ordinateurs commence avec ARPANET en 1969. Cela se développé pour devenir Internet (inventé en 1974), puis le World Wide Web en 1989.
La transmission publique de données numériques utilise d'abord les lignes téléphoniques existantes par accès commuté, à partir des années 1950, et reste le pilier d'Internet jusqu'au haut débit dans les années 2000. La transmission sans fil, l'introduction et la prolifération des réseaux sans fil, commence dans les années 1990 et est  rendue possible par l'adoption généralisée des amplificateurs de puissance RF basés sur MOSFET (Power MOSFET et LDMOS) et des circuits RF CMOS[56],[57],[58]. Les réseaux sans fil, combinés à la prolifération des satellites de télécommunications dans les années 2000, permettent la transmission numérique publique sans avoir besoin de câbles. Cette technologie conduit à la télévision numérique, au GPS, à la radio par satellite et aux téléphones mobiles dans les années 1990 à 2000.
La mise à l'échelle MOSFET et leur miniaturisation rapide des MOSFET à un taux prédit par la loi de Moore[59] conduit les ordinateurs à devenir plus petits et plus puissants, au point où ils peuvent être transportés. Au cours des années 1980 – 1990, les ordinateurs portables sont développés et les assistants numériques personnels (PDA) peuvent être utilisé debout ou en marchant. Les pagers, largement utilisés dans les années 80, sont remplacés par les téléphones mobiles à partir de la fin des années 90, fournissant des fonctionnalités de réseautage mobile à certains ordinateurs. Désormais courante, cette technologie est étendue aux appareils photo numériques et autres appareils portables. À partir de la fin des années 1990, les tablettes puis les smartphones combinent et étendent ces capacités de calcul, de mobilité et de partage d'informations.
Le codage par transformée en cosinus discrète (DCT), une technique de compression de données proposée pour la première fois par Nasir Ahmed en 1972[60] permet la transmission pratique de médias numériques[61],[62],[63] avec des formats de compression d'image tels que JPEG (1992), des formats de codage vidéo tels que MPEG (à partir de 1993)[64], des normes de codage audio telles que Dolby Digital (1991)[65],[66] et MP3 (1994), et normes de télévision numérique comme la vidéo à la demande (VOD)  et la télévision haute définition (HDTV)[67]. La vidéo sur Internet est popularisée par YouTube, une plate-forme de vidéo en ligne fondée par Chad Hurley, Jawed Karim et Steve Chen en 2005, qui permet la diffusion vidéo en continu de contenu généré par les utilisateurs de n'importe où sur le World Wide Web[68].
Le papier électronique, qui remonte aux années 1970, permet aux informations numériques d'apparaître sous forme de documents papier.
La communication optique joue un rôle important dans les réseaux de télécommunication. La communication optique fournit en effet la base matérielle de la technologie derrière Internet[69].
En 1953, Bram van Heel démontre la possibilité de transmission d'images à travers des faisceaux de fibres optiques avec un revêtement transparent. La même année, Harold Hopkins et Narinder Singh Kapany de l' Imperial College réussissent à créer des faisceaux de transmission d'images avec plus de 10000 fibres optiques, et réalisent par la suite réalisé une transmission d'image via un 75 de cm de long combinant plusieurs milliers de fibres[70].
Tout en travaillant à l'Université de Tohoku, l'ingénieur japonais Jun'ichi Nishizawa propose la communication par fibre optique, l'utilisation de fibres optiques pour la communication, en 1963[71]. Il invente par ailleurs d'autres technologies qui contribuent au développement des communications par fibre optique, telles que la fibre optique à gradient d'indice comme canal pour transmettre la lumière des lasers à semi-conducteurs[72],[73]. Il dépose un brevet pour cette invention en 1964[69] et crée La fibre optique à l'état solide en 1964[74].
Les trois éléments essentiels de la communication optique sont inventés par Jun-ichi Nishizawa : la diode laser (1957) étant la source lumineuse, la fibre optique à gradient d'indice (1964) comme ligne de transmission et la photodiode PIN (1950) comme optique destinataire[69]. Le développement du laser à semi-conducteur à onde continue par Izuo Hayashi en 1970 conduit directement à l'utilisation de sources lumineuses dans la communication par fibre optique, se trouvant ensuite dans les imprimantes laser, les lecteurs de code-barres et les lecteurs de disques optiques[75].
Les MOSFET permettant la capture photographique commençant à apparaître dans les années 1960, ils conduisent conduit à la transition de l'analogique vers l'imagerie numérique au cours des années 1980 – 1990. Les capteurs d'image les plus courants sont le  capteur photographique CCD (CCD) et le Complementary Metal Oxide Semiconductor (capteur CMOS)[76],[77].
« While the bipolar junction transistor was the first transistor device to take hold in the integrated circuit world, there is no question that the advent of MOSFETs, an acronym for metal-oxide-semiconductor field-effect transistor, is what truly revolutionized the world in the so-called information age. The density with which these devices can be made has allowed entire computers to exist on a few small chips rather than filling a room. »
« The Si MOSFET has revolutionized the electronics industry and as a result impacts our daily lives in almost every conceivable way. »
« The metal-oxide-semiconductor field-effect transistor (MOSFET) is the most commonly used active device in the very large-scale integration of digital integrated circuits (VLSI). During the 1970s these components revolutionized electronic signal processing, control systems and computers. »

