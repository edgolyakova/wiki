{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers and ðŸ¤— Datasets. Uncomment the following cell and run it. We also use the `sacrebleu` and `sentencepiece` libraries - you may need to install these even if you already have ðŸ¤— Transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MOsHUjgdIrIW",
    "outputId": "f84a093e-147f-470e-aad9-80fb51193c8e"
   },
   "outputs": [],
   "source": [
    "#! pip install transformers[sentencepiece] datasets\n",
    "#! pip install sacrebleu sentencepiece\n",
    "#! pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow==2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: evaluate\n",
      "Version: 0.3.0\n",
      "Summary: HuggingFace community-driven open-source library of evaluation\n",
      "Home-page: https://github.com/huggingface/evaluate\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: leandro@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /Users/egoliakova/opt/anaconda3/lib/python3.8/site-packages\n",
      "Requires: pandas, requests, xxhash, multiprocess, fsspec, dill, numpy, tqdm, huggingface-hub, datasets, packaging, responses\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
    "\n",
    "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then uncomment the following cell and input your token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/egoliakova/.huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to install Git-LFS and setup Git if you haven't already. Uncomment the following instructions and adapt with your name and email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install git-lfs\n",
    "# !git config --global user.email \"you@example.com\"\n",
    "# !git config --global user.name \"Your Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your version of Transformers is at least 4.16.0 since some of the functionality we use was introduced in that version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.21.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFASsisvIrIb"
   },
   "source": [
    "You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert CSV-file to a dataset-ready format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below works with a specifically formatted csv. Run the cell below to format your CSV accordingly.\n",
    "Your CSV should have at least 2 columns `en` and `xx` where xx is the code of the target language.\n",
    "\n",
    "If the CSV file has PoS tags for source and target language, the expected column names for them are:\n",
    "`pos_en` and `pos_xx`. \n",
    "\n",
    "If the CSV file has WA tags, the expected column name is `wa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# source_lang accepted value = 'en'\n",
    "# target_lang accepted values = 'fr'|'zh'\n",
    "# Choose pos_tags=True if the file has PoS tags for the both languages\n",
    "# Choose wa_tags=True if the file has WA tags.\n",
    "# Choose store=True if you want to create a json dump of the file that can be used later\n",
    "\n",
    "def csv_to_dataset(filename, source_lang, target_lang, pos_tags=False, wa_tags=False, store=False):\n",
    "    data = pd.read_csv(filename)\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['translation'] = [{source_lang: x, target_lang: y} for x, y in zip(data[source_lang], data[target_lang])]\n",
    "    if pos_tags:\n",
    "        new_df['pos'] = [{source_lang: x, target_lang: y} for x, y in zip(data[f'pos_{source_lang}'], data[f'pos_{target_lang}'])]\n",
    "    if wa_tags:\n",
    "        new_df['wa'] = data['wa']\n",
    "    return Dataset.from_pandas(new_df).train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "loaded_dataset = load_from_disk('dataset_split.hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1508\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 378\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dataset.remove_columns(['pos', 'wa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a translation task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTCFado4IrIc"
   },
   "source": [
    "In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) model for a translation task. We will use the [WMT dataset](http://www.statmt.org/wmt16/), a machine translation dataset composed from a collection of various sources, including news commentaries and parliament proceedings.\n",
    "\n",
    "![Widget inference on a translation task](images/translation.png)\n",
    "\n",
    "We will see how to easily load the dataset for this task using ðŸ¤— Datasets and how to fine-tune a model on it using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "This notebook is built to run  with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a sequence-to-sequence version in the Transformers library. Here we picked the [`Helsinki-NLP/opus-mt-en-romance`](https://huggingface.co/Helsinki-NLP/opus-mt-en-ROMANCE) checkpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the `datasets` function `load_dataset` and the `evaluate` function `load`. We use the English/Romanian part of the WMT dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "metric = load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>pos</th>\n",
       "      <th>wa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'en': 'Christian Matras died on 16 October 1988.', 'fr': 'Christian Matras mourut le 16 octobre 1988.'}</td>\n",
       "      <td>{'en': 'Christian PROPN\n",
       "Matras PROPN\n",
       "died VERB\n",
       "on ADP\n",
       "16 NUM\n",
       "October PROPN\n",
       "1988 NUM\n",
       ". PUNCT\n",
       "', 'fr': 'Christian PROPN\n",
       "Matras PROPN\n",
       "mourut PROPN\n",
       "le DET\n",
       "16 NUM\n",
       "octobre NOUN\n",
       "1988 NUM\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'en': 'Choekyi Gyaltsen 10th Panchen Lama himself declared as cited by an official Chinese review that \"according to Tibetan tradition the confirmation of either the Dalai or Panchen must be mutually recognized.\"', 'fr': 'Cependant le 10e panchen-lama lui-mÃªme avait fait une dÃ©claration qui fut citÃ©e dans une publication officielle chinoise Â« Selon l'histoire tibÃ©taine la confirmation du dalaÃ¯-lama ou du panchen-lama doit Ãªtre mutuellement reconnue Â».'}</td>\n",
       "      <td>{'en': 'Choekyi PROPN\n",
       "Gyaltsen PROPN\n",
       "10th ADJ\n",
       "Panchen PROPN\n",
       "Lama PROPN\n",
       "himself PRON\n",
       "declared VERB\n",
       "as SCONJ\n",
       "cited VERB\n",
       "by ADP\n",
       "an DET\n",
       "official ADJ\n",
       "Chinese ADJ\n",
       "review NOUN\n",
       "that SCONJ\n",
       "\" PUNCT\n",
       "according VERB\n",
       "to ADP\n",
       "Tibetan ADJ\n",
       "tradition NOUN\n",
       "the DET\n",
       "confirmation NOUN\n",
       "of ADP\n",
       "either CCONJ\n",
       "the DET\n",
       "Dalai PROPN\n",
       "or CCONJ\n",
       "Panchen PROPN\n",
       "must AUX\n",
       "be AUX\n",
       "mutually ADV\n",
       "recognized VERB\n",
       ". PUNCT\n",
       "\" PUNCT\n",
       "', 'fr': 'Cependant ADV\n",
       "le DET\n",
       "10e PROPN\n",
       "panchen PROPN\n",
       "- PROPN\n",
       "lama PROPN\n",
       "lui-mÃªme PRON\n",
       "avait AUX\n",
       "fait VERB\n",
       "une DET\n",
       "dÃ©claration NOUN\n",
       "qui PRON\n",
       "fut AUX\n",
       "citÃ©e VERB\n",
       "dans ADP\n",
       "une DET\n",
       "publication NOUN\n",
       "officielle ADJ\n",
       "chinoise ADJ\n",
       "Â« ADJ\n",
       "Selon ADP\n",
       "l' DET\n",
       "histoire NOUN\n",
       "tibÃ©taine ADJ\n",
       "la DET\n",
       "confirmation NOUN\n",
       "du ADP\n",
       "dalaÃ¯ NOUN\n",
       "- NOUN\n",
       "lama NOUN\n",
       "ou CCONJ\n",
       "du ADP\n",
       "panchen PROPN\n",
       "- PROPN\n",
       "lama PROPN\n",
       "doit VERB\n",
       "Ãªtre AUX\n",
       "mutuellement ADV\n",
       "reconnue VERB\n",
       "Â» PUNCT\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-1 0-2 1-4 2-2 3-3 4-5 5-6 6-8 6-10 7-12 8-13 9-14 10-15 11-17 12-18 13-16 15-19 16-20 17-21 18-23 19-22 20-24 21-25 22-31 24-26 25-27 25-29 26-30 27-32 27-33 28-35 29-36 30-37 31-38 32-40 33-39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'en': 'The Roland Hayes Committee was formed in 1990 to advocate the induction of Roland Hayes into the Georgia Music Hall of Fame.', 'fr': 'Le Roland Hayes Committee a Ã©tÃ© formÃ© en 1990 pour dÃ©fendre lâ€™implication de Roland Hayes dans le Georgia Music Hall of Fame.'}</td>\n",
       "      <td>{'en': 'The DET\n",
       "Roland PROPN\n",
       "Hayes PROPN\n",
       "Committee PROPN\n",
       "was AUX\n",
       "formed VERB\n",
       "in ADP\n",
       "1990 NUM\n",
       "to PART\n",
       "advocate VERB\n",
       "the DET\n",
       "induction NOUN\n",
       "of ADP\n",
       "Roland PROPN\n",
       "Hayes PROPN\n",
       "into ADP\n",
       "the DET\n",
       "Georgia PROPN\n",
       "Music PROPN\n",
       "Hall PROPN\n",
       "of ADP\n",
       "Fame PROPN\n",
       ". PUNCT\n",
       "', 'fr': 'Le DET\n",
       "Roland PROPN\n",
       "Hayes PROPN\n",
       "Committee NOUN\n",
       "a AUX\n",
       "Ã©tÃ© AUX\n",
       "formÃ© VERB\n",
       "en ADP\n",
       "1990 NUM\n",
       "pour ADP\n",
       "dÃ©fendre VERB\n",
       "lâ€™ SPACE\n",
       "implication NOUN\n",
       "de ADP\n",
       "Roland PROPN\n",
       "Hayes VERB\n",
       "dans ADP\n",
       "le DET\n",
       "Georgia PROPN\n",
       "Music PROPN\n",
       "Hall PROPN\n",
       "of ADP\n",
       "Fame PROPN\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 4-5 5-6 6-7 7-8 8-9 9-10 10-11 11-12 12-13 13-14 14-15 15-16 16-17 17-18 18-19 19-20 20-21 21-22 22-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'en': 'This meant that equipment could be maintained modified and regularly updated by the company.', 'fr': 'Cela permet Ã  l'entreprise de maintenir de mettre Ã  jour et de modifier l'Ã©quipement Ã  son grÃ©.'}</td>\n",
       "      <td>{'en': 'This PRON\n",
       "meant VERB\n",
       "that DET\n",
       "equipment NOUN\n",
       "could AUX\n",
       "be AUX\n",
       "maintained VERB\n",
       "modified ADJ\n",
       "and CCONJ\n",
       "regularly ADV\n",
       "updated VERB\n",
       "by ADP\n",
       "the DET\n",
       "company NOUN\n",
       ". PUNCT\n",
       "', 'fr': 'Cela PRON\n",
       "permet VERB\n",
       "Ã  ADP\n",
       "l' DET\n",
       "entreprise NOUN\n",
       "de ADP\n",
       "maintenir VERB\n",
       "de ADP\n",
       "mettre VERB\n",
       "Ã  ADP\n",
       "jour NOUN\n",
       "et CCONJ\n",
       "de ADP\n",
       "modifier VERB\n",
       "l' DET\n",
       "Ã©quipement VERB\n",
       "Ã  ADP\n",
       "son DET\n",
       "grÃ© NOUN\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-1 2-2 3-15 4-5 6-6 7-13 8-11 10-13 11-16 12-3 13-4 14-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'en': 'The abdomen has a scattering of long pinkish-grey scales and a triple dorsal line.', 'fr': 'L'abdomen a une dispersion de longues Ã©cailles gris rosÃ© et une triple ligne dorsale.'}</td>\n",
       "      <td>{'en': 'The DET\n",
       "abdomen NOUN\n",
       "has VERB\n",
       "a DET\n",
       "scattering NOUN\n",
       "of ADP\n",
       "long ADJ\n",
       "pinkish NOUN\n",
       "- PUNCT\n",
       "grey NOUN\n",
       "scales NOUN\n",
       "and CCONJ\n",
       "a DET\n",
       "triple ADJ\n",
       "dorsal NOUN\n",
       "line NOUN\n",
       ". PUNCT\n",
       "', 'fr': 'L' DET\n",
       "abdomen NOUN\n",
       "a VERB\n",
       "une DET\n",
       "dispersion NOUN\n",
       "de ADP\n",
       "longues ADJ\n",
       "Ã©cailles ADJ\n",
       "gris VERB\n",
       "rosÃ© VERB\n",
       "et CCONJ\n",
       "une DET\n",
       "triple ADJ\n",
       "ligne NOUN\n",
       "dorsale ADJ\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-9 9-8 10-7 11-10 12-11 13-12 14-14 15-13 15-14 16-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(loaded_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the mBART tokenizer (like we have here), we need to set the source and target languages (so the texts are preprocessed properly). You can check the language codes [here](https://huggingface.co/facebook/mbart-large-cc25) if you are using this notebook on a different pairs of languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mbart\" in model_checkpoint:\n",
    "    tokenizer.src_lang = \"en-XX\"\n",
    "    tokenizer.tgt_lang = \"fr-FR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[10537, 2, 67, 32, 15, 5776, 145, 0], [160, 32, 1036, 5776, 3, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, for Word Alignment encoding we will be using these token values instead of real words to express relatedness of words in 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "en_pos_sp = spacy.load(\"en_core_web_sm\")\n",
    "fr_pos_sp = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "If you are using one of the five T5 checkpoints that require a special prefix to put before the inputs, you should adapt the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"translate English to French: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PoS tags we will use a separate function that will parse the sentences and extract the PoS information from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_pos_tags receives a tokenized input from the model. The tokenization is a bit different from spacy model,\n",
    "so to keep the same dimensions of vectors as in the sentence embeddings for each token in a sentence we will:\n",
    "- decode the token received from the model\n",
    "- get a Part of Speech id for it from Spacy and return it\n",
    "\"\"\"\n",
    "\n",
    "def token_to_pos(token, lang):\n",
    "    if lang == 'en':\n",
    "        decoded = list(en_pos_sp(tokenizer.decode(token)))\n",
    "    elif lang == 'fr':\n",
    "        decoded = list(fr_pos_sp(tokenizer.decode(token)))\n",
    "    return decoded[-1].pos if decoded else -1\n",
    "\n",
    "def get_pos_tags(tokenized_sent, lang):\n",
    "    return list(map(lambda x: token_to_pos(x, lang), tokenized_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Alignment information can be encoded in different ways:\n",
    "- Name: `trg-ids`. Create a vector of the same length as the tokenized input sentence. For each position i in the new vector, find a corresponding word in the original input sentence. Find a connected word from the target sentence and put its tokenized value in the new vector.\n",
    "- Name: `sums`. Create a copy of the input vector. For i-th word that has a connected word in the target sentence, add its value to the tokenized value to the i-th position of the new vector.\n",
    "- Name: `mult`. Same as sums but replaces sums with multiplication of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def encode_wa(tokenized_input, tokenized_target, wa, wa_type):\n",
    "    wa_dict = {int(src): int(trg) for src, trg in map(lambda x: x.split('-'), wa.split())}\n",
    "    n = len(tokenized_input)\n",
    "    m = len(tokenized_target)\n",
    "    if wa_type == 'trg-ids':\n",
    "        wa_emb = [0]*n\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] = tokenized_target[v]\n",
    "        return wa_emb\n",
    "    elif wa_type == 'sums':\n",
    "        wa_emb = deepcopy(tokenized_input)\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] += tokenized_target[v]\n",
    "        return wa_emb\n",
    "    \n",
    "    elif wa_type == 'mult':\n",
    "        wa_emb = deepcopy(tokenized_input)\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] *= tokenized_target[v]\n",
    "        return wa_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags=False\n",
    "wa_type=None\n",
    "\n",
    "def preprocess_function(dataset):\n",
    "    global source_lang, target_lang, pos_tags, wa_type\n",
    "    inputs = [prefix + d[source_lang] for d in dataset[\"translation\"]]\n",
    "    targets = [d[target_lang] for d in dataset[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    if pos_tags:\n",
    "        model_inputs['pos'] = [get_pos_tags(x, 'en') for x in model_inputs['input_ids']]\n",
    "        model_inputs['target_pos'] = [get_pos_tags(y, 'fr') for y in model_inputs['labels']]\n",
    "        \n",
    "    if wa_type:\n",
    "        model_inputs['wa'] = [encode_wa(src, trg, wa, wa_type) for src, trg, wa \\\n",
    "                              in zip(model_inputs['input_ids'],  model_inputs['labels'], dataset[\"wa\"])]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "To apply this function on all the pairs of sentences in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def metric_fn(eval_predictions):\n",
    "    preds, labels = eval_predictions\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # We use -100 to mask labels - replace it with the tokenizer pad token when decoding\n",
    "    # so that no output is emitted for these\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import AdamWeightDecay\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model with no extra tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269cec81cbf949b992b0bed0380e2427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a592caf259a496794dcbd729549421d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags=False\n",
    "wa_tags = False\n",
    "wa_type = None\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['pos', 'wa'])\n",
    "\n",
    "\n",
    "no_anno_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "Note that  we don't get a warning like in our classification example. This means we used all the weights of the pretrained model and there is no randomly initialized head in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "Next we set some parameters like the learning rate and the `batch_size`and customize the weight decay. \n",
    "\n",
    "The last two arguments are to setup everything so we can push the model to the [Hub](https://huggingface.co/models) at the end of training. Remove the two of them if you didn't follow the installation steps at the top of the notebook, otherwise you can change the value of push_to_hub_model_id to something you would prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1\n",
    "\n",
    "#model_name = model_checkpoint.split(\"/\")[-1]\n",
    "#push_to_hub_model_id = f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels. Note that our data collators are multi-framework, so make sure you set `return_tensors='tf'` so you get `tf.Tensor` objects back and not something else!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert our datasets to `tf.data.Dataset`, which Keras understands natively. There are two ways to do this - we can use the slightly more low-level [`Dataset.to_tf_dataset()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset) method, or we can use [`Model.prepare_tf_dataset()`](https://huggingface.co/docs/transformers/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset). The main difference between these two is that the `Model` method can inspect the model to determine which column names it can use as input, which means you don't need to specify them yourself. Make sure to specify the collator we just created as our `collate_fn`!\n",
    "\n",
    "We also want to compute `BLEU` metrics, which will require us to generate text from our model. To speed things up, we can compile our generation loop with XLA. This results in a *huge* speedup - up to 100X! The downside of XLA generation, though, is that it doesn't like variable input shapes, because it needs to run a new compilation for each new input shape! To compensate for that, let's use `pad_to_multiple_of` for the dataset we use for text generation. This will reduce the number of unique input shapes a lot, meaning we can get the benefits of XLA generation with only a few compilations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model.prepare_tf_dataset(\n",
    "    no_anno_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "validation_dataset = model.prepare_tf_dataset(\n",
    "    no_anno_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "generation_dataset = model.prepare_tf_dataset(\n",
    "    no_anno_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize our loss and optimizer and compile the model. Note that most Transformers models compute loss internally, so we can just leave the loss argument blank to use the internal loss instead. For the optimizer, we can use the `AdamWeightDecay` optimizer in the Transformer library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our model. We can also add a few optional callbacks here, which you can remove if they aren't useful to you. In no particular order, these are:\n",
    "- PushToHubCallback will sync up our model with the Hub - this allows us to resume training from other machines, share the model after training is finished, and even test the model's inference quality midway through training!\n",
    "- TensorBoard is a built-in Keras callback that logs TensorBoard metrics.\n",
    "- KerasMetricCallback is a callback for computing advanced metrics. There are a number of common metrics in NLP like ROUGE which are hard to fit into your compiled training loop because they depend on decoding predictions and labels back to strings with the tokenizer, and calling arbitrary Python functions to compute the metric. The KerasMetricCallback will wrap a metric function, outputting metrics as training progresses.\n",
    "\n",
    "If this is the first time you've seen `KerasMetricCallback`, it's worth explaining what exactly is going on here. The callback takes two main arguments - a `metric_fn` and an `eval_dataset`. It then iterates over the `eval_dataset` and collects the model's outputs for each sample, before passing the `list` of predictions and the associated `list` of labels to the user-defined `metric_fn`. If the `predict_with_generate` argument is `True`, then it will call `model.generate()` for each input sample instead of `model.predict()` - this is useful for metrics that expect generated text from the model, like `ROUGE` and `BLEU`.\n",
    "\n",
    "This callback allows complex metrics to be computed each epoch that would not function as a standard Keras Metric. Metric values are printed each epoch, and can be used by other callbacks like `TensorBoard` or `EarlyStopping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the metric callback ready, now we can specify the other callbacks and fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1522s 16s/step - loss: 1.5214 - val_loss: 1.4031 - bleu: 18.7710 - gen_len: 53.7116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa09cc22b20>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./translation_model_save/logs\")\n",
    "\n",
    "\"\"\"push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"./translation_model_save\",\n",
    "    tokenizer=tokenizer,\n",
    "    hub_model_id=push_to_hub_model_id,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#callbacks = [metric_callback, tensorboard_callback, push_to_hub_callback]\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with no extra features is **18.7710**. This is our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model with PoS features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell can run pretty slow and can take 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a174ce77dce45dc8b3e8a4970521bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67549e86cc6b4ef3a66b80deff187c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "# Pos_tags need to be set to True in the cell\n",
    "pos_tags = True\n",
    "wa_tags = False\n",
    "wa_type = None\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['wa'])\n",
    "\n",
    "pos_anno_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_pos = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_pos = DataCollatorForSeq2Seq(tokenizer, model=model_with_pos, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_pos = DataCollatorForSeq2Seq(tokenizer, model=model_with_pos, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator_pos,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator_pos,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator_pos,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_pos.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 5074s 54s/step - loss: 1.5240 - val_loss: 1.3968 - bleu: 21.8055 - gen_len: 48.4577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d10a13a00>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./translation_model_save/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_pos.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with POS features only is **22.47899**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model with WA (vector with target ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2861dcecd5c64c09bfa0880849ba7139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21086d43e3684bf791ba034fa38142b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags = False\n",
    "wa_tags = True\n",
    "wa_type=\"trg-ids\"\n",
    "\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['pos'])\n",
    "wa_trg_id_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_wa_trg_ids = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_wa_trg_ids = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_trg_ids, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_wa_trg_ids = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_trg_ids, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_wa_trg_ids.prepare_tf_dataset(\n",
    "    wa_trg_id_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=generation_data_collator_wa_trg_ids,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_wa_trg_ids.prepare_tf_dataset(\n",
    "    wa_trg_id_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_trg_ids,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_wa_trg_ids.prepare_tf_dataset(\n",
    "    wa_trg_id_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_trg_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_wa_trg_ids.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 2004s 21s/step - loss: 1.5207 - val_loss: 1.4006 - bleu: 22.7393 - gen_len: 46.9921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9e37066880>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./wa_trg_ids/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_wa_trg_ids.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with WA alignment using target indeces is **31.5155**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model with WA (using sum of token ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abc3932bbab43a79e9214b0b8ff42ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f873028e854b4e97b7f006db7c3275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags = False\n",
    "wa_tags = True\n",
    "wa_type =\"sums\"\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['pos'])\n",
    "wa_sums_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_wa_sums = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_wa_sums = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_sums, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_wa_sums = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_sums, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_wa_sums.prepare_tf_dataset(\n",
    "    wa_sums_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=generation_data_collator_wa_sums,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_wa_sums.prepare_tf_dataset(\n",
    "    wa_sums_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_sums,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_wa_sums.prepare_tf_dataset(\n",
    "    wa_sums_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_sums,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_wa_sums.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1900s 20s/step - loss: 1.5205 - val_loss: 1.4024 - bleu: 20.2693 - gen_len: 50.7725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d8493f070>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./wa_trg_ids/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_wa_sums.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with WA alignment using sums of related words' indeces is **23.2457**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model with WA (using multiplication of token ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145283f150f44b7d9e1bcc1df3502e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1f831edb3e40f7b16c5f5a1f0c04b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags = False\n",
    "wa_tags = True\n",
    "wa_type=\"mult\"\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['pos'])\n",
    "wa_mult_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_wa_mult = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_wa_mult = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_mult, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_wa_mult = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_mult, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_wa_mult.prepare_tf_dataset(\n",
    "    wa_mult_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=generation_data_collator_wa_mult,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_wa_mult.prepare_tf_dataset(\n",
    "    wa_mult_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_mult,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_wa_mult.prepare_tf_dataset(\n",
    "    wa_mult_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_mult,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_wa_mult.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 2004s 21s/step - loss: 1.5170 - val_loss: 1.4007 - bleu: 18.0217 - gen_len: 56.3280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d89518be0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./wa_mult/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_wa_mult.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with WA alignment using sums of related words' indeces is **26.4307**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotation</th>\n",
       "      <th>BLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No annotation</td>\n",
       "      <td>18.7710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Part of Speech</td>\n",
       "      <td>21.8055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Word alignment: target token ids</td>\n",
       "      <td>22.7393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word alignment: sum of token ids</td>\n",
       "      <td>20.2693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Word alignment: multiplication of token ids</td>\n",
       "      <td>18.0217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Annotation     BLEU\n",
       "0                                No annotation  18.7710\n",
       "1                               Part of Speech  21.8055\n",
       "2             Word alignment: target token ids  22.7393\n",
       "3             Word alignment: sum of token ids  20.2693\n",
       "4  Word alignment: multiplication of token ids  18.0217"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Annotation','BLEU'])\n",
    "results_list = [\n",
    "    ('No annotation', 18.7710),\n",
    "    ('Part of Speech', 21.8055),\n",
    "    ('Word alignment: target token ids', 22.7393),\n",
    "    ('Word alignment: sum of token ids', 20.2693),\n",
    "    ('Word alignment: multiplication of token ids', 18.0217)\n",
    "]\n",
    "results.append([{'Annotation': x[0], 'BLEU': x[1]} for x in results_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation with the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've trained our model, let's see how we could load it and use it to translate text in future! First, let's load it from the hub. This means we can resume the code from here without needing to rerun everything above every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try tokenizing some text and passing it to the model to generate a translation. Don't forget to add the \"translate: \" string at the start if you're using a `T5` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[59513   134   148   157    76   558    53    34 27645 34846   366 13369\n",
      "    175 12143     5   371  1049     3     0 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513]], shape=(1, 128), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "input_text  = \"I'm not actually a very competent Romanian speaker, but let's try our best.\"\n",
    "\n",
    "tokenized = tokenizer([input_text], return_tensors='np')\n",
    "# In the line below use the variable name of the model you want to test\n",
    "out = model.generate(**tokenized, max_length=128)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's some tokens and a lot of padding! Let's decode those to see what it says, using the `skip_special_tokens` argument to skip those padding tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En fait je ne suis pas un orateur roumain trÃ¨s compÃ©tent mais faisons de notre mieux.\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Translation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
