{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers and ðŸ¤— Datasets. Uncomment the following cell and run it. We also use the `sacrebleu` and `sentencepiece` libraries - you may need to install these even if you already have ðŸ¤— Transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MOsHUjgdIrIW",
    "outputId": "f84a093e-147f-470e-aad9-80fb51193c8e"
   },
   "outputs": [],
   "source": [
    "#! pip install transformers[sentencepiece] datasets\n",
    "#! pip install sacrebleu sentencepiece\n",
    "#! pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow==2.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
    "\n",
    "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then uncomment the following cell and input your token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/egoliakova/.huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to install Git-LFS and setup Git if you haven't already. Uncomment the following instructions and adapt with your name and email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install git-lfs\n",
    "# !git config --global user.email \"you@example.com\"\n",
    "# !git config --global user.name \"Your Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your version of Transformers is at least 4.16.0 since some of the functionality we use was introduced in that version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.21.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFASsisvIrIb"
   },
   "source": [
    "You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert CSV-file to a dataset-ready format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below works with a specifically formatted csv. Run the cell below to format your CSV accordingly.\n",
    "Your CSV should have at least 2 columns `en` and `xx` where xx is the code of the target language.\n",
    "\n",
    "If the CSV file has PoS tags for source and target language, the expected column names for them are:\n",
    "`pos_en` and `pos_xx`. \n",
    "\n",
    "If the CSV file has WA tags, the expected column name is `wa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# source_lang accepted value = 'en'\n",
    "# target_lang accepted values = 'fr'|'zh'\n",
    "# Choose pos_tags=True if the file has PoS tags for the both languages\n",
    "# Choose wa_tags=True if the file has WA tags.\n",
    "# Choose store=True if you want to create a json dump of the file that can be used later\n",
    "\n",
    "def csv_to_dataset(filename, source_lang, target_lang, pos_tags=False, wa_tags=False, store=False):\n",
    "    data = pd.read_csv(filename)\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['translation'] = [{source_lang: x, target_lang: y} for x, y in zip(data[source_lang], data[target_lang])]\n",
    "    if pos_tags:\n",
    "        new_df['pos'] = [{source_lang: x, target_lang: y} for x, y in zip(data[f'pos_{source_lang}'], data[f'pos_{target_lang}'])]\n",
    "    if wa_tags:\n",
    "        new_df['wa'] = data['wa']\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = csv_to_dataset('en_fr_pos_wa_anno.csv', 'en', 'fr', pos_tags=True, wa_tags=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = Dataset.from_pandas(test).train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a translation task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTCFado4IrIc"
   },
   "source": [
    "In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) model for a translation task. We will use the [WMT dataset](http://www.statmt.org/wmt16/), a machine translation dataset composed from a collection of various sources, including news commentaries and parliament proceedings.\n",
    "\n",
    "![Widget inference on a translation task](images/translation.png)\n",
    "\n",
    "We will see how to easily load the dataset for this task using ðŸ¤— Datasets and how to fine-tune a model on it using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "This notebook is built to run  with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a sequence-to-sequence version in the Transformers library. Here we picked the [`Helsinki-NLP/opus-mt-en-romance`](https://huggingface.co/Helsinki-NLP/opus-mt-en-ROMANCE) checkpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the `datasets` function `load_dataset` and the `evaluate` function `load`. We use the English/Romanian part of the WMT dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "metric = load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>pos</th>\n",
       "      <th>wa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'en': 'Sydney rescued 47 out of 225 men from the Italian destroyer and thirty six more escaped on rafts but only six of them were later found alive by Italian submarine Topazio almost 20 days later.', 'fr': 'Le Sydney sauva 47 hommes du destroyer italien et six autres furent retrouvÃ©s vivants par un sous-marin italien prÃ¨s de 20 jours plus tard.'}</td>\n",
       "      <td>{'en': 'Sydney PROPN\n",
       "rescued VERB\n",
       "47 NUM\n",
       "out ADP\n",
       "of ADP\n",
       "225 NUM\n",
       "men NOUN\n",
       "from ADP\n",
       "the DET\n",
       "Italian ADJ\n",
       "destroyer NOUN\n",
       "and CCONJ\n",
       "thirty NUM\n",
       "six NUM\n",
       "more ADV\n",
       "escaped ADJ\n",
       "on ADP\n",
       "rafts NOUN\n",
       "but CCONJ\n",
       "only ADV\n",
       "six NUM\n",
       "of ADP\n",
       "them PRON\n",
       "were AUX\n",
       "later ADV\n",
       "found VERB\n",
       "alive ADJ\n",
       "by ADP\n",
       "Italian ADJ\n",
       "submarine NOUN\n",
       "Topazio PROPN\n",
       "almost ADV\n",
       "20 NUM\n",
       "days NOUN\n",
       "later ADV\n",
       ". PUNCT\n",
       "', 'fr': 'Le DET\n",
       "Sydney PROPN\n",
       "sauva ADJ\n",
       "47 NUM\n",
       "hommes NOUN\n",
       "du ADP\n",
       "destroyer NOUN\n",
       "italien ADJ\n",
       "et CCONJ\n",
       "six NUM\n",
       "autres ADJ\n",
       "furent AUX\n",
       "retrouvÃ©s VERB\n",
       "vivants NOUN\n",
       "par ADP\n",
       "un DET\n",
       "sous-marin NOUN\n",
       "italien ADJ\n",
       "prÃ¨s ADV\n",
       "de ADP\n",
       "20 NUM\n",
       "jours NOUN\n",
       "plus ADV\n",
       "tard ADV\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-1 1-2 2-3 4-19 5-3 6-4 7-5 8-0 9-7 10-6 11-8 13-9 14-10 16-15 18-8 20-9 22-12 23-11 25-12 26-13 27-14 28-17 29-16 30-16 31-18 32-20 33-21 34-22 34-23 35-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'en': 'The National Ignition Facility (NIF) is a large laser-based inertial confinement fusion (ICF) research device located at the Lawrence Livermore National Laboratory in Livermore California.', 'fr': 'Le National Ignition Facility ou NIF est un laser de recherche extrÃªmement Ã©nergÃ©tique construit au sein du Lawrence Livermore National Laboratory Ã  Livermore (Californie Ã‰tats-Unis).'}</td>\n",
       "      <td>{'en': 'The DET\n",
       "National PROPN\n",
       "Ignition PROPN\n",
       "Facility PROPN\n",
       "( PUNCT\n",
       "NIF PROPN\n",
       ") PUNCT\n",
       "is AUX\n",
       "a DET\n",
       "large ADJ\n",
       "laser NOUN\n",
       "- PUNCT\n",
       "based VERB\n",
       "inertial ADJ\n",
       "confinement NOUN\n",
       "fusion NOUN\n",
       "( PUNCT\n",
       "ICF PROPN\n",
       ") PUNCT\n",
       "research NOUN\n",
       "device NOUN\n",
       "located VERB\n",
       "at ADP\n",
       "the DET\n",
       "Lawrence PROPN\n",
       "Livermore PROPN\n",
       "National PROPN\n",
       "Laboratory PROPN\n",
       "in ADP\n",
       "Livermore PROPN\n",
       "California PROPN\n",
       ". PUNCT\n",
       "', 'fr': 'Le DET\n",
       "National NOUN\n",
       "Ignition PROPN\n",
       "Facility PROPN\n",
       "ou CCONJ\n",
       "NIF PROPN\n",
       "est AUX\n",
       "un DET\n",
       "laser NOUN\n",
       "de ADP\n",
       "recherche NOUN\n",
       "extrÃªmement ADV\n",
       "Ã©nergÃ©tique ADJ\n",
       "construit ADJ\n",
       "au ADP\n",
       "sein NOUN\n",
       "du ADP\n",
       "Lawrence PROPN\n",
       "Livermore PROPN\n",
       "National PROPN\n",
       "Laboratory PROPN\n",
       "Ã  ADP\n",
       "Livermore PROPN\n",
       "( PUNCT\n",
       "Californie NOUN\n",
       "Ã‰tats-Unis PROPN\n",
       ") PUNCT\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 6-26 7-6 8-7 9-11 10-8 11-9 13-12 17-5 19-10 21-13 22-15 23-16 24-17 25-18 26-19 27-20 28-21 29-22 30-24 30-25 31-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'en': 'The French king comes to battle them at Cayeux (Cayeux-en-Santerre or Cayeux-sur-Mer).', 'fr': 'Le roi de France se porte Ã  leur rencontre Ã  Cayeux (Cayeux-sur-Mer).'}</td>\n",
       "      <td>{'en': 'The DET\n",
       "French ADJ\n",
       "king NOUN\n",
       "comes VERB\n",
       "to PART\n",
       "battle VERB\n",
       "them PRON\n",
       "at ADP\n",
       "Cayeux PROPN\n",
       "( PUNCT\n",
       "Cayeux PROPN\n",
       "- PUNCT\n",
       "en PROPN\n",
       "- PUNCT\n",
       "Santerre PROPN\n",
       "or CCONJ\n",
       "Cayeux PROPN\n",
       "- PUNCT\n",
       "sur ADJ\n",
       "- PUNCT\n",
       "Mer PROPN\n",
       ") PUNCT\n",
       ". PUNCT\n",
       "', 'fr': 'Le DET\n",
       "roi NOUN\n",
       "de ADP\n",
       "France PROPN\n",
       "se PRON\n",
       "porte VERB\n",
       "Ã  ADP\n",
       "leur DET\n",
       "rencontre NOUN\n",
       "Ã  ADP\n",
       "Cayeux PROPN\n",
       "( PUNCT\n",
       "Cayeux-sur-Mer NOUN\n",
       ") PUNCT\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-3 2-1 2-2 3-4 3-5 4-6 5-8 6-7 7-9 8-10 9-11 10-12 11-12 12-12 13-12 14-12 15-11 16-12 17-12 18-12 19-12 20-12 21-13 22-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'en': 'They were partially inspired by the actions of the Greenham Common Women's Peace Camp.', 'fr': 'Elles s'inspirent en partie des actions du camp de la femme de Greenham..'}</td>\n",
       "      <td>{'en': 'They PRON\n",
       "were AUX\n",
       "partially ADV\n",
       "inspired VERB\n",
       "by ADP\n",
       "the DET\n",
       "actions NOUN\n",
       "of ADP\n",
       "the DET\n",
       "Greenham PROPN\n",
       "Common PROPN\n",
       "Women PROPN\n",
       "'s PART\n",
       "Peace PROPN\n",
       "Camp PROPN\n",
       ". PUNCT\n",
       "', 'fr': 'Elles PRON\n",
       "s' PRON\n",
       "inspirent ADV\n",
       "en ADP\n",
       "partie NOUN\n",
       "des DET\n",
       "actions NOUN\n",
       "du ADP\n",
       "camp NOUN\n",
       "de ADP\n",
       "la DET\n",
       "femme NOUN\n",
       "de ADP\n",
       "Greenham PROPN\n",
       ".. PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-2 2-1 3-2 5-5 6-6 7-7 8-10 9-13 11-11 14-8 15-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'en': 'He married Margaret de Multon Baroness Multon of Gilsland.', 'fr': 'Il Ã©pouse avant 1319 Margaret Multon baronne Multon de Gilsland.'}</td>\n",
       "      <td>{'en': 'He PRON\n",
       "married VERB\n",
       "Margaret PROPN\n",
       "de PROPN\n",
       "Multon PROPN\n",
       "Baroness PROPN\n",
       "Multon PROPN\n",
       "of ADP\n",
       "Gilsland PROPN\n",
       ". PUNCT\n",
       "', 'fr': 'Il PRON\n",
       "Ã©pouse VERB\n",
       "avant ADP\n",
       "1319 NUM\n",
       "Margaret PROPN\n",
       "Multon PROPN\n",
       "baronne ADJ\n",
       "Multon PROPN\n",
       "de ADP\n",
       "Gilsland PROPN\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-1 2-4 3-8 4-5 5-6 6-7 7-8 8-9 9-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the mBART tokenizer (like we have here), we need to set the source and target languages (so the texts are preprocessed properly). You can check the language codes [here](https://huggingface.co/facebook/mbart-large-cc25) if you are using this notebook on a different pairs of languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mbart\" in model_checkpoint:\n",
    "    tokenizer.src_lang = \"en-XX\"\n",
    "    tokenizer.tgt_lang = \"fr-FR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[10537, 2, 67, 32, 15, 5776, 145, 0], [160, 32, 1036, 5776, 3, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, for Word Alignment encoding we will be using these token values instead of real words to express relatedness of words in 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "en_pos_sp = spacy.load(\"en_core_web_sm\")\n",
    "fr_pos_sp = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "If you are using one of the five T5 checkpoints that require a special prefix to put before the inputs, you should adapt the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"translate English to French: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PoS tags we will use a separate function that will parse the sentences and extract the PoS information from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_pos_tags receives a tokenized input from the model. The tokenization is a bit different from spacy model,\n",
    "so to keep the same dimensions of vectors as in the sentence embeddings for each token in a sentence we will:\n",
    "- decode the token received from the model\n",
    "- get a Part of Speech id for it from Spacy and return it\n",
    "\"\"\"\n",
    "\n",
    "def token_to_pos(token, lang):\n",
    "    if lang == 'en':\n",
    "        decoded = list(en_pos_sp(tokenizer.decode(token)))\n",
    "    elif lang == 'fr':\n",
    "        decoded = list(fr_pos_sp(tokenizer.decode(token)))\n",
    "    return decoded[-1].pos if decoded else -1\n",
    "\n",
    "def get_pos_tags(tokenized_sent, lang):\n",
    "    return list(map(lambda x: token_to_pos(x, lang), tokenized_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Alignment information can be encoded in different ways:\n",
    "- Name: `trg-ids`. Create a vector of the same length as the tokenized input sentence. For each position i in the new vector, find a corresponding word in the original input sentence. Find a connected word from the target sentence and put its tokenized value in the new vector.\n",
    "- Name: `sums`. Create a copy of the input vector. For i-th word that has a connected word in the target sentence, add its value to the tokenized value to the i-th position of the new vector.\n",
    "- Name: `mult`. Same as sums but replaces sums with multiplication of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def encode_wa(tokenized_input, tokenized_target, wa, wa_type):\n",
    "    wa_dict = {int(src): int(trg) for src, trg in map(lambda x: x.split('-'), wa.split())}\n",
    "    n = len(tokenized_input)\n",
    "    m = len(tokenized_target)\n",
    "    if wa_type == 'trg-ids':\n",
    "        wa_emb = [0]*n\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] = tokenized_target[v]\n",
    "        return wa_emb\n",
    "    elif wa_type == 'sums':\n",
    "        wa_emb = deepcopy(tokenized_input)\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] += tokenized_target[v]\n",
    "        return wa_emb\n",
    "    \n",
    "    elif wa_type == 'mult':\n",
    "        wa_emb = deepcopy(tokenized_input)\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] *= tokenized_target[v]\n",
    "        return wa_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags=False\n",
    "wa_type=None\n",
    "\n",
    "def preprocess_function(dataset):\n",
    "    global source_lang, target_lang, pos_tags, wa_type\n",
    "    inputs = [prefix + d[source_lang] for d in dataset[\"translation\"]]\n",
    "    targets = [d[target_lang] for d in dataset[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    if pos_tags:\n",
    "        model_inputs['pos'] = [get_pos_tags(x, 'en') for x in model_inputs['input_ids']]\n",
    "        model_inputs['target_pos'] = [get_pos_tags(y, 'fr') for y in model_inputs['labels']]\n",
    "        \n",
    "    if wa_type:\n",
    "        model_inputs['wa'] = [encode_wa(src, trg, wa, wa_type) for src, trg, wa \\\n",
    "                              in zip(model_inputs['input_ids'],  model_inputs['labels'], dataset[\"wa\"])]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "To apply this function on all the pairs of sentences in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def metric_fn(eval_predictions):\n",
    "    preds, labels = eval_predictions\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # We use -100 to mask labels - replace it with the tokenizer pad token when decoding\n",
    "    # so that no output is emitted for these\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import AdamWeightDecay\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model with no extra tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f859cc6cfe44ec87b33fbf2cdf5269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa92dc2dc9a4f8b80a1a1ceb0e82567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags=False\n",
    "wa_tags = False\n",
    "wa_type = None\n",
    "\n",
    "test = csv_to_dataset('en_fr_pos_wa_anno.csv', 'en', 'fr', pos_tags=pos_tags, wa_tags=wa_tags)\n",
    "raw_datasets = Dataset.from_pandas(test).train_test_split(test_size=0.2)\n",
    "\n",
    "\n",
    "no_anno_dataset = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "Note that  we don't get a warning like in our classification example. This means we used all the weights of the pretrained model and there is no randomly initialized head in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "Next we set some parameters like the learning rate and the `batch_size`and customize the weight decay. \n",
    "\n",
    "The last two arguments are to setup everything so we can push the model to the [Hub](https://huggingface.co/models) at the end of training. Remove the two of them if you didn't follow the installation steps at the top of the notebook, otherwise you can change the value of push_to_hub_model_id to something you would prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1\n",
    "\n",
    "#model_name = model_checkpoint.split(\"/\")[-1]\n",
    "#push_to_hub_model_id = f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels. Note that our data collators are multi-framework, so make sure you set `return_tensors='tf'` so you get `tf.Tensor` objects back and not something else!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert our datasets to `tf.data.Dataset`, which Keras understands natively. There are two ways to do this - we can use the slightly more low-level [`Dataset.to_tf_dataset()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset) method, or we can use [`Model.prepare_tf_dataset()`](https://huggingface.co/docs/transformers/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset). The main difference between these two is that the `Model` method can inspect the model to determine which column names it can use as input, which means you don't need to specify them yourself. Make sure to specify the collator we just created as our `collate_fn`!\n",
    "\n",
    "We also want to compute `BLEU` metrics, which will require us to generate text from our model. To speed things up, we can compile our generation loop with XLA. This results in a *huge* speedup - up to 100X! The downside of XLA generation, though, is that it doesn't like variable input shapes, because it needs to run a new compilation for each new input shape! To compensate for that, let's use `pad_to_multiple_of` for the dataset we use for text generation. This will reduce the number of unique input shapes a lot, meaning we can get the benefits of XLA generation with only a few compilations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model.prepare_tf_dataset(\n",
    "    no_anno_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "validation_dataset = model.prepare_tf_dataset(\n",
    "    no_anno_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "generation_dataset = model.prepare_tf_dataset(\n",
    "    no_anno_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize our loss and optimizer and compile the model. Note that most Transformers models compute loss internally, so we can just leave the loss argument blank to use the internal loss instead. For the optimizer, we can use the `AdamWeightDecay` optimizer in the Transformer library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our model. We can also add a few optional callbacks here, which you can remove if they aren't useful to you. In no particular order, these are:\n",
    "- PushToHubCallback will sync up our model with the Hub - this allows us to resume training from other machines, share the model after training is finished, and even test the model's inference quality midway through training!\n",
    "- TensorBoard is a built-in Keras callback that logs TensorBoard metrics.\n",
    "- KerasMetricCallback is a callback for computing advanced metrics. There are a number of common metrics in NLP like ROUGE which are hard to fit into your compiled training loop because they depend on decoding predictions and labels back to strings with the tokenizer, and calling arbitrary Python functions to compute the metric. The KerasMetricCallback will wrap a metric function, outputting metrics as training progresses.\n",
    "\n",
    "If this is the first time you've seen `KerasMetricCallback`, it's worth explaining what exactly is going on here. The callback takes two main arguments - a `metric_fn` and an `eval_dataset`. It then iterates over the `eval_dataset` and collects the model's outputs for each sample, before passing the `list` of predictions and the associated `list` of labels to the user-defined `metric_fn`. If the `predict_with_generate` argument is `True`, then it will call `model.generate()` for each input sample instead of `model.predict()` - this is useful for metrics that expect generated text from the model, like `ROUGE` and `BLEU`.\n",
    "\n",
    "This callback allows complex metrics to be computed each epoch that would not function as a standard Keras Metric. Metric values are printed each epoch, and can be used by other callbacks like `TensorBoard` or `EarlyStopping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the metric callback ready, now we can specify the other callbacks and fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1376s 14s/step - loss: 1.5058 - val_loss: 1.4396 - bleu: 22.7236 - gen_len: 48.6111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc07929e910>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./translation_model_save/logs\")\n",
    "\n",
    "\"\"\"push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"./translation_model_save\",\n",
    "    tokenizer=tokenizer,\n",
    "    hub_model_id=push_to_hub_model_id,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#callbacks = [metric_callback, tensorboard_callback, push_to_hub_callback]\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with no extra features is **22.7236**. This is our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model with PoS features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell can run pretty slow and can take 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882a1d42fbbd47fd8d2a032eac99d6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15297fc0ed254397a036f5aed4e0a121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "# Pos_tags need to be set to True in the cell\n",
    "pos_tags = True\n",
    "wa_tags = False\n",
    "wa_type = None\n",
    "\n",
    "test = csv_to_dataset('en_fr_pos_wa_anno.csv', 'en', 'fr', pos_tags=pos_tags, wa_tags=wa_tags)\n",
    "raw_datasets = Dataset.from_pandas(test).train_test_split(test_size=0.2)\n",
    "\n",
    "pos_anno_dataset = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_pos = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_pos = DataCollatorForSeq2Seq(tokenizer, model=model_with_pos, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_pos = DataCollatorForSeq2Seq(tokenizer, model=model_with_pos, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator_pos,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator_pos,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator_pos,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_pos.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 2605s 27s/step - loss: 1.5307 - val_loss: 1.3788 - bleu: 22.4789 - gen_len: 47.3439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc07829bf10>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./translation_model_save/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_pos.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with POS features only is **22.47899**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model with WA (vector with target ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057f0b7f4f464e48ac08a16a2628ff4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8234b401b7f4cb2bbd46ff6da4d43cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags = False\n",
    "wa_tags = True\n",
    "wa_type=\"trg-ids\"\n",
    "\n",
    "test = csv_to_dataset('en_fr_pos_wa_anno.csv', 'en', 'fr', pos_tags=pos_tags, wa_tags=wa_tags)\n",
    "raw_datasets = Dataset.from_pandas(test).train_test_split(test_size=0.2)\n",
    "\n",
    "wa_trg_id_dataset = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_wa_trg_ids = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_wa_trg_ids = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_trg_ids, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_wa_trg_ids = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_trg_ids, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_wa_trg_ids.prepare_tf_dataset(\n",
    "    wa_trg_id_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=generation_data_collator_wa_trg_ids,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_wa_trg_ids.prepare_tf_dataset(\n",
    "    wa_trg_id_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_trg_ids,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_wa_trg_ids.prepare_tf_dataset(\n",
    "    wa_trg_id_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_trg_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_wa_trg_ids.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1813s 19s/step - loss: 1.5150 - val_loss: 1.3766 - bleu: 31.5155 - gen_len: 38.7249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc07904d520>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./wa_trg_ids/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_wa_trg_ids.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with WA alignment using target indeces is **31.5155**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model with WA (using sum of token ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9c876ecd2041c696fcb08e9997e69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf35188dc0f4d379c05910bd37b6692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags = False\n",
    "wa_tags = True\n",
    "wa_type =\"sums\"\n",
    "\n",
    "test = csv_to_dataset('en_fr_pos_wa_anno.csv', 'en', 'fr', pos_tags=pos_tags, wa_tags=wa_tags)\n",
    "raw_datasets = Dataset.from_pandas(test).train_test_split(test_size=0.2)\n",
    "\n",
    "wa_sums_dataset = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_wa_sums = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_wa_sums = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_sums, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_wa_sums = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_sums, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_wa_sums.prepare_tf_dataset(\n",
    "    wa_sums_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=generation_data_collator_wa_sums,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_wa_sums.prepare_tf_dataset(\n",
    "    wa_sums_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_sums,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_wa_sums.prepare_tf_dataset(\n",
    "    wa_sums_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_sums,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_wa_sums.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1808s 19s/step - loss: 1.5250 - val_loss: 1.4002 - bleu: 23.2457 - gen_len: 49.0291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc079287dc0>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./wa_trg_ids/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_wa_sums.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with WA alignment using sums of related words' indeces is **23.2457**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model with WA (using multiplication of token ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfc37e1cdc3470483b30f03b5a13be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494b6496024849b3bbe30b93e55dfe14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags = False\n",
    "wa_tags = True\n",
    "wa_type=\"mult\"\n",
    "\n",
    "test = csv_to_dataset('en_fr_pos_wa_anno.csv', 'en', 'fr', pos_tags=pos_tags, wa_tags=wa_tags)\n",
    "raw_datasets = Dataset.from_pandas(test).train_test_split(test_size=0.2)\n",
    "\n",
    "wa_mult_dataset = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_wa_mult = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_wa_mult = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_mult, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_wa_mult = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_mult, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_wa_mult.prepare_tf_dataset(\n",
    "    wa_mult_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=generation_data_collator_wa_mult,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_wa_mult.prepare_tf_dataset(\n",
    "    wa_mult_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_mult,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_wa_mult.prepare_tf_dataset(\n",
    "    wa_mult_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_mult,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_wa_mult.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1768s 19s/step - loss: 1.5196 - val_loss: 1.3942 - bleu: 26.4307 - gen_len: 43.6799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc078404c10>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./wa_mult/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_wa_mult.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with WA alignment using sums of related words' indeces is **26.4307**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotation</th>\n",
       "      <th>BLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No annotation</td>\n",
       "      <td>22.7236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Part of Speech</td>\n",
       "      <td>22.4789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Word alignment: target token ids</td>\n",
       "      <td>31.5155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word alignment: sum of token ids</td>\n",
       "      <td>23.2457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Word alignment: multiplication of token ids</td>\n",
       "      <td>26.4307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Annotation     BLEU\n",
       "0                                No annotation  22.7236\n",
       "1                               Part of Speech  22.4789\n",
       "2             Word alignment: target token ids  31.5155\n",
       "3             Word alignment: sum of token ids  23.2457\n",
       "4  Word alignment: multiplication of token ids  26.4307"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Annotation','BLEU'])\n",
    "results_list = [\n",
    "    ('No annotation', 22.7236),\n",
    "    ('Part of Speech', 22.4789),\n",
    "    ('Word alignment: target token ids', 31.5155),\n",
    "    ('Word alignment: sum of token ids', 23.2457),\n",
    "    ('Word alignment: multiplication of token ids', 26.4307)\n",
    "]\n",
    "results.append([{'Annotation': x[0], 'BLEU': x[1]} for x in results_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation with the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've trained our model, let's see how we could load it and use it to translate text in future! First, let's load it from the hub. This means we can resume the code from here without needing to rerun everything above every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try tokenizing some text and passing it to the model to generate a translation. Don't forget to add the \"translate: \" string at the start if you're using a `T5` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[59513   134   148   157    76   558    53    34 27645 34846   366 13369\n",
      "    175 12143     5   371  1049     3     0 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513]], shape=(1, 128), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "input_text  = \"I'm not actually a very competent Romanian speaker, but let's try our best.\"\n",
    "\n",
    "tokenized = tokenizer([input_text], return_tensors='np')\n",
    "# In the line below use the variable name of the model you want to test\n",
    "out = model.generate(**tokenized, max_length=128)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's some tokens and a lot of padding! Let's decode those to see what it says, using the `skip_special_tokens` argument to skip those padding tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En fait je ne suis pas un orateur roumain trÃ¨s compÃ©tent mais faisons de notre mieux.\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Translation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
