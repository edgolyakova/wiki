{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install 🤗 Transformers and 🤗 Datasets. Uncomment the following cell and run it. We also use the `sacrebleu` and `sentencepiece` libraries - you may need to install these even if you already have 🤗 Transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MOsHUjgdIrIW",
    "outputId": "f84a093e-147f-470e-aad9-80fb51193c8e"
   },
   "outputs": [],
   "source": [
    "#! pip install transformers[sentencepiece] datasets\n",
    "#! pip install sacrebleu sentencepiece\n",
    "#! pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
    "\n",
    "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then uncomment the following cell and input your token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to install Git-LFS and setup Git if you haven't already. Uncomment the following instructions and adapt with your name and email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install git-lfs\n",
    "# !git config --global user.email \"you@example.com\"\n",
    "# !git config --global user.name \"Your Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your version of Transformers is at least 4.16.0 since some of the functionality we use was introduced in that version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFASsisvIrIb"
   },
   "source": [
    "You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert CSV-file to a dataset-ready format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below works with a specifically formatted csv. Run the cell below to format your CSV accordingly.\n",
    "Your CSV should have at least 2 columns `en` and `xx` where xx is the code of the target language.\n",
    "\n",
    "If the CSV file has PoS tags for source and target language, the expected column names for them are:\n",
    "`pos_en` and `pos_xx`. \n",
    "\n",
    "If the CSV file has WA tags, the expected column name is `wa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# source_lang accepted value = 'en'\n",
    "# target_lang accepted values = 'fr'|'zh'\n",
    "# Choose pos_tags=True if the file has PoS tags for the both languages\n",
    "# Choose wa_tags=True if the file has WA tags.\n",
    "# Choose store=True if you want to create a json dump of the file that can be used later\n",
    "\n",
    "def csv_to_dataset(filename, source_lang, target_lang, pos_tags=False, wa_tags=False, store=False):\n",
    "    data = pd.read_csv(filename)\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['translation'] = [{source_lang: x, target_lang: y} for x, y in zip(data[source_lang], data[target_lang])]\n",
    "    if pos_tags:\n",
    "        new_df['pos'] = [{source_lang: x, target_lang: y} for x, y in zip(data[f'pos_{source_lang}'], data[f'pos_{target_lang}'])]\n",
    "    if wa_tags:\n",
    "        new_df['wa'] = data['wa']\n",
    "    return Dataset.from_pandas(new_df).train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "loaded_dataset = load_from_disk('fr_dataset_split.hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1508\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 378\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dataset.remove_columns(['pos', 'wa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a translation task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTCFado4IrIc"
   },
   "source": [
    "In this notebook, we will see how to fine-tune one of the [🤗 Transformers](https://github.com/huggingface/transformers) model for a translation task. We will use the [WMT dataset](http://www.statmt.org/wmt16/), a machine translation dataset composed from a collection of various sources, including news commentaries and parliament proceedings.\n",
    "\n",
    "![Widget inference on a translation task](images/translation.png)\n",
    "\n",
    "We will see how to easily load the dataset for this task using 🤗 Datasets and how to fine-tune a model on it using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "This notebook is built to run  with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a sequence-to-sequence version in the Transformers library. Here we picked the [`Helsinki-NLP/opus-mt-en-romance`](https://huggingface.co/Helsinki-NLP/opus-mt-en-ROMANCE) checkpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will use the [🤗 Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the `datasets` function `load_dataset` and the `evaluate` function `load`. We use the English/Romanian part of the WMT dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "metric = load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>pos</th>\n",
       "      <th>wa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'en': 'The Chickasaw began to trade with the British after the colony of Carolina was founded in 1670.', 'fr': 'Les Chicachas ont commencé à commercer avec les Britanniques après la fondation de la colonie de la province de Caroline en 1670.'}</td>\n",
       "      <td>{'en': 'The DET\n",
       "Chickasaw PROPN\n",
       "began VERB\n",
       "to PART\n",
       "trade VERB\n",
       "with ADP\n",
       "the DET\n",
       "British ADJ\n",
       "after SCONJ\n",
       "the DET\n",
       "colony NOUN\n",
       "of ADP\n",
       "Carolina PROPN\n",
       "was AUX\n",
       "founded VERB\n",
       "in ADP\n",
       "1670 NUM\n",
       ". PUNCT\n",
       "', 'fr': 'Les DET\n",
       "Chicachas PROPN\n",
       "ont AUX\n",
       "commencé VERB\n",
       "à ADP\n",
       "commercer VERB\n",
       "avec ADP\n",
       "les DET\n",
       "Britanniques NOUN\n",
       "après ADP\n",
       "la DET\n",
       "fondation NOUN\n",
       "de ADP\n",
       "la DET\n",
       "colonie NOUN\n",
       "de ADP\n",
       "la DET\n",
       "province NOUN\n",
       "de ADP\n",
       "Caroline NOUN\n",
       "en ADP\n",
       "1670 NUM\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-1 2-2 2-3 3-4 4-5 5-6 6-7 7-8 8-9 9-13 10-14 10-17 11-18 12-19 13-12 14-11 15-20 16-21 17-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'en': 'The genes for mitosomal components are contained in the nuclear genome.', 'fr': 'Les gènes codant les composants mitosomaux font partie du génome nucléaire.'}</td>\n",
       "      <td>{'en': 'The DET\n",
       "genes NOUN\n",
       "for ADP\n",
       "mitosomal ADJ\n",
       "components NOUN\n",
       "are AUX\n",
       "contained VERB\n",
       "in ADP\n",
       "the DET\n",
       "nuclear ADJ\n",
       "genome NOUN\n",
       ". PUNCT\n",
       "', 'fr': 'Les DET\n",
       "gènes NOUN\n",
       "codant VERB\n",
       "les DET\n",
       "composants NOUN\n",
       "mitosomaux ADJ\n",
       "font VERB\n",
       "partie NOUN\n",
       "du ADP\n",
       "génome NOUN\n",
       "nucléaire ADJ\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-1 2-2 3-5 4-4 5-6 7-7 8-8 9-10 10-9 11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'en': 'After hearing of the many Sendai men already appointed Takusaburō joined the school in 1880.', 'fr': 'Après avoir entendu mentionnés de nombreux hommes de Sendai déjà nommés Takusaburō rejoint l'école en 1880.'}</td>\n",
       "      <td>{'en': 'After ADP\n",
       "hearing NOUN\n",
       "of ADP\n",
       "the DET\n",
       "many ADJ\n",
       "Sendai PROPN\n",
       "men NOUN\n",
       "already ADV\n",
       "appointed VERB\n",
       "Takusaburō PROPN\n",
       "joined VERB\n",
       "the DET\n",
       "school NOUN\n",
       "in ADP\n",
       "1880 NUM\n",
       ". PUNCT\n",
       "', 'fr': 'Après ADP\n",
       "avoir AUX\n",
       "entendu VERB\n",
       "mentionnés VERB\n",
       "de DET\n",
       "nombreux ADJ\n",
       "hommes NOUN\n",
       "de ADP\n",
       "Sendai PROPN\n",
       "déjà ADV\n",
       "nommés VERB\n",
       "Takusaburō PROPN\n",
       "rejoint VERB\n",
       "l' DET\n",
       "école NOUN\n",
       "en ADP\n",
       "1880 NUM\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-2 2-3 3-4 4-5 5-8 6-6 7-9 8-10 9-11 10-12 11-13 12-14 13-15 14-16 15-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'en': 'Renville was named after counties in Minnesota and North Dakota.', 'fr': 'Il portait le nom de comtés du Minnesota et du Dakota du Nord.'}</td>\n",
       "      <td>{'en': 'Renville PROPN\n",
       "was AUX\n",
       "named VERB\n",
       "after ADP\n",
       "counties NOUN\n",
       "in ADP\n",
       "Minnesota PROPN\n",
       "and CCONJ\n",
       "North PROPN\n",
       "Dakota PROPN\n",
       ". PUNCT\n",
       "', 'fr': 'Il PRON\n",
       "portait VERB\n",
       "le DET\n",
       "nom NOUN\n",
       "de ADP\n",
       "comtés NOUN\n",
       "du ADP\n",
       "Minnesota PROPN\n",
       "et CCONJ\n",
       "du ADP\n",
       "Dakota NOUN\n",
       "du ADP\n",
       "Nord NOUN\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-5 5-6 6-7 7-8 8-12 9-10 10-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'en': 'The Japanese governmental shipyards were overwhelmed with the volume of construction and for the first time civilian shipyards were also assigned to produce warships.', 'fr': 'Les chantiers navals gouvernementaux japonais ont été submergés par le volume de la commande et pour la première fois des chantiers navals civils ont été affectés pour produire des navires de guerre.'}</td>\n",
       "      <td>{'en': 'The DET\n",
       "Japanese ADJ\n",
       "governmental ADJ\n",
       "shipyards NOUN\n",
       "were AUX\n",
       "overwhelmed VERB\n",
       "with ADP\n",
       "the DET\n",
       "volume NOUN\n",
       "of ADP\n",
       "construction NOUN\n",
       "and CCONJ\n",
       "for ADP\n",
       "the DET\n",
       "first ADJ\n",
       "time NOUN\n",
       "civilian ADJ\n",
       "shipyards NOUN\n",
       "were AUX\n",
       "also ADV\n",
       "assigned VERB\n",
       "to PART\n",
       "produce VERB\n",
       "warships NOUN\n",
       ". PUNCT\n",
       "', 'fr': 'Les DET\n",
       "chantiers NOUN\n",
       "navals ADJ\n",
       "gouvernementaux ADJ\n",
       "japonais NOUN\n",
       "ont AUX\n",
       "été AUX\n",
       "submergés VERB\n",
       "par ADP\n",
       "le DET\n",
       "volume NOUN\n",
       "de ADP\n",
       "la DET\n",
       "commande NOUN\n",
       "et CCONJ\n",
       "pour ADP\n",
       "la DET\n",
       "première ADJ\n",
       "fois NOUN\n",
       "des ADP\n",
       "chantiers NOUN\n",
       "navals ADJ\n",
       "civils ADJ\n",
       "ont AUX\n",
       "été AUX\n",
       "affectés VERB\n",
       "pour ADP\n",
       "produire VERB\n",
       "des ADP\n",
       "navires NOUN\n",
       "de ADP\n",
       "guerre NOUN\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-4 2-3 3-1 3-2 4-5 4-6 5-7 6-8 7-9 8-10 9-11 9-12 10-13 11-14 12-15 13-16 14-17 15-18 16-22 17-20 17-21 18-23 18-24 19-25 20-25 21-26 22-27 23-29 23-31 24-32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(loaded_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the mBART tokenizer (like we have here), we need to set the source and target languages (so the texts are preprocessed properly). You can check the language codes [here](https://huggingface.co/facebook/mbart-large-cc25) if you are using this notebook on a different pairs of languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mbart\" in model_checkpoint:\n",
    "    tokenizer.src_lang = \"en-XX\"\n",
    "    tokenizer.tgt_lang = \"fr-FR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[10537, 2, 67, 32, 15, 5776, 145, 0], [160, 32, 1036, 5776, 3, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, for Word Alignment encoding we will be using these token values instead of real words to express relatedness of words in 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "en_pos_sp = spacy.load(\"en_core_web_sm\")\n",
    "fr_pos_sp = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "If you are using one of the five T5 checkpoints that require a special prefix to put before the inputs, you should adapt the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"translate English to French: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PoS tags we will use a separate function that will parse the sentences and extract the PoS information from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_pos_tags receives a tokenized input from the model. The tokenization is a bit different from spacy model,\n",
    "so to keep the same dimensions of vectors as in the sentence embeddings for each token in a sentence we will:\n",
    "- decode the token received from the model\n",
    "- get a Part of Speech id for it from Spacy and return it\n",
    "\"\"\"\n",
    "\n",
    "def token_to_pos(token, lang):\n",
    "    if lang == 'en':\n",
    "        decoded = list(en_pos_sp(tokenizer.decode(token)))\n",
    "    elif lang == 'fr':\n",
    "        decoded = list(fr_pos_sp(tokenizer.decode(token)))\n",
    "    return decoded[-1].pos if decoded else -1\n",
    "\n",
    "def get_pos_tags(tokenized_sent, lang):\n",
    "    return list(map(lambda x: token_to_pos(x, lang), tokenized_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Alignment information can be encoded in different ways:\n",
    "- Name: `trg-ids`. Create a vector of the same length as the tokenized input sentence. For each position i in the new vector, find a corresponding word in the original input sentence. Find a connected word from the target sentence and put its tokenized value in the new vector.\n",
    "- Name: `sums`. Create a copy of the input vector. For i-th word that has a connected word in the target sentence, add its value to the tokenized value to the i-th position of the new vector.\n",
    "- Name: `mult`. Same as sums but replaces sums with multiplication of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def encode_wa(tokenized_input, tokenized_target, wa, wa_type):\n",
    "    wa_dict = {int(src): int(trg) for src, trg in map(lambda x: x.split('-'), wa.split())}\n",
    "    n = len(tokenized_input)\n",
    "    m = len(tokenized_target)\n",
    "    if wa_type == 'trg-ids':\n",
    "        wa_emb = [0]*n\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] = tokenized_target[v]\n",
    "        return wa_emb\n",
    "    elif wa_type == 'sums':\n",
    "        wa_emb = deepcopy(tokenized_input)\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] += tokenized_target[v]\n",
    "        return wa_emb\n",
    "    \n",
    "    elif wa_type == 'mult':\n",
    "        wa_emb = deepcopy(tokenized_input)\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] *= tokenized_target[v]\n",
    "        return wa_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags=False\n",
    "wa_type=None\n",
    "\n",
    "def preprocess_function(dataset):\n",
    "    global source_lang, target_lang, pos_tags, wa_type\n",
    "    inputs = [prefix + d[source_lang] for d in dataset[\"translation\"]]\n",
    "    targets = [d[target_lang] for d in dataset[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    if pos_tags:\n",
    "        model_inputs['pos'] = [get_pos_tags(x, 'en') for x in model_inputs['input_ids']]\n",
    "        model_inputs['target_pos'] = [get_pos_tags(y, 'fr') for y in model_inputs['labels']]\n",
    "        \n",
    "    if wa_type:\n",
    "        model_inputs['wa'] = [encode_wa(src, trg, wa, wa_type) for src, trg, wa \\\n",
    "                              in zip(model_inputs['input_ids'],  model_inputs['labels'], dataset[\"wa\"])]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "To apply this function on all the pairs of sentences in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def metric_fn(eval_predictions):\n",
    "    preds, labels = eval_predictions\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # We use -100 to mask labels - replace it with the tokenizer pad token when decoding\n",
    "    # so that no output is emitted for these\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import AdamWeightDecay\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model with no extra tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at fr_dataset_split.hf/train\\cache-4ac5e2a245c8bb7d.arrow\n",
      "Loading cached processed dataset at fr_dataset_split.hf/test\\cache-96397f53f35e1795.arrow\n"
     ]
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags=False\n",
    "wa_tags = False\n",
    "wa_type = None\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['pos', 'wa'])\n",
    "\n",
    "\n",
    "no_anno_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "Note that  we don't get a warning like in our classification example. This means we used all the weights of the pretrained model and there is no randomly initialized head in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "Next we set some parameters like the learning rate and the `batch_size`and customize the weight decay. \n",
    "\n",
    "The last two arguments are to setup everything so we can push the model to the [Hub](https://huggingface.co/models) at the end of training. Remove the two of them if you didn't follow the installation steps at the top of the notebook, otherwise you can change the value of push_to_hub_model_id to something you would prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1\n",
    "\n",
    "#model_name = model_checkpoint.split(\"/\")[-1]\n",
    "#push_to_hub_model_id = f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels. Note that our data collators are multi-framework, so make sure you set `return_tensors='tf'` so you get `tf.Tensor` objects back and not something else!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert our datasets to `tf.data.Dataset`, which Keras understands natively. There are two ways to do this - we can use the slightly more low-level [`Dataset.to_tf_dataset()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset) method, or we can use [`Model.prepare_tf_dataset()`](https://huggingface.co/docs/transformers/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset). The main difference between these two is that the `Model` method can inspect the model to determine which column names it can use as input, which means you don't need to specify them yourself. Make sure to specify the collator we just created as our `collate_fn`!\n",
    "\n",
    "We also want to compute `BLEU` metrics, which will require us to generate text from our model. To speed things up, we can compile our generation loop with XLA. This results in a *huge* speedup - up to 100X! The downside of XLA generation, though, is that it doesn't like variable input shapes, because it needs to run a new compilation for each new input shape! To compensate for that, let's use `pad_to_multiple_of` for the dataset we use for text generation. This will reduce the number of unique input shapes a lot, meaning we can get the benefits of XLA generation with only a few compilations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model.prepare_tf_dataset(\n",
    "    no_anno_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "validation_dataset = model.prepare_tf_dataset(\n",
    "    no_anno_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "generation_dataset = model.prepare_tf_dataset(\n",
    "    no_anno_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize our loss and optimizer and compile the model. Note that most Transformers models compute loss internally, so we can just leave the loss argument blank to use the internal loss instead. For the optimizer, we can use the `AdamWeightDecay` optimizer in the Transformer library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our model. We can also add a few optional callbacks here, which you can remove if they aren't useful to you. In no particular order, these are:\n",
    "- PushToHubCallback will sync up our model with the Hub - this allows us to resume training from other machines, share the model after training is finished, and even test the model's inference quality midway through training!\n",
    "- TensorBoard is a built-in Keras callback that logs TensorBoard metrics.\n",
    "- KerasMetricCallback is a callback for computing advanced metrics. There are a number of common metrics in NLP like ROUGE which are hard to fit into your compiled training loop because they depend on decoding predictions and labels back to strings with the tokenizer, and calling arbitrary Python functions to compute the metric. The KerasMetricCallback will wrap a metric function, outputting metrics as training progresses.\n",
    "\n",
    "If this is the first time you've seen `KerasMetricCallback`, it's worth explaining what exactly is going on here. The callback takes two main arguments - a `metric_fn` and an `eval_dataset`. It then iterates over the `eval_dataset` and collects the model's outputs for each sample, before passing the `list` of predictions and the associated `list` of labels to the user-defined `metric_fn`. If the `predict_with_generate` argument is `True`, then it will call `model.generate()` for each input sample instead of `model.predict()` - this is useful for metrics that expect generated text from the model, like `ROUGE` and `BLEU`.\n",
    "\n",
    "This callback allows complex metrics to be computed each epoch that would not function as a standard Keras Metric. Metric values are printed each epoch, and can be used by other callbacks like `TensorBoard` or `EarlyStopping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the metric callback ready, now we can specify the other callbacks and fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1790s 19s/step - loss: 1.4436 - val_loss: 1.3762 - bleu: 23.9309 - gen_len: 45.0317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fbbee1a0a0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./translation_model_save/logs\")\n",
    "\n",
    "\"\"\"push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"./translation_model_save\",\n",
    "    tokenizer=tokenizer,\n",
    "    hub_model_id=push_to_hub_model_id,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#callbacks = [metric_callback, tensorboard_callback, push_to_hub_callback]\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with no extra features is **18.7710**. This is our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model with PoS features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell can run pretty slow and can take 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f034f9fdd04a20be71f03fdafd9065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\MTproject\\MTvenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8d9c4082ff423f845abfef0335841b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "# Pos_tags need to be set to True in the cell\n",
    "pos_tags = True\n",
    "wa_tags = False\n",
    "wa_type = None\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['wa'])\n",
    "\n",
    "pos_anno_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_pos = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_pos = DataCollatorForSeq2Seq(tokenizer, model=model_with_pos, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_pos = DataCollatorForSeq2Seq(tokenizer, model=model_with_pos, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator_pos,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator_pos,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator_pos,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_pos.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - ETA: 0s - loss: 1.4439WARNING:tensorflow:5 out of the last 5 calls to <function KerasMetricCallback.on_epoch_end.<locals>.generation_function at 0x000001F3C2278B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function KerasMetricCallback.on_epoch_end.<locals>.generation_function at 0x000001F3C2278B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe Kernel s’est bloqué lors de l’exécution du code dans la cellule active ou une cellule précédente. Veuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. Cliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. Pour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./translation_model_save/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_pos.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with POS features only is **22.47899**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model with WA (vector with target ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at fr_dataset_split.hf/train\\cache-da7d73a19c07a55b.arrow\n",
      "Loading cached processed dataset at fr_dataset_split.hf/test\\cache-29b0fe0d33970b62.arrow\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags = False\n",
    "wa_tags = True\n",
    "wa_type=\"trg-ids\"\n",
    "\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['pos'])\n",
    "wa_trg_id_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_wa_trg_ids = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_wa_trg_ids = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_trg_ids, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_wa_trg_ids = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_trg_ids, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_wa_trg_ids.prepare_tf_dataset(\n",
    "    wa_trg_id_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=generation_data_collator_wa_trg_ids,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_wa_trg_ids.prepare_tf_dataset(\n",
    "    wa_trg_id_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_trg_ids,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_wa_trg_ids.prepare_tf_dataset(\n",
    "    wa_trg_id_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_trg_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_wa_trg_ids.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 2145s 23s/step - loss: 1.4369 - val_loss: 1.3800 - bleu: 23.2846 - gen_len: 45.7751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d26ad1deb0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./wa_trg_ids/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_wa_trg_ids.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with WA alignment using target indeces is **31.5155**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model with WA (using sum of token ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abc3932bbab43a79e9214b0b8ff42ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f873028e854b4e97b7f006db7c3275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags = False\n",
    "wa_tags = True\n",
    "wa_type =\"sums\"\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['pos'])\n",
    "wa_sums_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_wa_sums = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_wa_sums = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_sums, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_wa_sums = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_sums, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_wa_sums.prepare_tf_dataset(\n",
    "    wa_sums_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=generation_data_collator_wa_sums,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_wa_sums.prepare_tf_dataset(\n",
    "    wa_sums_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_sums,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_wa_sums.prepare_tf_dataset(\n",
    "    wa_sums_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_sums,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_wa_sums.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1900s 20s/step - loss: 1.5205 - val_loss: 1.4024 - bleu: 20.2693 - gen_len: 50.7725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d8493f070>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./wa_trg_ids/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_wa_sums.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with WA alignment using sums of related words' indeces is **23.2457**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model with WA (using multiplication of token ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145283f150f44b7d9e1bcc1df3502e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1f831edb3e40f7b16c5f5a1f0c04b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags = False\n",
    "wa_tags = True\n",
    "wa_type=\"mult\"\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['pos'])\n",
    "wa_mult_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_wa_mult = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_wa_mult = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_mult, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_wa_mult = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_mult, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_wa_mult.prepare_tf_dataset(\n",
    "    wa_mult_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=generation_data_collator_wa_mult,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_wa_mult.prepare_tf_dataset(\n",
    "    wa_mult_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_mult,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_wa_mult.prepare_tf_dataset(\n",
    "    wa_mult_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_mult,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_wa_mult.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 2004s 21s/step - loss: 1.5170 - val_loss: 1.4007 - bleu: 18.0217 - gen_len: 56.3280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d89518be0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./wa_mult/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_wa_mult.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with WA alignment using sums of related words' indeces is **26.4307**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotation</th>\n",
       "      <th>BLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No annotation</td>\n",
       "      <td>18.7710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Part of Speech</td>\n",
       "      <td>21.8055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Word alignment: target token ids</td>\n",
       "      <td>22.7393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word alignment: sum of token ids</td>\n",
       "      <td>20.2693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Word alignment: multiplication of token ids</td>\n",
       "      <td>18.0217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Annotation     BLEU\n",
       "0                                No annotation  18.7710\n",
       "1                               Part of Speech  21.8055\n",
       "2             Word alignment: target token ids  22.7393\n",
       "3             Word alignment: sum of token ids  20.2693\n",
       "4  Word alignment: multiplication of token ids  18.0217"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Annotation','BLEU'])\n",
    "results_list = [\n",
    "    ('No annotation', 18.7710),\n",
    "    ('Part of Speech', 21.8055),\n",
    "    ('Word alignment: target token ids', 22.7393),\n",
    "    ('Word alignment: sum of token ids', 20.2693),\n",
    "    ('Word alignment: multiplication of token ids', 18.0217)\n",
    "]\n",
    "results.append([{'Annotation': x[0], 'BLEU': x[1]} for x in results_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation with the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've trained our model, let's see how we could load it and use it to translate text in future! First, let's load it from the hub. This means we can resume the code from here without needing to rerun everything above every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try tokenizing some text and passing it to the model to generate a translation. Don't forget to add the \"translate: \" string at the start if you're using a `T5` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[59513   277     8 12720 18389     2    14     6  2914  7750  1078   168\n",
      "    100 21481    51    17     8 43774 21520 23696 30510  2734     2    44\n",
      "     43  2692   274    20     6  1936 30324    19  2955     5 12720    17\n",
      "  13967    20     6 12319 19727  2130     9  6092     3     0 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513]], shape=(1, 128), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "input_text  = \"In Chinese painting, abstraction can be traced to the Tang dynasty painter Wang Mo (王墨), who is credited to have invented the splashed-ink painting style.\"\n",
    "\n",
    "tokenized = tokenizer([input_text], return_tensors='np')\n",
    "# In the line below use the variable name of the model you want to test\n",
    "out = model.generate(**tokenized, max_length=128)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's some tokens and a lot of padding! Let's decode those to see what it says, using the `skip_special_tokens` argument to skip those padding tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans la peinture chinoise, l'abstraction peut être tracée à la dynastie Tang peintre Wang Mo, qui est crédité d'avoir inventé le style de peinture à jet d'éclaboussures.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\MTproject\\MTvenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application to the Vital articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article                                          sentences\n",
       "0  Abstract art  In Chinese painting, abstraction can be traced...\n",
       "1  Abstract art  While none of his paintings remain, this style...\n",
       "2  Abstract art  The Chan buddhist painter Liang Kai (??, c. 11...\n",
       "3  Abstract art  A late Song painter named Yu Jian, adept to Ti...\n",
       "4   Alan Turing  When Turing was 39 years old in 1951, he turne..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital = pd.read_csv(\"C:\\\\Users\\\\Utilisateur\\\\Documents\\\\wiki\\\\scrapping\\\\ver2\\\\articles_clean_ver2\\\\en_only_fr.csv\", sep=\";\", encoding=\"iso-8859-1\")\n",
    "vital.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model without any fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\MTproject\\MTvenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans la peinture chinoise, l'abstraction peut être tracée à la dynastie Tang peintre Wang Mo (??), qui est crédité d'avoir inventé le style de peinture éclaboussée.\n",
      "Bien qu'aucune de ses peintures ne reste, ce style est clairement vu dans certains Song Dynasty Paintings.\n",
      "Le peintre bouddhiste Chan Liang Kai (??, vers 1140=1210) a appliqué le style à la peinture figurative dans son \"Immortal in splashed enk\" dans lequel une représentation précise est sacrifiée pour améliorer la spontanéité liée à l'esprit non-rationnel de l'éclairé.\n",
      "Un peintre de feu Song nommé Yu Jian, adepte du bouddhisme de Tiantai, a créé une série de paysages d'encre éclaboussées qui a finalement inspiré de nombreux peintres japonais Zen.\n",
      "Quand Turing avait 39 ans en 1951, il se tourna vers la biologie mathématique, publiant finalement son chef-d'œuvre \"The Chemical Bases of Morphogenèse\" en janvier 1952.\n",
      "Il s'intéressait à la morphogenèse, au développement de modèles et de formes dans les organismes biologiques.\n",
      "Il a suggéré qu'un système de produits chimiques qui réagissent les uns avec les autres et qui se diffusent dans l'espace, appelé système de diffusion des réactions, pourrait expliquer « les principaux phénomènes de morphogenèse ».\n",
      "Il a utilisé des systèmes d'équations différentielles partielles pour modéliser les réactions chimiques catalytiques.\n",
      "Par exemple, si un catalyseur A est nécessaire pour qu'une certaine réaction chimique ait lieu, et si la réaction produite plus du catalyseur A, alors nous disons que la réaction est autocatalytique, et il y a une rétroaction positive qui peut être modélisée par des équations différentielles non linéaires.\n",
      "Turing a découvert que des patrons pouvaient être créés si la réaction chimique non seulement produisait le catalyseur A, mais également un inhibiteur B qui ralentissait la production de A.\n",
      "Si A et B ont ensuite diffusé à travers le conteneur à des vitesses différentes, alors vous pourriez avoir certaines régions où A a dominé et certaines où B l'a fait.\n",
      "Pour en calculer l'ampleur, Turing aurait eu besoin d'un ordinateur puissant, mais ceux-ci n'étaient pas si librement disponibles en 1951, donc il a dû utiliser des approximations linéaires pour résoudre les équations à la main.\n",
      "Ces calculs ont donné les bons résultats qualitatifs et ont produit, par exemple, un mélange uniforme qui, bizarrement, avait régulièrement espacé des taches rouges fixes.\n",
      "Le biochimiste russe Boris Belousov avait réalisé des expériences avec des résultats similaires, mais n'a pas pu faire publier ses articles à cause du préjugé contemporain selon lequel une telle chose violait la deuxième loi de la thermodynamique.\n",
      "Belousov n'était pas au courant du papier de Turing dans les Transactions Philosophiques de la Société Royale.\n",
      "Bien que publié avant que la structure et le rôle de l'ADN ait été compris, le travail de Turing sur la morphogenèse reste pertinent aujourd'hui et est considéré comme un travail séminal en biologie mathématique.\n",
      "L'une des premières applications du papier de Turing a été le travail de James Murray expliquant les taches et les rayures sur la fourrure des chats, grands et petits.\n",
      "D'autres recherches dans le domaine suggèrent que le travail de Turing peut expliquer en partie la croissance des « plumes, des follicules pileux, le modèle de ramification des poumons, et même l'asymétrie gauche-droite qui place le cœur sur le côté gauche de la poitrine ».\n",
      "En 2012, Sheth et al.\n",
      "Chez la souris, l'élimination des gènes Hox entraîne une augmentation du nombre de chiffres sans augmentation de la taille globale du membre, ce qui suggère que les gènes Hox contrôlent la formation de chiffres en harmonisant la longueur d'onde d'un mécanisme de type Turing.\n",
      "Par la suite, les documents n'ont pas été disponibles avant la publication de Collected Works of A. M. Turing en 1992.\n",
      "Dans le judaïsme humaniste, Talmud est étudié comme un texte historique, afin de découvrir comment il peut démontrer sa pertinence pratique pour vivre aujourd'hui.\n",
      "Une cérémonie religieuse pratiquée au Gabon et au Cameroun est celle des Okuyi, pratiquée par plusieurs groupes ethniques bantous.\n",
      "Dans cet état, selon la région, des rythmes de tambours ou instrumentaux joués par des musiciens respectés (chacun étant unique à une divinité ou à un ancêtre donné), les participants incarnent une divinité ou un ancêtre, une énergie ou un état d'esprit en effectuant des mouvements rituels ou des danses distincts qui renforcent encore leur conscience élevée.\n",
      "Lorsque cet état semblable à la transe est vu et compris, les adhérents sont mis au courant d'une façon de contempler l'incarnation pure ou symbolique d'un état d'esprit ou d'un cadre de référence particulier.\n",
      "Cela renforce les compétences pour séparer les sentiments suscités par cet état d'esprit de leurs manifestations situationnelles dans la vie quotidienne.\n",
      "Une telle séparation et la contemplation subséquente de la nature et des sources d'énergie ou de sentiments purs aident les participants à les gérer et à les accepter lorsqu'ils surviennent dans des contextes banals.\n",
      "En linguistique, la phonétique articulaire est l'étude de la façon dont la langue, les lèvres, la mâchoire, les cordes vocales et d'autres organes de la parole sont utilisés pour faire des sons.\n",
      "Les sons de la parole sont classés par mode d'articulation et lieu d'articulation.\n",
      "Le lieu d'articulation désigne l'endroit où, dans le cou ou la bouche, le courant d'air est restreint.\n",
      "Le mode d'articulation désigne la façon dont les organes de la parole interagissent, comme la façon dont l'air est restreint, quelle forme de courant d'air est utilisée (p. ex.\n",
      "pulmonique, implosif, éjectifs et clics), que les cordes vocales vibrent ou non, et que la cavité nasale soit ouverte au courant d'air.\n",
      "Le concept est principalement utilisé pour la production de consonnes, mais peut être utilisé pour les voyelles dans des qualités telles que la voix et la nasalisation.\n",
      "Pour n'importe quel lieu d'articulation, il peut y avoir plusieurs manières d'articulation, et donc plusieurs consonnes homorganiques.\n",
      "La parole humaine normale est pulmonique, produite avec la pression des poumons, qui crée la phonation dans le glattis dans le larynx, qui est ensuite modifié par le tractus vocal et la bouche en différentes voyelles et consonnes.\n",
      "Cependant, les humains peuvent prononcer des mots sans utiliser les poumons et les glattis dans le discours alaryngéal, dont il existe trois types : le discours oesophagien, le discours pharyngéal et le discours buccal (plus connu sous le nom de Donald Duck).\n",
      "La production de discours est une activité complexe et, par conséquent, les erreurs sont fréquentes, surtout chez les enfants.\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for input_sentence in vital[\"sentences\"]:\n",
    "    tokenized_sentence = tokenizer([input_sentence], return_tensors='np')\n",
    "    out = model.generate(**tokenized_sentence, max_length=128)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        output_sentence = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        print(output_sentence)\n",
    "        outputs.append(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital[\"initial_model\"] = outputs\n",
    "vital.to_csv(\"translations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "      <th>initial_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "      <td>Dans la peinture chinoise, l'abstraction peut ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "      <td>Un peintre de feu Song nommé Yu Jian, adepte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951, il se tourn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article                                          sentences  \\\n",
       "0  Abstract art  In Chinese painting, abstraction can be traced...   \n",
       "1  Abstract art  While none of his paintings remain, this style...   \n",
       "2  Abstract art  The Chan buddhist painter Liang Kai (??, c. 11...   \n",
       "3  Abstract art  A late Song painter named Yu Jian, adept to Ti...   \n",
       "4   Alan Turing  When Turing was 39 years old in 1951, he turne...   \n",
       "\n",
       "                                       initial_model  \n",
       "0  Dans la peinture chinoise, l'abstraction peut ...  \n",
       "1  Bien qu'aucune de ses peintures ne reste, ce s...  \n",
       "2  Le peintre bouddhiste Chan Liang Kai (??, vers...  \n",
       "3  Un peintre de feu Song nommé Yu Jian, adepte d...  \n",
       "4  Quand Turing avait 39 ans en 1951, il se tourn...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuned model: no annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "      <th>initial_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "      <td>Dans la peinture chinoise, l'abstraction peut ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "      <td>Un peintre de feu Song nommé Yu Jian, adepte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951, il se tourn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article                                          sentences  \\\n",
       "0  Abstract art  In Chinese painting, abstraction can be traced...   \n",
       "1  Abstract art  While none of his paintings remain, this style...   \n",
       "2  Abstract art  The Chan buddhist painter Liang Kai (??, c. 11...   \n",
       "3  Abstract art  A late Song painter named Yu Jian, adept to Ti...   \n",
       "4   Alan Turing  When Turing was 39 years old in 1951, he turne...   \n",
       "\n",
       "                                       initial_model  \n",
       "0  Dans la peinture chinoise, l'abstraction peut ...  \n",
       "1  Bien qu'aucune de ses peintures ne reste, ce s...  \n",
       "2  Le peintre bouddhiste Chan Liang Kai (??, vers...  \n",
       "3  Un peintre de feu Song nommé Yu Jian, adepte d...  \n",
       "4  Quand Turing avait 39 ans en 1951, il se tourn...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital = pd.read_csv(\"translations.csv\")\n",
    "vital.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\MTproject\\MTvenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans la peinture chinoise l'abstraction peut être tracée par le peintre de la dynastie Tang Wang Mo (??) qui est crédité d'avoir inventé le style de peinture à jets d'éclaboussures.\n",
      "Bien qu'aucune de ses peintures ne subsiste, ce style est clairement vu dans certains Song Dynasty Paintings.\n",
      "Le peintre bouddhiste Chan Liang Kai (??, vers 1140=1210) a appliqué le style à la peinture figurative dans son « Immortal in splashed enk » dans lequel une représentation précise est sacrifiée pour renforcer la spontanéité liée à l'esprit non rationnel de l'éclairé.\n",
      "Un peintre de feu Song nommé Yu Jian, adepte du Bouddhisme de Tiantai, crée une série de paysages à l'encre éclaboussées qui inspirent finalement de nombreux peintres japonais Zens............................................................................\n",
      "Quand Turing avait 39 ans en 1951 il se tourna vers la biologie mathématique et publia finalement son chef-d'œuvre « The Chemical Bases of Morphogenèse » en janvier 1952.\n",
      "Il s'intéressait à la morphogenèse, au développement de motifs et de formes dans les organismes biologiques......................................................................................................\n",
      "Il suggère qu'un système de produits chimiques réagissant l'un avec l'autre et diffusant à travers l'espace appelé système de diffusion de réaction pourrait expliquer « les principaux phénomènes de morphogenèse ».\n",
      "Il utilise des systèmes d'équations différentielles partielles pour modéliser les réactions chimiques catalytiques.\n",
      "Par exemple, si un catalyseur A est nécessaire pour qu'une certaine réaction chimique ait lieu et si la réaction produite plus du catalyseur A est alors nous disons que la réaction est autocatalytique et qu'il y a un retour positif qui peut être modélisé par des équations différentielles non linéaires.\n",
      "Turing découvre que des motifs peuvent être créés si la réaction chimique produit non seulement le catalyseur A mais aussi un inhibiteur B qui ralentit la production de A.\n",
      "Si A et B se diffusent alors à travers le conteneur à des vitesses différentes, alors vous pouvez avoir certaines régions où A domine et d'autres où B fait.\n",
      "Pour calculer l'étendue de cela, Turing aurait eu besoin d'un ordinateur puissant, mais ceux-ci n'étaient pas si librement disponibles en 1951, il a donc dû utiliser des approximations linéaires pour résoudre les équations à la main.\n",
      "Ces calculs ont donné les bons résultats qualitatifs et ont produit par exemple un mélange uniforme qui a été assez étrangement espacé régulièrement des taches rouges fixes.\n",
      "Le biochimiste russe Boris Belousov avait réalisé des expériences avec des résultats similaires mais ne pouvait pas faire publier ses articles à cause du préjugé contemporain selon lequel une telle chose violait la deuxième loi de la thermodynamique.\n",
      "Belousov n'était pas au courant du journal de Turing dans les Transactions Philosophiques de la Royal Society.\n",
      "Bien que publié avant que la structure et le rôle de l'ADN n'aient été compris, les travaux de Turing sur la morphogenèse restent pertinents aujourd'hui et sont considérés comme un travail séminal en biologie mathématique..............................................................................\n",
      "L'une des premières applications du papier de Turing est le travail de James Murray expliquant les taches et les rayures sur la fourrure des chats, grands et petits.\n",
      "D'autres recherches dans ce domaine suggèrent que les travaux de Turing peuvent expliquer en partie la croissance des « plumes, des follicules pileux, le modèle de ramification des poumons et même l'asymétrie gauche-droite qui place le cœur sur le côté gauche de la poitrine ».\n",
      "En 2012, Sheth et al.\n",
      "Chez la souris, l'élimination des gènes Hox provoque une augmentation du nombre de chiffres sans augmentation de la taille globale du membre, ce qui suggère que les gènes Hox contrôlent la formation de chiffres en harmonisant la longueur d'onde d'un mécanisme de type Turing.\n",
      "Des articles plus tard n'ont pas été disponibles avant que Collected Works of A. M. Turing n'ait été publié en 1992.\n",
      "Au sein du judaïsme humaniste, Talmud est étudié comme un texte historique afin de découvrir comment il peut démontrer sa pertinence pratique pour la vie d'aujourd'hui............................................................................................\n",
      "Une cérémonie religieuse pratiquée au Gabon et au Cameroun est l'Okuyi, pratiquée par plusieurs groupes ethniques bantous.\n",
      "Dans cet état, selon la région des rythmes de tambours ou instrumentaux joués par des musiciens respectés (chacun étant unique à une divinité ou un ancêtre donné), les participants incarnent une divinité ou un ancêtre, une énergie ou un état d'esprit en effectuant des mouvements rituels distincts ou des danses qui renforcent encore leur conscience élevée.\n",
      "Quand cet état de transe est vu et compris, les adhérents sont en mesure de contempler l'incarnation pure ou symbolique d'un état d'esprit particulier ou d'un cadre de référence......................................................................................\n",
      "Cela crée des compétences pour séparer les sentiments suscités par cet état d'esprit de leurs manifestations situationnelles dans la vie de tous les jours..................................................................................................\n",
      "Une telle séparation et la contemplation subséquente de la nature et des sources d'énergie ou de sentiments purs aident les participants à les gérer et à les accepter lorsqu'ils apparaissent dans des contextes mondains.\n",
      "En linguistique, la phonétique articulaire est l'étude de la façon dont la langue, les lèvres, la mâchoire, les cordes vocales et d'autres organes de parole sont utilisés pour faire des sons.\n",
      "Les sons de la parole sont classés par mode d'articulation et lieu d'articulation.\n",
      "Le lieu d'articulation désigne l'endroit où dans le cou ou la bouche le courant d'air est restreint.\n",
      "Le mode d'articulation désigne la façon dont les organes de parole interagissent, comme la façon dont l'air est restreint et quelle forme de courant d'air est utilisée (p. ex.\n",
      "pulmonique, implosif, éjectifs et clics), que les cordes vocales vibrent ou non et que la cavité nasale soit ouverte au courant d'air.\n",
      "Le concept est principalement utilisé pour la production de consonnes mais peut être utilisé pour les voyelles dans des qualités telles que la vocation et la nasalisation.\n",
      "Pour n'importe quel lieu d'articulation il peut y avoir plusieurs manières d'articulation et donc plusieurs consonnes homorganiques.\n",
      "La parole humaine normale est pulmonique, produite avec la pression des poumons qui crée la phonation dans les glattis dans le larynx qui est ensuite modifiée par le chant et la bouche en différentes voyelles et consonnes.\n",
      "Cependant les humains peuvent prononcer des paroles sans utiliser les poumons et les glattis dans le discours alaryngéal, dont il existe trois types : le discours ésophageal, le discours pharyngéal et le discours buccal (plus connu sous le nom de Donald Duck).\n",
      "La production de la parole est une activité complexe et par conséquent les erreurs sont fréquentes, en particulier chez les enfants.\n"
     ]
    }
   ],
   "source": [
    "outputs_no_anno = []\n",
    "\n",
    "for input_sentence in vital[\"sentences\"]:\n",
    "    tokenized_sentence = tokenizer([input_sentence], return_tensors='np')\n",
    "    out = model.generate(**tokenized_sentence, max_length=128)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        output_sentence = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        print(output_sentence)\n",
    "        outputs_no_anno.append(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "      <th>initial_model</th>\n",
       "      <th>ft_no_anno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "      <td>Dans la peinture chinoise, l'abstraction peut ...</td>\n",
       "      <td>Dans la peinture chinoise l'abstraction peut ê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne subsiste, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "      <td>Un peintre de feu Song nommé Yu Jian, adepte d...</td>\n",
       "      <td>Un peintre de feu Song nommé Yu Jian, adepte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951, il se tourn...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951 il se tourna...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article                                          sentences  \\\n",
       "0  Abstract art  In Chinese painting, abstraction can be traced...   \n",
       "1  Abstract art  While none of his paintings remain, this style...   \n",
       "2  Abstract art  The Chan buddhist painter Liang Kai (??, c. 11...   \n",
       "3  Abstract art  A late Song painter named Yu Jian, adept to Ti...   \n",
       "4   Alan Turing  When Turing was 39 years old in 1951, he turne...   \n",
       "\n",
       "                                       initial_model  \\\n",
       "0  Dans la peinture chinoise, l'abstraction peut ...   \n",
       "1  Bien qu'aucune de ses peintures ne reste, ce s...   \n",
       "2  Le peintre bouddhiste Chan Liang Kai (??, vers...   \n",
       "3  Un peintre de feu Song nommé Yu Jian, adepte d...   \n",
       "4  Quand Turing avait 39 ans en 1951, il se tourn...   \n",
       "\n",
       "                                          ft_no_anno  \n",
       "0  Dans la peinture chinoise l'abstraction peut ê...  \n",
       "1  Bien qu'aucune de ses peintures ne subsiste, c...  \n",
       "2  Le peintre bouddhiste Chan Liang Kai (??, vers...  \n",
       "3  Un peintre de feu Song nommé Yu Jian, adepte d...  \n",
       "4  Quand Turing avait 39 ans en 1951 il se tourna...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital[\"ft_no_anno\"] = outputs_no_anno\n",
    "vital.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "      <th>initial_model</th>\n",
       "      <th>ft_no_anno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "      <td>Dans la peinture chinoise, l'abstraction peut ...</td>\n",
       "      <td>Dans la peinture chinoise l'abstraction peut ê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne subsiste, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "      <td>Un peintre de feu Song nommé Yu Jian, adepte d...</td>\n",
       "      <td>Un peintre de feu Song nommé Yu Jian, adepte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951, il se tourn...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951 il se tourna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>He was interested in morphogenesis, the develo...</td>\n",
       "      <td>Il s'intéressait à la morphogenèse, au dévelop...</td>\n",
       "      <td>Il s'intéressait à la morphogenèse, au dévelop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>He suggested that a system of chemicals reacti...</td>\n",
       "      <td>Il a suggéré qu'un système de produits chimiqu...</td>\n",
       "      <td>Il suggère qu'un système de produits chimiques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>He used systems of partial differential equati...</td>\n",
       "      <td>Il a utilisé des systèmes d'équations différen...</td>\n",
       "      <td>Il utilise des systèmes d'équations différenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>For example, if a catalyst A is required for a...</td>\n",
       "      <td>Par exemple, si un catalyseur A est nécessaire...</td>\n",
       "      <td>Par exemple, si un catalyseur A est nécessaire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Turing discovered that patterns could be creat...</td>\n",
       "      <td>Turing a découvert que des patrons pouvaient ê...</td>\n",
       "      <td>Turing découvre que des motifs peuvent être cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>If A and B then diffused through the container...</td>\n",
       "      <td>Si A et B ont ensuite diffusé à travers le con...</td>\n",
       "      <td>Si A et B se diffusent alors à travers le cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>To calculate the extent of this, Turing would ...</td>\n",
       "      <td>Pour en calculer l'ampleur, Turing aurait eu b...</td>\n",
       "      <td>Pour calculer l'étendue de cela, Turing aurait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>These calculations gave the right qualitative ...</td>\n",
       "      <td>Ces calculs ont donné les bons résultats quali...</td>\n",
       "      <td>Ces calculs ont donné les bons résultats quali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>The Russian biochemist Boris Belousov had perf...</td>\n",
       "      <td>Le biochimiste russe Boris Belousov avait réal...</td>\n",
       "      <td>Le biochimiste russe Boris Belousov avait réal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Belousov was not aware of Turing's paper in th...</td>\n",
       "      <td>Belousov n'était pas au courant du papier de T...</td>\n",
       "      <td>Belousov n'était pas au courant du journal de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Although published before the structure and ro...</td>\n",
       "      <td>Bien que publié avant que la structure et le r...</td>\n",
       "      <td>Bien que publié avant que la structure et le r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>One of the early applications of Turing's pape...</td>\n",
       "      <td>L'une des premières applications du papier de ...</td>\n",
       "      <td>L'une des premières applications du papier de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Further research in the area suggests that Tur...</td>\n",
       "      <td>D'autres recherches dans le domaine suggèrent ...</td>\n",
       "      <td>D'autres recherches dans ce domaine suggèrent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>In 2012, Sheth, et al.</td>\n",
       "      <td>En 2012, Sheth et al.</td>\n",
       "      <td>En 2012, Sheth et al.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>found that in mice, removal of Hox genes cause...</td>\n",
       "      <td>Chez la souris, l'élimination des gènes Hox en...</td>\n",
       "      <td>Chez la souris, l'élimination des gènes Hox pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Later papers were not available until Collecte...</td>\n",
       "      <td>Par la suite, les documents n'ont pas été disp...</td>\n",
       "      <td>Des articles plus tard n'ont pas été disponibl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Talmud</td>\n",
       "      <td>Within Humanistic Judaism, Talmud is studied a...</td>\n",
       "      <td>Dans le judaïsme humaniste, Talmud est étudié ...</td>\n",
       "      <td>Au sein du judaïsme humaniste, Talmud est étud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>One religious ceremony practiced in Gabon and ...</td>\n",
       "      <td>Une cérémonie religieuse pratiquée au Gabon et...</td>\n",
       "      <td>Une cérémonie religieuse pratiquée au Gabon et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>In this state, depending upon the region, drum...</td>\n",
       "      <td>Dans cet état, selon la région, des rythmes de...</td>\n",
       "      <td>Dans cet état, selon la région des rythmes de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>When this trance-like state is witnessed and u...</td>\n",
       "      <td>Lorsque cet état semblable à la transe est vu ...</td>\n",
       "      <td>Quand cet état de transe est vu et compris, le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>This builds skills at separating the feelings ...</td>\n",
       "      <td>Cela renforce les compétences pour séparer les...</td>\n",
       "      <td>Cela crée des compétences pour séparer les sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>Such separation and subsequent contemplation o...</td>\n",
       "      <td>Une telle séparation et la contemplation subsé...</td>\n",
       "      <td>Une telle séparation et la contemplation subsé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Speech</td>\n",
       "      <td>In linguistics, articulatory phonetics is the ...</td>\n",
       "      <td>En linguistique, la phonétique articulaire est...</td>\n",
       "      <td>En linguistique, la phonétique articulaire est...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Speech sounds are categorized by manner of art...</td>\n",
       "      <td>Les sons de la parole sont classés par mode d'...</td>\n",
       "      <td>Les sons de la parole sont classés par mode d'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Place of articulation refers to where in the n...</td>\n",
       "      <td>Le lieu d'articulation désigne l'endroit où, d...</td>\n",
       "      <td>Le lieu d'articulation désigne l'endroit où da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Manner of articulation refers to the manner in...</td>\n",
       "      <td>Le mode d'articulation désigne la façon dont l...</td>\n",
       "      <td>Le mode d'articulation désigne la façon dont l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Speech</td>\n",
       "      <td>pulmonic, implosive, ejectives, and clicks), w...</td>\n",
       "      <td>pulmonique, implosif, éjectifs et clics), que ...</td>\n",
       "      <td>pulmonique, implosif, éjectifs et clics), que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Speech</td>\n",
       "      <td>The concept is primarily used for the producti...</td>\n",
       "      <td>Le concept est principalement utilisé pour la ...</td>\n",
       "      <td>Le concept est principalement utilisé pour la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Speech</td>\n",
       "      <td>For any place of articulation, there may be se...</td>\n",
       "      <td>Pour n'importe quel lieu d'articulation, il pe...</td>\n",
       "      <td>Pour n'importe quel lieu d'articulation il peu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Normal human speech is pulmonic, produced with...</td>\n",
       "      <td>La parole humaine normale est pulmonique, prod...</td>\n",
       "      <td>La parole humaine normale est pulmonique, prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Speech</td>\n",
       "      <td>However humans can pronounce words without the...</td>\n",
       "      <td>Cependant, les humains peuvent prononcer des m...</td>\n",
       "      <td>Cependant les humains peuvent prononcer des pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Speech production is a complex activity, and a...</td>\n",
       "      <td>La production de discours est une activité com...</td>\n",
       "      <td>La production de la parole est une activité co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          article  \\\n",
       "0                    Abstract art   \n",
       "1                    Abstract art   \n",
       "2                    Abstract art   \n",
       "3                    Abstract art   \n",
       "4                     Alan Turing   \n",
       "5                     Alan Turing   \n",
       "6                     Alan Turing   \n",
       "7                     Alan Turing   \n",
       "8                     Alan Turing   \n",
       "9                     Alan Turing   \n",
       "10                    Alan Turing   \n",
       "11                    Alan Turing   \n",
       "12                    Alan Turing   \n",
       "13                    Alan Turing   \n",
       "14                    Alan Turing   \n",
       "15                    Alan Turing   \n",
       "16                    Alan Turing   \n",
       "17                    Alan Turing   \n",
       "18                    Alan Turing   \n",
       "19                    Alan Turing   \n",
       "20                    Alan Turing   \n",
       "21                         Talmud   \n",
       "22  Traditional African religions   \n",
       "23  Traditional African religions   \n",
       "24  Traditional African religions   \n",
       "25  Traditional African religions   \n",
       "26  Traditional African religions   \n",
       "27                         Speech   \n",
       "28                         Speech   \n",
       "29                         Speech   \n",
       "30                         Speech   \n",
       "31                         Speech   \n",
       "32                         Speech   \n",
       "33                         Speech   \n",
       "34                         Speech   \n",
       "35                         Speech   \n",
       "36                         Speech   \n",
       "\n",
       "                                            sentences  \\\n",
       "0   In Chinese painting, abstraction can be traced...   \n",
       "1   While none of his paintings remain, this style...   \n",
       "2   The Chan buddhist painter Liang Kai (??, c. 11...   \n",
       "3   A late Song painter named Yu Jian, adept to Ti...   \n",
       "4   When Turing was 39 years old in 1951, he turne...   \n",
       "5   He was interested in morphogenesis, the develo...   \n",
       "6   He suggested that a system of chemicals reacti...   \n",
       "7   He used systems of partial differential equati...   \n",
       "8   For example, if a catalyst A is required for a...   \n",
       "9   Turing discovered that patterns could be creat...   \n",
       "10  If A and B then diffused through the container...   \n",
       "11  To calculate the extent of this, Turing would ...   \n",
       "12  These calculations gave the right qualitative ...   \n",
       "13  The Russian biochemist Boris Belousov had perf...   \n",
       "14  Belousov was not aware of Turing's paper in th...   \n",
       "15  Although published before the structure and ro...   \n",
       "16  One of the early applications of Turing's pape...   \n",
       "17  Further research in the area suggests that Tur...   \n",
       "18                             In 2012, Sheth, et al.   \n",
       "19  found that in mice, removal of Hox genes cause...   \n",
       "20  Later papers were not available until Collecte...   \n",
       "21  Within Humanistic Judaism, Talmud is studied a...   \n",
       "22  One religious ceremony practiced in Gabon and ...   \n",
       "23  In this state, depending upon the region, drum...   \n",
       "24  When this trance-like state is witnessed and u...   \n",
       "25  This builds skills at separating the feelings ...   \n",
       "26  Such separation and subsequent contemplation o...   \n",
       "27  In linguistics, articulatory phonetics is the ...   \n",
       "28  Speech sounds are categorized by manner of art...   \n",
       "29  Place of articulation refers to where in the n...   \n",
       "30  Manner of articulation refers to the manner in...   \n",
       "31  pulmonic, implosive, ejectives, and clicks), w...   \n",
       "32  The concept is primarily used for the producti...   \n",
       "33  For any place of articulation, there may be se...   \n",
       "34  Normal human speech is pulmonic, produced with...   \n",
       "35  However humans can pronounce words without the...   \n",
       "36  Speech production is a complex activity, and a...   \n",
       "\n",
       "                                        initial_model  \\\n",
       "0   Dans la peinture chinoise, l'abstraction peut ...   \n",
       "1   Bien qu'aucune de ses peintures ne reste, ce s...   \n",
       "2   Le peintre bouddhiste Chan Liang Kai (??, vers...   \n",
       "3   Un peintre de feu Song nommé Yu Jian, adepte d...   \n",
       "4   Quand Turing avait 39 ans en 1951, il se tourn...   \n",
       "5   Il s'intéressait à la morphogenèse, au dévelop...   \n",
       "6   Il a suggéré qu'un système de produits chimiqu...   \n",
       "7   Il a utilisé des systèmes d'équations différen...   \n",
       "8   Par exemple, si un catalyseur A est nécessaire...   \n",
       "9   Turing a découvert que des patrons pouvaient ê...   \n",
       "10  Si A et B ont ensuite diffusé à travers le con...   \n",
       "11  Pour en calculer l'ampleur, Turing aurait eu b...   \n",
       "12  Ces calculs ont donné les bons résultats quali...   \n",
       "13  Le biochimiste russe Boris Belousov avait réal...   \n",
       "14  Belousov n'était pas au courant du papier de T...   \n",
       "15  Bien que publié avant que la structure et le r...   \n",
       "16  L'une des premières applications du papier de ...   \n",
       "17  D'autres recherches dans le domaine suggèrent ...   \n",
       "18                              En 2012, Sheth et al.   \n",
       "19  Chez la souris, l'élimination des gènes Hox en...   \n",
       "20  Par la suite, les documents n'ont pas été disp...   \n",
       "21  Dans le judaïsme humaniste, Talmud est étudié ...   \n",
       "22  Une cérémonie religieuse pratiquée au Gabon et...   \n",
       "23  Dans cet état, selon la région, des rythmes de...   \n",
       "24  Lorsque cet état semblable à la transe est vu ...   \n",
       "25  Cela renforce les compétences pour séparer les...   \n",
       "26  Une telle séparation et la contemplation subsé...   \n",
       "27  En linguistique, la phonétique articulaire est...   \n",
       "28  Les sons de la parole sont classés par mode d'...   \n",
       "29  Le lieu d'articulation désigne l'endroit où, d...   \n",
       "30  Le mode d'articulation désigne la façon dont l...   \n",
       "31  pulmonique, implosif, éjectifs et clics), que ...   \n",
       "32  Le concept est principalement utilisé pour la ...   \n",
       "33  Pour n'importe quel lieu d'articulation, il pe...   \n",
       "34  La parole humaine normale est pulmonique, prod...   \n",
       "35  Cependant, les humains peuvent prononcer des m...   \n",
       "36  La production de discours est une activité com...   \n",
       "\n",
       "                                           ft_no_anno  \n",
       "0   Dans la peinture chinoise l'abstraction peut ê...  \n",
       "1   Bien qu'aucune de ses peintures ne subsiste, c...  \n",
       "2   Le peintre bouddhiste Chan Liang Kai (??, vers...  \n",
       "3   Un peintre de feu Song nommé Yu Jian, adepte d...  \n",
       "4   Quand Turing avait 39 ans en 1951 il se tourna...  \n",
       "5   Il s'intéressait à la morphogenèse, au dévelop...  \n",
       "6   Il suggère qu'un système de produits chimiques...  \n",
       "7   Il utilise des systèmes d'équations différenti...  \n",
       "8   Par exemple, si un catalyseur A est nécessaire...  \n",
       "9   Turing découvre que des motifs peuvent être cr...  \n",
       "10  Si A et B se diffusent alors à travers le cont...  \n",
       "11  Pour calculer l'étendue de cela, Turing aurait...  \n",
       "12  Ces calculs ont donné les bons résultats quali...  \n",
       "13  Le biochimiste russe Boris Belousov avait réal...  \n",
       "14  Belousov n'était pas au courant du journal de ...  \n",
       "15  Bien que publié avant que la structure et le r...  \n",
       "16  L'une des premières applications du papier de ...  \n",
       "17  D'autres recherches dans ce domaine suggèrent ...  \n",
       "18                              En 2012, Sheth et al.  \n",
       "19  Chez la souris, l'élimination des gènes Hox pr...  \n",
       "20  Des articles plus tard n'ont pas été disponibl...  \n",
       "21  Au sein du judaïsme humaniste, Talmud est étud...  \n",
       "22  Une cérémonie religieuse pratiquée au Gabon et...  \n",
       "23  Dans cet état, selon la région des rythmes de ...  \n",
       "24  Quand cet état de transe est vu et compris, le...  \n",
       "25  Cela crée des compétences pour séparer les sen...  \n",
       "26  Une telle séparation et la contemplation subsé...  \n",
       "27  En linguistique, la phonétique articulaire est...  \n",
       "28  Les sons de la parole sont classés par mode d'...  \n",
       "29  Le lieu d'articulation désigne l'endroit où da...  \n",
       "30  Le mode d'articulation désigne la façon dont l...  \n",
       "31  pulmonique, implosif, éjectifs et clics), que ...  \n",
       "32  Le concept est principalement utilisé pour la ...  \n",
       "33  Pour n'importe quel lieu d'articulation il peu...  \n",
       "34  La parole humaine normale est pulmonique, prod...  \n",
       "35  Cependant les humains peuvent prononcer des pa...  \n",
       "36  La production de la parole est une activité co...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital.to_csv(\"translations_ft_no.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuned model: POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "      <th>initial_model</th>\n",
       "      <th>ft_no_anno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "      <td>Dans la peinture chinoise, l'abstraction peut ...</td>\n",
       "      <td>Dans la peinture chinoise l'abstraction peut ê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne subsiste, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "      <td>Un peintre de feu Song nommé Yu Jian, adepte d...</td>\n",
       "      <td>Un peintre de feu Song nommé Yu Jian, adepte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951, il se tourn...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951 il se tourna...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article                                          sentences  \\\n",
       "0  Abstract art  In Chinese painting, abstraction can be traced...   \n",
       "1  Abstract art  While none of his paintings remain, this style...   \n",
       "2  Abstract art  The Chan buddhist painter Liang Kai (??, c. 11...   \n",
       "3  Abstract art  A late Song painter named Yu Jian, adept to Ti...   \n",
       "4   Alan Turing  When Turing was 39 years old in 1951, he turne...   \n",
       "\n",
       "                                       initial_model  \\\n",
       "0  Dans la peinture chinoise, l'abstraction peut ...   \n",
       "1  Bien qu'aucune de ses peintures ne reste, ce s...   \n",
       "2  Le peintre bouddhiste Chan Liang Kai (??, vers...   \n",
       "3  Un peintre de feu Song nommé Yu Jian, adepte d...   \n",
       "4  Quand Turing avait 39 ans en 1951, il se tourn...   \n",
       "\n",
       "                                          ft_no_anno  \n",
       "0  Dans la peinture chinoise l'abstraction peut ê...  \n",
       "1  Bien qu'aucune de ses peintures ne subsiste, c...  \n",
       "2  Le peintre bouddhiste Chan Liang Kai (??, vers...  \n",
       "3  Un peintre de feu Song nommé Yu Jian, adepte d...  \n",
       "4  Quand Turing avait 39 ans en 1951 il se tourna...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuned model: WA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital = pd.read_csv(\"translations_ft_no.csv\")\n",
    "vital.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\MTproject\\MTvenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans la peinture chinoise l'abstraction peut être tracée à la dynastie Tang peintre Wang Mo (??), qui est crédité d'avoir inventé le style de peinture éclaboussure-puce.................................................................................\n",
      "Bien qu'aucune de ses peintures ne reste, ce style est clairement vu dans certains Song Dynasty Paintings.....................................................................................................\n",
      "Le peintre bouddhiste de Chan Liang Kai (?? vers 1140=1210) a appliqué le style à la peinture figurative dans son « Immortal in splashed enk » dans lequel une représentation précise est sacrifiée pour améliorer la spontanéité liée à l'esprit non rationnel de l'éclairé.\n",
      "Un peintre de feu Song nommé Yu Jian, adepte du Bouddhisme de Tiantai a créé une série de paysages d'encre éclaboussés qui ont finalement inspiré de nombreux peintres japonais Zen.\n",
      "Quand Turing avait 39 ans en 1951 il se tourna vers la biologie mathématique et publia finalement son chef-d'œuvre « The Chemical Bases of Morphogenèse » en janvier 1952.\n",
      "Il s'intéressait à la morphogenèse, au développement de motifs et de formes dans les organismes biologiques.\n",
      "Il suggère qu'un système de produits chimiques réagissant l'un avec l'autre et diffusant dans l'espace appelé système de diffusion de réaction pourrait expliquer « les principaux phénomènes de morphogenèse ».\n",
      "Il utilise des systèmes d'équations différentielles partielles pour modéliser les réactions chimiques catalytiques.\n",
      "Par exemple, si un catalyseur A est nécessaire pour qu'une certaine réaction chimique ait lieu et si la réaction produite plus du catalyseur A est alors nous disons que la réaction est autocatalytique et qu'il y a un retour positif qui peut être modélisé par des équations différentielles non linéaires.\n",
      "Turing découvre que des motifs peuvent être créés si la réaction chimique produit non seulement le catalyseur A mais aussi un inhibiteur B qui ralentit la production de A.\n",
      "Si A et B ont ensuite diffusé à travers le conteneur à des vitesses différentes, alors vous pourriez avoir certaines régions où A a dominé et certaines où B l'a fait.\n",
      "Pour calculer l'étendue de cela, Turing aurait eu besoin d'un ordinateur puissant, mais ceux-ci n'étaient pas si librement disponibles en 1951, il a donc dû utiliser des approximations linéaires pour résoudre les équations à la main.\n",
      "Ces calculs donnent les bons résultats qualitatifs et produisent par exemple un mélange uniforme qui est étrangement assez espacé régulièrement des taches rouges fixes.\n",
      "Le biochimiste russe Boris Belousov avait réalisé des expériences avec des résultats similaires mais ne pouvait pas faire publier ses articles à cause du préjugé contemporain selon lequel une telle chose violait la deuxième loi de la thermodynamique.\n",
      "Belousov n'était pas au courant du journal de Turing dans les Transactions Philosophiques de la Royal Society.\n",
      "Bien que publié avant que la structure et le rôle de l'ADN aient été compris, le travail de Turing sur la morphogenèse reste pertinent aujourd'hui et est considéré comme un travail séminal en biologie mathématique.\n",
      "L'une des premières applications du papier de Turing fut le travail de James Murray expliquant les taches et les rayures sur la fourrure des chats, grands et petits.\n",
      "D'autres recherches dans ce domaine suggèrent que les travaux de Turing peuvent expliquer en partie la croissance des « plumes, des follicules pileux, le modèle de ramification des poumons et même l'asymétrie gauche-droite qui place le cœur sur le côté gauche de la poitrine ».\n",
      "En 2012, Sheth et al.\n",
      "Chez la souris, l'élimination des gènes Hox provoque une augmentation du nombre de chiffres sans augmentation de la taille globale du membre, ce qui suggère que les gènes Hox contrôlent la formation de chiffres en ajustant la longueur d'onde d'un mécanisme de type Turing.\n",
      "Plus tard les articles n'ont pas été disponibles avant que Collected Works of A. M. Turing n'ait été publié en 1992.\n",
      "Au sein du judaïsme humaniste le Talmud est étudié comme un texte historique afin de découvrir comment il peut démontrer sa pertinence pratique pour la vie d'aujourd'hui............................................................................................\n",
      "Une cérémonie religieuse pratiquée au Gabon et au Cameroun est l'Okuyi pratiqué par plusieurs groupes ethniques bantous.\n",
      "Dans cet état, selon la région, les rythmes de tambour ou instrumentaux joués par des musiciens respectés (chacun étant unique à une divinité ou un ancêtre donné), les participants incarnent une divinité ou un ancêtre, une énergie ou un état d'esprit en effectuant des mouvements rituels distincts ou des danses qui renforcent encore leur conscience élevée.\n",
      "Lorsque cet état de transe est vu et compris, les adhérents sont mis au courant d'une façon de contempler l'incarnation pure ou symbolique d'un état d'esprit particulier ou d'un cadre de référence particulier................................................................................\n",
      "Cela crée des compétences pour séparer les sentiments suscités par cet état d'esprit de leurs manifestations situationnelles dans la vie de tous les jours..................................................................................................\n",
      "Une telle séparation et la contemplation subséquente de la nature et des sources d'énergie pure ou de sentiments aide les participants à les gérer et à les accepter lorsqu'ils apparaissent dans des contextes banals.\n",
      "En linguistique, la phonétique articulaire est l'étude de la façon dont la langue, les lèvres, la mâchoire, les cordes vocales et d'autres organes de parole sont utilisés pour faire des sons.\n",
      "Les sons de la parole sont classés par mode d'articulation et lieu d'articulation.\n",
      "Le lieu d'articulation désigne l'endroit où dans le cou ou la bouche le courant d'air est constrictionné....................................................................................................\n",
      "Le mode d'articulation désigne la manière dont les organes de parole interagissent, comme la manière dont l'air est restreint et quelle forme de courant d'air est utilisée (p. ex.\n",
      "pulmonique, implosif, éjectifs et clics), que les cordes vocales vibrent ou non et que la cavité nasale soit ouverte au courant d'air.\n",
      "Le concept est principalement utilisé pour la production de consonnes mais peut être utilisé pour les voyelles dans des qualités telles que la vocation et la nasalisation.\n",
      "Pour n'importe quel lieu d'articulation il peut y avoir plusieurs manières d'articulation et donc plusieurs consonnes homorganiques.\n",
      "La parole humaine normale est pulmonique, produite avec la pression des poumons qui crée la phonation dans les glattis dans le larynx qui est ensuite modifiée par le chant et la bouche en différentes voyelles et consonnes..........................................................................\n",
      "Cependant les humains peuvent prononcer des paroles sans utiliser les poumons et les glattis dans le discours alaryngéal, dont il existe trois types : le discours ésophageal, le discours pharyngéal et le discours buccal (plus connu sous le nom de Donald Duck).\n",
      "La production de discours est une activité complexe et par conséquent les erreurs sont fréquentes, en particulier chez les enfants et les adolescents......................................................................................................\n"
     ]
    }
   ],
   "source": [
    "outputs_wa = []\n",
    "\n",
    "for input_sentence in vital[\"sentences\"]:\n",
    "    tokenized_sentence = tokenizer([input_sentence], return_tensors='np')\n",
    "    out = model_with_wa_trg_ids.generate(**tokenized_sentence, max_length=128)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        output_sentence = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        print(output_sentence)\n",
    "        outputs_wa.append(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "      <th>initial_model</th>\n",
       "      <th>ft_no_anno</th>\n",
       "      <th>ft_wa_trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "      <td>Dans la peinture chinoise, l'abstraction peut ...</td>\n",
       "      <td>Dans la peinture chinoise l'abstraction peut ê...</td>\n",
       "      <td>Dans la peinture chinoise l'abstraction peut ê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne subsiste, c...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "      <td>Le peintre bouddhiste de Chan Liang Kai (?? ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "      <td>Un peintre de feu Song nommé Yu Jian, adepte d...</td>\n",
       "      <td>Un peintre de feu Song nommé Yu Jian, adepte d...</td>\n",
       "      <td>Un peintre de feu Song nommé Yu Jian, adepte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951, il se tourn...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951 il se tourna...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951 il se tourna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>He was interested in morphogenesis, the develo...</td>\n",
       "      <td>Il s'intéressait à la morphogenèse, au dévelop...</td>\n",
       "      <td>Il s'intéressait à la morphogenèse, au dévelop...</td>\n",
       "      <td>Il s'intéressait à la morphogenèse, au dévelop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>He suggested that a system of chemicals reacti...</td>\n",
       "      <td>Il a suggéré qu'un système de produits chimiqu...</td>\n",
       "      <td>Il suggère qu'un système de produits chimiques...</td>\n",
       "      <td>Il suggère qu'un système de produits chimiques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>He used systems of partial differential equati...</td>\n",
       "      <td>Il a utilisé des systèmes d'équations différen...</td>\n",
       "      <td>Il utilise des systèmes d'équations différenti...</td>\n",
       "      <td>Il utilise des systèmes d'équations différenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>For example, if a catalyst A is required for a...</td>\n",
       "      <td>Par exemple, si un catalyseur A est nécessaire...</td>\n",
       "      <td>Par exemple, si un catalyseur A est nécessaire...</td>\n",
       "      <td>Par exemple, si un catalyseur A est nécessaire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Turing discovered that patterns could be creat...</td>\n",
       "      <td>Turing a découvert que des patrons pouvaient ê...</td>\n",
       "      <td>Turing découvre que des motifs peuvent être cr...</td>\n",
       "      <td>Turing découvre que des motifs peuvent être cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>If A and B then diffused through the container...</td>\n",
       "      <td>Si A et B ont ensuite diffusé à travers le con...</td>\n",
       "      <td>Si A et B se diffusent alors à travers le cont...</td>\n",
       "      <td>Si A et B ont ensuite diffusé à travers le con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>To calculate the extent of this, Turing would ...</td>\n",
       "      <td>Pour en calculer l'ampleur, Turing aurait eu b...</td>\n",
       "      <td>Pour calculer l'étendue de cela, Turing aurait...</td>\n",
       "      <td>Pour calculer l'étendue de cela, Turing aurait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>These calculations gave the right qualitative ...</td>\n",
       "      <td>Ces calculs ont donné les bons résultats quali...</td>\n",
       "      <td>Ces calculs ont donné les bons résultats quali...</td>\n",
       "      <td>Ces calculs donnent les bons résultats qualita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>The Russian biochemist Boris Belousov had perf...</td>\n",
       "      <td>Le biochimiste russe Boris Belousov avait réal...</td>\n",
       "      <td>Le biochimiste russe Boris Belousov avait réal...</td>\n",
       "      <td>Le biochimiste russe Boris Belousov avait réal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Belousov was not aware of Turing's paper in th...</td>\n",
       "      <td>Belousov n'était pas au courant du papier de T...</td>\n",
       "      <td>Belousov n'était pas au courant du journal de ...</td>\n",
       "      <td>Belousov n'était pas au courant du journal de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Although published before the structure and ro...</td>\n",
       "      <td>Bien que publié avant que la structure et le r...</td>\n",
       "      <td>Bien que publié avant que la structure et le r...</td>\n",
       "      <td>Bien que publié avant que la structure et le r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>One of the early applications of Turing's pape...</td>\n",
       "      <td>L'une des premières applications du papier de ...</td>\n",
       "      <td>L'une des premières applications du papier de ...</td>\n",
       "      <td>L'une des premières applications du papier de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Further research in the area suggests that Tur...</td>\n",
       "      <td>D'autres recherches dans le domaine suggèrent ...</td>\n",
       "      <td>D'autres recherches dans ce domaine suggèrent ...</td>\n",
       "      <td>D'autres recherches dans ce domaine suggèrent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>In 2012, Sheth, et al.</td>\n",
       "      <td>En 2012, Sheth et al.</td>\n",
       "      <td>En 2012, Sheth et al.</td>\n",
       "      <td>En 2012, Sheth et al.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>found that in mice, removal of Hox genes cause...</td>\n",
       "      <td>Chez la souris, l'élimination des gènes Hox en...</td>\n",
       "      <td>Chez la souris, l'élimination des gènes Hox pr...</td>\n",
       "      <td>Chez la souris, l'élimination des gènes Hox pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Later papers were not available until Collecte...</td>\n",
       "      <td>Par la suite, les documents n'ont pas été disp...</td>\n",
       "      <td>Des articles plus tard n'ont pas été disponibl...</td>\n",
       "      <td>Plus tard les articles n'ont pas été disponibl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Talmud</td>\n",
       "      <td>Within Humanistic Judaism, Talmud is studied a...</td>\n",
       "      <td>Dans le judaïsme humaniste, Talmud est étudié ...</td>\n",
       "      <td>Au sein du judaïsme humaniste, Talmud est étud...</td>\n",
       "      <td>Au sein du judaïsme humaniste le Talmud est ét...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>One religious ceremony practiced in Gabon and ...</td>\n",
       "      <td>Une cérémonie religieuse pratiquée au Gabon et...</td>\n",
       "      <td>Une cérémonie religieuse pratiquée au Gabon et...</td>\n",
       "      <td>Une cérémonie religieuse pratiquée au Gabon et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>In this state, depending upon the region, drum...</td>\n",
       "      <td>Dans cet état, selon la région, des rythmes de...</td>\n",
       "      <td>Dans cet état, selon la région des rythmes de ...</td>\n",
       "      <td>Dans cet état, selon la région, les rythmes de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>When this trance-like state is witnessed and u...</td>\n",
       "      <td>Lorsque cet état semblable à la transe est vu ...</td>\n",
       "      <td>Quand cet état de transe est vu et compris, le...</td>\n",
       "      <td>Lorsque cet état de transe est vu et compris, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>This builds skills at separating the feelings ...</td>\n",
       "      <td>Cela renforce les compétences pour séparer les...</td>\n",
       "      <td>Cela crée des compétences pour séparer les sen...</td>\n",
       "      <td>Cela crée des compétences pour séparer les sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>Such separation and subsequent contemplation o...</td>\n",
       "      <td>Une telle séparation et la contemplation subsé...</td>\n",
       "      <td>Une telle séparation et la contemplation subsé...</td>\n",
       "      <td>Une telle séparation et la contemplation subsé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Speech</td>\n",
       "      <td>In linguistics, articulatory phonetics is the ...</td>\n",
       "      <td>En linguistique, la phonétique articulaire est...</td>\n",
       "      <td>En linguistique, la phonétique articulaire est...</td>\n",
       "      <td>En linguistique, la phonétique articulaire est...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Speech sounds are categorized by manner of art...</td>\n",
       "      <td>Les sons de la parole sont classés par mode d'...</td>\n",
       "      <td>Les sons de la parole sont classés par mode d'...</td>\n",
       "      <td>Les sons de la parole sont classés par mode d'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Place of articulation refers to where in the n...</td>\n",
       "      <td>Le lieu d'articulation désigne l'endroit où, d...</td>\n",
       "      <td>Le lieu d'articulation désigne l'endroit où da...</td>\n",
       "      <td>Le lieu d'articulation désigne l'endroit où da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Manner of articulation refers to the manner in...</td>\n",
       "      <td>Le mode d'articulation désigne la façon dont l...</td>\n",
       "      <td>Le mode d'articulation désigne la façon dont l...</td>\n",
       "      <td>Le mode d'articulation désigne la manière dont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Speech</td>\n",
       "      <td>pulmonic, implosive, ejectives, and clicks), w...</td>\n",
       "      <td>pulmonique, implosif, éjectifs et clics), que ...</td>\n",
       "      <td>pulmonique, implosif, éjectifs et clics), que ...</td>\n",
       "      <td>pulmonique, implosif, éjectifs et clics), que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Speech</td>\n",
       "      <td>The concept is primarily used for the producti...</td>\n",
       "      <td>Le concept est principalement utilisé pour la ...</td>\n",
       "      <td>Le concept est principalement utilisé pour la ...</td>\n",
       "      <td>Le concept est principalement utilisé pour la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Speech</td>\n",
       "      <td>For any place of articulation, there may be se...</td>\n",
       "      <td>Pour n'importe quel lieu d'articulation, il pe...</td>\n",
       "      <td>Pour n'importe quel lieu d'articulation il peu...</td>\n",
       "      <td>Pour n'importe quel lieu d'articulation il peu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Normal human speech is pulmonic, produced with...</td>\n",
       "      <td>La parole humaine normale est pulmonique, prod...</td>\n",
       "      <td>La parole humaine normale est pulmonique, prod...</td>\n",
       "      <td>La parole humaine normale est pulmonique, prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Speech</td>\n",
       "      <td>However humans can pronounce words without the...</td>\n",
       "      <td>Cependant, les humains peuvent prononcer des m...</td>\n",
       "      <td>Cependant les humains peuvent prononcer des pa...</td>\n",
       "      <td>Cependant les humains peuvent prononcer des pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Speech production is a complex activity, and a...</td>\n",
       "      <td>La production de discours est une activité com...</td>\n",
       "      <td>La production de la parole est une activité co...</td>\n",
       "      <td>La production de discours est une activité com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          article  \\\n",
       "0                    Abstract art   \n",
       "1                    Abstract art   \n",
       "2                    Abstract art   \n",
       "3                    Abstract art   \n",
       "4                     Alan Turing   \n",
       "5                     Alan Turing   \n",
       "6                     Alan Turing   \n",
       "7                     Alan Turing   \n",
       "8                     Alan Turing   \n",
       "9                     Alan Turing   \n",
       "10                    Alan Turing   \n",
       "11                    Alan Turing   \n",
       "12                    Alan Turing   \n",
       "13                    Alan Turing   \n",
       "14                    Alan Turing   \n",
       "15                    Alan Turing   \n",
       "16                    Alan Turing   \n",
       "17                    Alan Turing   \n",
       "18                    Alan Turing   \n",
       "19                    Alan Turing   \n",
       "20                    Alan Turing   \n",
       "21                         Talmud   \n",
       "22  Traditional African religions   \n",
       "23  Traditional African religions   \n",
       "24  Traditional African religions   \n",
       "25  Traditional African religions   \n",
       "26  Traditional African religions   \n",
       "27                         Speech   \n",
       "28                         Speech   \n",
       "29                         Speech   \n",
       "30                         Speech   \n",
       "31                         Speech   \n",
       "32                         Speech   \n",
       "33                         Speech   \n",
       "34                         Speech   \n",
       "35                         Speech   \n",
       "36                         Speech   \n",
       "\n",
       "                                            sentences  \\\n",
       "0   In Chinese painting, abstraction can be traced...   \n",
       "1   While none of his paintings remain, this style...   \n",
       "2   The Chan buddhist painter Liang Kai (??, c. 11...   \n",
       "3   A late Song painter named Yu Jian, adept to Ti...   \n",
       "4   When Turing was 39 years old in 1951, he turne...   \n",
       "5   He was interested in morphogenesis, the develo...   \n",
       "6   He suggested that a system of chemicals reacti...   \n",
       "7   He used systems of partial differential equati...   \n",
       "8   For example, if a catalyst A is required for a...   \n",
       "9   Turing discovered that patterns could be creat...   \n",
       "10  If A and B then diffused through the container...   \n",
       "11  To calculate the extent of this, Turing would ...   \n",
       "12  These calculations gave the right qualitative ...   \n",
       "13  The Russian biochemist Boris Belousov had perf...   \n",
       "14  Belousov was not aware of Turing's paper in th...   \n",
       "15  Although published before the structure and ro...   \n",
       "16  One of the early applications of Turing's pape...   \n",
       "17  Further research in the area suggests that Tur...   \n",
       "18                             In 2012, Sheth, et al.   \n",
       "19  found that in mice, removal of Hox genes cause...   \n",
       "20  Later papers were not available until Collecte...   \n",
       "21  Within Humanistic Judaism, Talmud is studied a...   \n",
       "22  One religious ceremony practiced in Gabon and ...   \n",
       "23  In this state, depending upon the region, drum...   \n",
       "24  When this trance-like state is witnessed and u...   \n",
       "25  This builds skills at separating the feelings ...   \n",
       "26  Such separation and subsequent contemplation o...   \n",
       "27  In linguistics, articulatory phonetics is the ...   \n",
       "28  Speech sounds are categorized by manner of art...   \n",
       "29  Place of articulation refers to where in the n...   \n",
       "30  Manner of articulation refers to the manner in...   \n",
       "31  pulmonic, implosive, ejectives, and clicks), w...   \n",
       "32  The concept is primarily used for the producti...   \n",
       "33  For any place of articulation, there may be se...   \n",
       "34  Normal human speech is pulmonic, produced with...   \n",
       "35  However humans can pronounce words without the...   \n",
       "36  Speech production is a complex activity, and a...   \n",
       "\n",
       "                                        initial_model  \\\n",
       "0   Dans la peinture chinoise, l'abstraction peut ...   \n",
       "1   Bien qu'aucune de ses peintures ne reste, ce s...   \n",
       "2   Le peintre bouddhiste Chan Liang Kai (??, vers...   \n",
       "3   Un peintre de feu Song nommé Yu Jian, adepte d...   \n",
       "4   Quand Turing avait 39 ans en 1951, il se tourn...   \n",
       "5   Il s'intéressait à la morphogenèse, au dévelop...   \n",
       "6   Il a suggéré qu'un système de produits chimiqu...   \n",
       "7   Il a utilisé des systèmes d'équations différen...   \n",
       "8   Par exemple, si un catalyseur A est nécessaire...   \n",
       "9   Turing a découvert que des patrons pouvaient ê...   \n",
       "10  Si A et B ont ensuite diffusé à travers le con...   \n",
       "11  Pour en calculer l'ampleur, Turing aurait eu b...   \n",
       "12  Ces calculs ont donné les bons résultats quali...   \n",
       "13  Le biochimiste russe Boris Belousov avait réal...   \n",
       "14  Belousov n'était pas au courant du papier de T...   \n",
       "15  Bien que publié avant que la structure et le r...   \n",
       "16  L'une des premières applications du papier de ...   \n",
       "17  D'autres recherches dans le domaine suggèrent ...   \n",
       "18                              En 2012, Sheth et al.   \n",
       "19  Chez la souris, l'élimination des gènes Hox en...   \n",
       "20  Par la suite, les documents n'ont pas été disp...   \n",
       "21  Dans le judaïsme humaniste, Talmud est étudié ...   \n",
       "22  Une cérémonie religieuse pratiquée au Gabon et...   \n",
       "23  Dans cet état, selon la région, des rythmes de...   \n",
       "24  Lorsque cet état semblable à la transe est vu ...   \n",
       "25  Cela renforce les compétences pour séparer les...   \n",
       "26  Une telle séparation et la contemplation subsé...   \n",
       "27  En linguistique, la phonétique articulaire est...   \n",
       "28  Les sons de la parole sont classés par mode d'...   \n",
       "29  Le lieu d'articulation désigne l'endroit où, d...   \n",
       "30  Le mode d'articulation désigne la façon dont l...   \n",
       "31  pulmonique, implosif, éjectifs et clics), que ...   \n",
       "32  Le concept est principalement utilisé pour la ...   \n",
       "33  Pour n'importe quel lieu d'articulation, il pe...   \n",
       "34  La parole humaine normale est pulmonique, prod...   \n",
       "35  Cependant, les humains peuvent prononcer des m...   \n",
       "36  La production de discours est une activité com...   \n",
       "\n",
       "                                           ft_no_anno  \\\n",
       "0   Dans la peinture chinoise l'abstraction peut ê...   \n",
       "1   Bien qu'aucune de ses peintures ne subsiste, c...   \n",
       "2   Le peintre bouddhiste Chan Liang Kai (??, vers...   \n",
       "3   Un peintre de feu Song nommé Yu Jian, adepte d...   \n",
       "4   Quand Turing avait 39 ans en 1951 il se tourna...   \n",
       "5   Il s'intéressait à la morphogenèse, au dévelop...   \n",
       "6   Il suggère qu'un système de produits chimiques...   \n",
       "7   Il utilise des systèmes d'équations différenti...   \n",
       "8   Par exemple, si un catalyseur A est nécessaire...   \n",
       "9   Turing découvre que des motifs peuvent être cr...   \n",
       "10  Si A et B se diffusent alors à travers le cont...   \n",
       "11  Pour calculer l'étendue de cela, Turing aurait...   \n",
       "12  Ces calculs ont donné les bons résultats quali...   \n",
       "13  Le biochimiste russe Boris Belousov avait réal...   \n",
       "14  Belousov n'était pas au courant du journal de ...   \n",
       "15  Bien que publié avant que la structure et le r...   \n",
       "16  L'une des premières applications du papier de ...   \n",
       "17  D'autres recherches dans ce domaine suggèrent ...   \n",
       "18                              En 2012, Sheth et al.   \n",
       "19  Chez la souris, l'élimination des gènes Hox pr...   \n",
       "20  Des articles plus tard n'ont pas été disponibl...   \n",
       "21  Au sein du judaïsme humaniste, Talmud est étud...   \n",
       "22  Une cérémonie religieuse pratiquée au Gabon et...   \n",
       "23  Dans cet état, selon la région des rythmes de ...   \n",
       "24  Quand cet état de transe est vu et compris, le...   \n",
       "25  Cela crée des compétences pour séparer les sen...   \n",
       "26  Une telle séparation et la contemplation subsé...   \n",
       "27  En linguistique, la phonétique articulaire est...   \n",
       "28  Les sons de la parole sont classés par mode d'...   \n",
       "29  Le lieu d'articulation désigne l'endroit où da...   \n",
       "30  Le mode d'articulation désigne la façon dont l...   \n",
       "31  pulmonique, implosif, éjectifs et clics), que ...   \n",
       "32  Le concept est principalement utilisé pour la ...   \n",
       "33  Pour n'importe quel lieu d'articulation il peu...   \n",
       "34  La parole humaine normale est pulmonique, prod...   \n",
       "35  Cependant les humains peuvent prononcer des pa...   \n",
       "36  La production de la parole est une activité co...   \n",
       "\n",
       "                                            ft_wa_trg  \n",
       "0   Dans la peinture chinoise l'abstraction peut ê...  \n",
       "1   Bien qu'aucune de ses peintures ne reste, ce s...  \n",
       "2   Le peintre bouddhiste de Chan Liang Kai (?? ve...  \n",
       "3   Un peintre de feu Song nommé Yu Jian, adepte d...  \n",
       "4   Quand Turing avait 39 ans en 1951 il se tourna...  \n",
       "5   Il s'intéressait à la morphogenèse, au dévelop...  \n",
       "6   Il suggère qu'un système de produits chimiques...  \n",
       "7   Il utilise des systèmes d'équations différenti...  \n",
       "8   Par exemple, si un catalyseur A est nécessaire...  \n",
       "9   Turing découvre que des motifs peuvent être cr...  \n",
       "10  Si A et B ont ensuite diffusé à travers le con...  \n",
       "11  Pour calculer l'étendue de cela, Turing aurait...  \n",
       "12  Ces calculs donnent les bons résultats qualita...  \n",
       "13  Le biochimiste russe Boris Belousov avait réal...  \n",
       "14  Belousov n'était pas au courant du journal de ...  \n",
       "15  Bien que publié avant que la structure et le r...  \n",
       "16  L'une des premières applications du papier de ...  \n",
       "17  D'autres recherches dans ce domaine suggèrent ...  \n",
       "18                              En 2012, Sheth et al.  \n",
       "19  Chez la souris, l'élimination des gènes Hox pr...  \n",
       "20  Plus tard les articles n'ont pas été disponibl...  \n",
       "21  Au sein du judaïsme humaniste le Talmud est ét...  \n",
       "22  Une cérémonie religieuse pratiquée au Gabon et...  \n",
       "23  Dans cet état, selon la région, les rythmes de...  \n",
       "24  Lorsque cet état de transe est vu et compris, ...  \n",
       "25  Cela crée des compétences pour séparer les sen...  \n",
       "26  Une telle séparation et la contemplation subsé...  \n",
       "27  En linguistique, la phonétique articulaire est...  \n",
       "28  Les sons de la parole sont classés par mode d'...  \n",
       "29  Le lieu d'articulation désigne l'endroit où da...  \n",
       "30  Le mode d'articulation désigne la manière dont...  \n",
       "31  pulmonique, implosif, éjectifs et clics), que ...  \n",
       "32  Le concept est principalement utilisé pour la ...  \n",
       "33  Pour n'importe quel lieu d'articulation il peu...  \n",
       "34  La parole humaine normale est pulmonique, prod...  \n",
       "35  Cependant les humains peuvent prononcer des pa...  \n",
       "36  La production de discours est une activité com...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital[\"ft_wa_trg\"] = outputs_wa\n",
    "vital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital.to_csv(\"translations_ft_wa.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Translation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('MTvenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc73298858d0756eee327543f37df2c3feab8645937b6dca4052bb1287dcab38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
