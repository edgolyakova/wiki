{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install ü§ó Transformers and ü§ó Datasets. Uncomment the following cell and run it. We also use the `sacrebleu` and `sentencepiece` libraries - you may need to install these even if you already have ü§ó Transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MOsHUjgdIrIW",
    "outputId": "f84a093e-147f-470e-aad9-80fb51193c8e"
   },
   "outputs": [],
   "source": [
    "#! pip install transformers[sentencepiece] datasets\n",
    "#! pip install sacrebleu sentencepiece\n",
    "#! pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
    "\n",
    "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then uncomment the following cell and input your token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to install Git-LFS and setup Git if you haven't already. Uncomment the following instructions and adapt with your name and email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install git-lfs\n",
    "# !git config --global user.email \"you@example.com\"\n",
    "# !git config --global user.name \"Your Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your version of Transformers is at least 4.16.0 since some of the functionality we use was introduced in that version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFASsisvIrIb"
   },
   "source": [
    "You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert CSV-file to a dataset-ready format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below works with a specifically formatted csv. Run the cell below to format your CSV accordingly.\n",
    "Your CSV should have at least 2 columns `en` and `xx` where xx is the code of the target language.\n",
    "\n",
    "If the CSV file has PoS tags for source and target language, the expected column names for them are:\n",
    "`pos_en` and `pos_xx`. \n",
    "\n",
    "If the CSV file has WA tags, the expected column name is `wa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# source_lang accepted value = 'en'\n",
    "# target_lang accepted values = 'fr'|'zh'\n",
    "# Choose pos_tags=True if the file has PoS tags for the both languages\n",
    "# Choose wa_tags=True if the file has WA tags.\n",
    "# Choose store=True if you want to create a json dump of the file that can be used later\n",
    "\n",
    "def csv_to_dataset(filename, source_lang, target_lang, pos_tags=False, wa_tags=False, store=False):\n",
    "    data = pd.read_csv(filename)\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['translation'] = [{source_lang: x, target_lang: y} for x, y in zip(data[source_lang], data[target_lang])]\n",
    "    if pos_tags:\n",
    "        new_df['pos'] = [{source_lang: x, target_lang: y} for x, y in zip(data[f'pos_{source_lang}'], data[f'pos_{target_lang}'])]\n",
    "    if wa_tags:\n",
    "        new_df['wa'] = data['wa']\n",
    "    return Dataset.from_pandas(new_df).train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "loaded_dataset = load_from_disk('fr_dataset_split.hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1508\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 378\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dataset.remove_columns(['pos', 'wa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a translation task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTCFado4IrIc"
   },
   "source": [
    "In this notebook, we will see how to fine-tune one of the [ü§ó Transformers](https://github.com/huggingface/transformers) model for a translation task. We will use the [WMT dataset](http://www.statmt.org/wmt16/), a machine translation dataset composed from a collection of various sources, including news commentaries and parliament proceedings.\n",
    "\n",
    "![Widget inference on a translation task](images/translation.png)\n",
    "\n",
    "We will see how to easily load the dataset for this task using ü§ó Datasets and how to fine-tune a model on it using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "This notebook is built to run  with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a sequence-to-sequence version in the Transformers library. Here we picked the [`Helsinki-NLP/opus-mt-en-romance`](https://huggingface.co/Helsinki-NLP/opus-mt-en-ROMANCE) checkpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will use the [ü§ó Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the `datasets` function `load_dataset` and the `evaluate` function `load`. We use the English/Romanian part of the WMT dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "metric = load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>pos</th>\n",
       "      <th>wa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'en': 'The Chickasaw began to trade with the British after the colony of Carolina was founded in 1670.', 'fr': 'Les Chicachas ont commenc√© √† commercer avec les Britanniques apr√®s la fondation de la colonie de la province de Caroline en 1670.'}</td>\n",
       "      <td>{'en': 'The DET\n",
       "Chickasaw PROPN\n",
       "began VERB\n",
       "to PART\n",
       "trade VERB\n",
       "with ADP\n",
       "the DET\n",
       "British ADJ\n",
       "after SCONJ\n",
       "the DET\n",
       "colony NOUN\n",
       "of ADP\n",
       "Carolina PROPN\n",
       "was AUX\n",
       "founded VERB\n",
       "in ADP\n",
       "1670 NUM\n",
       ". PUNCT\n",
       "', 'fr': 'Les DET\n",
       "Chicachas PROPN\n",
       "ont AUX\n",
       "commenc√© VERB\n",
       "√† ADP\n",
       "commercer VERB\n",
       "avec ADP\n",
       "les DET\n",
       "Britanniques NOUN\n",
       "apr√®s ADP\n",
       "la DET\n",
       "fondation NOUN\n",
       "de ADP\n",
       "la DET\n",
       "colonie NOUN\n",
       "de ADP\n",
       "la DET\n",
       "province NOUN\n",
       "de ADP\n",
       "Caroline NOUN\n",
       "en ADP\n",
       "1670 NUM\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-1 2-2 2-3 3-4 4-5 5-6 6-7 7-8 8-9 9-13 10-14 10-17 11-18 12-19 13-12 14-11 15-20 16-21 17-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'en': 'The genes for mitosomal components are contained in the nuclear genome.', 'fr': 'Les g√®nes codant les composants mitosomaux font partie du g√©nome nucl√©aire.'}</td>\n",
       "      <td>{'en': 'The DET\n",
       "genes NOUN\n",
       "for ADP\n",
       "mitosomal ADJ\n",
       "components NOUN\n",
       "are AUX\n",
       "contained VERB\n",
       "in ADP\n",
       "the DET\n",
       "nuclear ADJ\n",
       "genome NOUN\n",
       ". PUNCT\n",
       "', 'fr': 'Les DET\n",
       "g√®nes NOUN\n",
       "codant VERB\n",
       "les DET\n",
       "composants NOUN\n",
       "mitosomaux ADJ\n",
       "font VERB\n",
       "partie NOUN\n",
       "du ADP\n",
       "g√©nome NOUN\n",
       "nucl√©aire ADJ\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-1 2-2 3-5 4-4 5-6 7-7 8-8 9-10 10-9 11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'en': 'After hearing of the many Sendai men already appointed Takusabur≈ç joined the school in 1880.', 'fr': 'Apr√®s avoir entendu mentionn√©s de nombreux hommes de Sendai d√©j√† nomm√©s Takusabur≈ç rejoint l'√©cole en 1880.'}</td>\n",
       "      <td>{'en': 'After ADP\n",
       "hearing NOUN\n",
       "of ADP\n",
       "the DET\n",
       "many ADJ\n",
       "Sendai PROPN\n",
       "men NOUN\n",
       "already ADV\n",
       "appointed VERB\n",
       "Takusabur≈ç PROPN\n",
       "joined VERB\n",
       "the DET\n",
       "school NOUN\n",
       "in ADP\n",
       "1880 NUM\n",
       ". PUNCT\n",
       "', 'fr': 'Apr√®s ADP\n",
       "avoir AUX\n",
       "entendu VERB\n",
       "mentionn√©s VERB\n",
       "de DET\n",
       "nombreux ADJ\n",
       "hommes NOUN\n",
       "de ADP\n",
       "Sendai PROPN\n",
       "d√©j√† ADV\n",
       "nomm√©s VERB\n",
       "Takusabur≈ç PROPN\n",
       "rejoint VERB\n",
       "l' DET\n",
       "√©cole NOUN\n",
       "en ADP\n",
       "1880 NUM\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-2 2-3 3-4 4-5 5-8 6-6 7-9 8-10 9-11 10-12 11-13 12-14 13-15 14-16 15-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'en': 'Renville was named after counties in Minnesota and North Dakota.', 'fr': 'Il portait le nom de comt√©s du Minnesota et du Dakota du Nord.'}</td>\n",
       "      <td>{'en': 'Renville PROPN\n",
       "was AUX\n",
       "named VERB\n",
       "after ADP\n",
       "counties NOUN\n",
       "in ADP\n",
       "Minnesota PROPN\n",
       "and CCONJ\n",
       "North PROPN\n",
       "Dakota PROPN\n",
       ". PUNCT\n",
       "', 'fr': 'Il PRON\n",
       "portait VERB\n",
       "le DET\n",
       "nom NOUN\n",
       "de ADP\n",
       "comt√©s NOUN\n",
       "du ADP\n",
       "Minnesota PROPN\n",
       "et CCONJ\n",
       "du ADP\n",
       "Dakota NOUN\n",
       "du ADP\n",
       "Nord NOUN\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-5 5-6 6-7 7-8 8-12 9-10 10-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'en': 'The Japanese governmental shipyards were overwhelmed with the volume of construction and for the first time civilian shipyards were also assigned to produce warships.', 'fr': 'Les chantiers navals gouvernementaux japonais ont √©t√© submerg√©s par le volume de la commande et pour la premi√®re fois des chantiers navals civils ont √©t√© affect√©s pour produire des navires de guerre.'}</td>\n",
       "      <td>{'en': 'The DET\n",
       "Japanese ADJ\n",
       "governmental ADJ\n",
       "shipyards NOUN\n",
       "were AUX\n",
       "overwhelmed VERB\n",
       "with ADP\n",
       "the DET\n",
       "volume NOUN\n",
       "of ADP\n",
       "construction NOUN\n",
       "and CCONJ\n",
       "for ADP\n",
       "the DET\n",
       "first ADJ\n",
       "time NOUN\n",
       "civilian ADJ\n",
       "shipyards NOUN\n",
       "were AUX\n",
       "also ADV\n",
       "assigned VERB\n",
       "to PART\n",
       "produce VERB\n",
       "warships NOUN\n",
       ". PUNCT\n",
       "', 'fr': 'Les DET\n",
       "chantiers NOUN\n",
       "navals ADJ\n",
       "gouvernementaux ADJ\n",
       "japonais NOUN\n",
       "ont AUX\n",
       "√©t√© AUX\n",
       "submerg√©s VERB\n",
       "par ADP\n",
       "le DET\n",
       "volume NOUN\n",
       "de ADP\n",
       "la DET\n",
       "commande NOUN\n",
       "et CCONJ\n",
       "pour ADP\n",
       "la DET\n",
       "premi√®re ADJ\n",
       "fois NOUN\n",
       "des ADP\n",
       "chantiers NOUN\n",
       "navals ADJ\n",
       "civils ADJ\n",
       "ont AUX\n",
       "√©t√© AUX\n",
       "affect√©s VERB\n",
       "pour ADP\n",
       "produire VERB\n",
       "des ADP\n",
       "navires NOUN\n",
       "de ADP\n",
       "guerre NOUN\n",
       ". PUNCT\n",
       "'}</td>\n",
       "      <td>0-0 1-4 2-3 3-1 3-2 4-5 4-6 5-7 6-8 7-9 8-10 9-11 9-12 10-13 11-14 12-15 13-16 14-17 15-18 16-22 17-20 17-21 18-23 18-24 19-25 20-25 21-26 22-27 23-29 23-31 24-32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(loaded_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the mBART tokenizer (like we have here), we need to set the source and target languages (so the texts are preprocessed properly). You can check the language codes [here](https://huggingface.co/facebook/mbart-large-cc25) if you are using this notebook on a different pairs of languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mbart\" in model_checkpoint:\n",
    "    tokenizer.src_lang = \"en-XX\"\n",
    "    tokenizer.tgt_lang = \"fr-FR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[10537, 2, 67, 32, 15, 5776, 145, 0], [160, 32, 1036, 5776, 3, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, for Word Alignment encoding we will be using these token values instead of real words to express relatedness of words in 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "en_pos_sp = spacy.load(\"en_core_web_sm\")\n",
    "fr_pos_sp = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "If you are using one of the five T5 checkpoints that require a special prefix to put before the inputs, you should adapt the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"translate English to French: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PoS tags we will use a separate function that will parse the sentences and extract the PoS information from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_pos_tags receives a tokenized input from the model. The tokenization is a bit different from spacy model,\n",
    "so to keep the same dimensions of vectors as in the sentence embeddings for each token in a sentence we will:\n",
    "- decode the token received from the model\n",
    "- get a Part of Speech id for it from Spacy and return it\n",
    "\"\"\"\n",
    "\n",
    "def token_to_pos(token, lang):\n",
    "    if lang == 'en':\n",
    "        decoded = list(en_pos_sp(tokenizer.decode(token)))\n",
    "    elif lang == 'fr':\n",
    "        decoded = list(fr_pos_sp(tokenizer.decode(token)))\n",
    "    return decoded[-1].pos if decoded else -1\n",
    "\n",
    "def get_pos_tags(tokenized_sent, lang):\n",
    "    return list(map(lambda x: token_to_pos(x, lang), tokenized_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Alignment information can be encoded in different ways:\n",
    "- Name: `trg-ids`. Create a vector of the same length as the tokenized input sentence. For each position i in the new vector, find a corresponding word in the original input sentence. Find a connected word from the target sentence and put its tokenized value in the new vector.\n",
    "- Name: `sums`. Create a copy of the input vector. For i-th word that has a connected word in the target sentence, add its value to the tokenized value to the i-th position of the new vector.\n",
    "- Name: `mult`. Same as sums but replaces sums with multiplication of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def encode_wa(tokenized_input, tokenized_target, wa, wa_type):\n",
    "    wa_dict = {int(src): int(trg) for src, trg in map(lambda x: x.split('-'), wa.split())}\n",
    "    n = len(tokenized_input)\n",
    "    m = len(tokenized_target)\n",
    "    if wa_type == 'trg-ids':\n",
    "        wa_emb = [0]*n\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] = tokenized_target[v]\n",
    "        return wa_emb\n",
    "    elif wa_type == 'sums':\n",
    "        wa_emb = deepcopy(tokenized_input)\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] += tokenized_target[v]\n",
    "        return wa_emb\n",
    "    \n",
    "    elif wa_type == 'mult':\n",
    "        wa_emb = deepcopy(tokenized_input)\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] *= tokenized_target[v]\n",
    "        return wa_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags=False\n",
    "wa_type=None\n",
    "\n",
    "def preprocess_function(dataset):\n",
    "    global source_lang, target_lang, pos_tags, wa_type\n",
    "    inputs = [prefix + d[source_lang] for d in dataset[\"translation\"]]\n",
    "    targets = [d[target_lang] for d in dataset[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    if pos_tags:\n",
    "        model_inputs['pos'] = [get_pos_tags(x, 'en') for x in model_inputs['input_ids']]\n",
    "        model_inputs['target_pos'] = [get_pos_tags(y, 'fr') for y in model_inputs['labels']]\n",
    "        \n",
    "    if wa_type:\n",
    "        model_inputs['wa'] = [encode_wa(src, trg, wa, wa_type) for src, trg, wa \\\n",
    "                              in zip(model_inputs['input_ids'],  model_inputs['labels'], dataset[\"wa\"])]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "To apply this function on all the pairs of sentences in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def metric_fn(eval_predictions):\n",
    "    preds, labels = eval_predictions\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # We use -100 to mask labels - replace it with the tokenizer pad token when decoding\n",
    "    # so that no output is emitted for these\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import AdamWeightDecay\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model with no extra tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at fr_dataset_split.hf/train\\cache-4ac5e2a245c8bb7d.arrow\n",
      "Loading cached processed dataset at fr_dataset_split.hf/test\\cache-96397f53f35e1795.arrow\n"
     ]
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags=False\n",
    "wa_tags = False\n",
    "wa_type = None\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['pos', 'wa'])\n",
    "\n",
    "\n",
    "no_anno_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "Note that  we don't get a warning like in our classification example. This means we used all the weights of the pretrained model and there is no randomly initialized head in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "Next we set some parameters like the learning rate and the `batch_size`and customize the weight decay. \n",
    "\n",
    "The last two arguments are to setup everything so we can push the model to the [Hub](https://huggingface.co/models) at the end of training. Remove the two of them if you didn't follow the installation steps at the top of the notebook, otherwise you can change the value of push_to_hub_model_id to something you would prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1\n",
    "\n",
    "#model_name = model_checkpoint.split(\"/\")[-1]\n",
    "#push_to_hub_model_id = f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels. Note that our data collators are multi-framework, so make sure you set `return_tensors='tf'` so you get `tf.Tensor` objects back and not something else!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert our datasets to `tf.data.Dataset`, which Keras understands natively. There are two ways to do this - we can use the slightly more low-level [`Dataset.to_tf_dataset()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset) method, or we can use [`Model.prepare_tf_dataset()`](https://huggingface.co/docs/transformers/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset). The main difference between these two is that the `Model` method can inspect the model to determine which column names it can use as input, which means you don't need to specify them yourself. Make sure to specify the collator we just created as our `collate_fn`!\n",
    "\n",
    "We also want to compute `BLEU` metrics, which will require us to generate text from our model. To speed things up, we can compile our generation loop with XLA. This results in a *huge* speedup - up to 100X! The downside of XLA generation, though, is that it doesn't like variable input shapes, because it needs to run a new compilation for each new input shape! To compensate for that, let's use `pad_to_multiple_of` for the dataset we use for text generation. This will reduce the number of unique input shapes a lot, meaning we can get the benefits of XLA generation with only a few compilations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model.prepare_tf_dataset(\n",
    "    no_anno_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "validation_dataset = model.prepare_tf_dataset(\n",
    "    no_anno_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "generation_dataset = model.prepare_tf_dataset(\n",
    "    no_anno_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize our loss and optimizer and compile the model. Note that most Transformers models compute loss internally, so we can just leave the loss argument blank to use the internal loss instead. For the optimizer, we can use the `AdamWeightDecay` optimizer in the Transformer library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our model. We can also add a few optional callbacks here, which you can remove if they aren't useful to you. In no particular order, these are:\n",
    "- PushToHubCallback will sync up our model with the Hub - this allows us to resume training from other machines, share the model after training is finished, and even test the model's inference quality midway through training!\n",
    "- TensorBoard is a built-in Keras callback that logs TensorBoard metrics.\n",
    "- KerasMetricCallback is a callback for computing advanced metrics. There are a number of common metrics in NLP like ROUGE which are hard to fit into your compiled training loop because they depend on decoding predictions and labels back to strings with the tokenizer, and calling arbitrary Python functions to compute the metric. The KerasMetricCallback will wrap a metric function, outputting metrics as training progresses.\n",
    "\n",
    "If this is the first time you've seen `KerasMetricCallback`, it's worth explaining what exactly is going on here. The callback takes two main arguments - a `metric_fn` and an `eval_dataset`. It then iterates over the `eval_dataset` and collects the model's outputs for each sample, before passing the `list` of predictions and the associated `list` of labels to the user-defined `metric_fn`. If the `predict_with_generate` argument is `True`, then it will call `model.generate()` for each input sample instead of `model.predict()` - this is useful for metrics that expect generated text from the model, like `ROUGE` and `BLEU`.\n",
    "\n",
    "This callback allows complex metrics to be computed each epoch that would not function as a standard Keras Metric. Metric values are printed each epoch, and can be used by other callbacks like `TensorBoard` or `EarlyStopping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the metric callback ready, now we can specify the other callbacks and fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1790s 19s/step - loss: 1.4436 - val_loss: 1.3762 - bleu: 23.9309 - gen_len: 45.0317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fbbee1a0a0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./translation_model_save/logs\")\n",
    "\n",
    "\"\"\"push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"./translation_model_save\",\n",
    "    tokenizer=tokenizer,\n",
    "    hub_model_id=push_to_hub_model_id,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#callbacks = [metric_callback, tensorboard_callback, push_to_hub_callback]\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with no extra features is **18.7710**. This is our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model with PoS features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell can run pretty slow and can take 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f034f9fdd04a20be71f03fdafd9065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\MTproject\\MTvenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8d9c4082ff423f845abfef0335841b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "# Pos_tags need to be set to True in the cell\n",
    "pos_tags = True\n",
    "wa_tags = False\n",
    "wa_type = None\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['wa'])\n",
    "\n",
    "pos_anno_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_pos = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_pos = DataCollatorForSeq2Seq(tokenizer, model=model_with_pos, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_pos = DataCollatorForSeq2Seq(tokenizer, model=model_with_pos, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator_pos,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator_pos,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator_pos,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_pos.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - ETA: 0s - loss: 1.4439WARNING:tensorflow:5 out of the last 5 calls to <function KerasMetricCallback.on_epoch_end.<locals>.generation_function at 0x000001F3C2278B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function KerasMetricCallback.on_epoch_end.<locals>.generation_function at 0x000001F3C2278B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe Kernel s‚Äôest bloqu√© lors de l‚Äôex√©cution du code dans la cellule active ou une cellule pr√©c√©dente. Veuillez v√©rifier le code dans la ou les cellules pour identifier une cause possible de l‚Äô√©chec. Cliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d‚Äôinformations. Pour plus d‚Äôinformations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./translation_model_save/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_pos.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with POS features only is **22.47899**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model with WA (vector with target ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at fr_dataset_split.hf/train\\cache-da7d73a19c07a55b.arrow\n",
      "Loading cached processed dataset at fr_dataset_split.hf/test\\cache-29b0fe0d33970b62.arrow\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags = False\n",
    "wa_tags = True\n",
    "wa_type=\"trg-ids\"\n",
    "\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['pos'])\n",
    "wa_trg_id_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_wa_trg_ids = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_wa_trg_ids = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_trg_ids, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_wa_trg_ids = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_trg_ids, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_wa_trg_ids.prepare_tf_dataset(\n",
    "    wa_trg_id_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=generation_data_collator_wa_trg_ids,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_wa_trg_ids.prepare_tf_dataset(\n",
    "    wa_trg_id_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_trg_ids,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_wa_trg_ids.prepare_tf_dataset(\n",
    "    wa_trg_id_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_trg_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_wa_trg_ids.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 2145s 23s/step - loss: 1.4369 - val_loss: 1.3800 - bleu: 23.2846 - gen_len: 45.7751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d26ad1deb0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./wa_trg_ids/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_wa_trg_ids.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with WA alignment using target indeces is **31.5155**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model with WA (using sum of token ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abc3932bbab43a79e9214b0b8ff42ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f873028e854b4e97b7f006db7c3275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags = False\n",
    "wa_tags = True\n",
    "wa_type =\"sums\"\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['pos'])\n",
    "wa_sums_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_wa_sums = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_wa_sums = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_sums, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_wa_sums = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_sums, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_wa_sums.prepare_tf_dataset(\n",
    "    wa_sums_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=generation_data_collator_wa_sums,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_wa_sums.prepare_tf_dataset(\n",
    "    wa_sums_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_sums,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_wa_sums.prepare_tf_dataset(\n",
    "    wa_sums_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_sums,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_wa_sums.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1900s 20s/step - loss: 1.5205 - val_loss: 1.4024 - bleu: 20.2693 - gen_len: 50.7725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d8493f070>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./wa_trg_ids/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_wa_sums.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with WA alignment using sums of related words' indeces is **23.2457**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running model with WA (using multiplication of token ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145283f150f44b7d9e1bcc1df3502e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1f831edb3e40f7b16c5f5a1f0c04b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "pos_tags = False\n",
    "wa_tags = True\n",
    "wa_type=\"mult\"\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['pos'])\n",
    "wa_mult_dataset = split_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_with_wa_mult = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_wa_mult = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_mult, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator_wa_mult = DataCollatorForSeq2Seq(tokenizer, model=model_with_wa_mult, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model_with_wa_mult.prepare_tf_dataset(\n",
    "    wa_mult_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=generation_data_collator_wa_mult,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_wa_mult.prepare_tf_dataset(\n",
    "    wa_mult_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_mult,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_wa_mult.prepare_tf_dataset(\n",
    "    wa_mult_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator_wa_mult,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_wa_mult.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 2004s 21s/step - loss: 1.5170 - val_loss: 1.4007 - bleu: 18.0217 - gen_len: 56.3280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d89518be0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./wa_mult/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_wa_mult.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU** metric after the run with WA alignment using sums of related words' indeces is **26.4307**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotation</th>\n",
       "      <th>BLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No annotation</td>\n",
       "      <td>18.7710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Part of Speech</td>\n",
       "      <td>21.8055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Word alignment: target token ids</td>\n",
       "      <td>22.7393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word alignment: sum of token ids</td>\n",
       "      <td>20.2693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Word alignment: multiplication of token ids</td>\n",
       "      <td>18.0217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Annotation     BLEU\n",
       "0                                No annotation  18.7710\n",
       "1                               Part of Speech  21.8055\n",
       "2             Word alignment: target token ids  22.7393\n",
       "3             Word alignment: sum of token ids  20.2693\n",
       "4  Word alignment: multiplication of token ids  18.0217"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Annotation','BLEU'])\n",
    "results_list = [\n",
    "    ('No annotation', 18.7710),\n",
    "    ('Part of Speech', 21.8055),\n",
    "    ('Word alignment: target token ids', 22.7393),\n",
    "    ('Word alignment: sum of token ids', 20.2693),\n",
    "    ('Word alignment: multiplication of token ids', 18.0217)\n",
    "]\n",
    "results.append([{'Annotation': x[0], 'BLEU': x[1]} for x in results_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation with the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've trained our model, let's see how we could load it and use it to translate text in future! First, let's load it from the hub. This means we can resume the code from here without needing to rerun everything above every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try tokenizing some text and passing it to the model to generate a translation. Don't forget to add the \"translate: \" string at the start if you're using a `T5` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[59513   277     8 12720 18389     2    14     6  2914  7750  1078   168\n",
      "    100 21481    51    17     8 43774 21520 23696 30510  2734     2    44\n",
      "     43  2692   274    20     6  1936 30324    19  2955     5 12720    17\n",
      "  13967    20     6 12319 19727  2130     9  6092     3     0 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513 59513\n",
      "  59513 59513 59513 59513 59513 59513 59513 59513]], shape=(1, 128), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "input_text  = \"In Chinese painting, abstraction can be traced to the Tang dynasty painter Wang Mo (ÁéãÂ¢®), who is credited to have invented the splashed-ink painting style.\"\n",
    "\n",
    "tokenized = tokenizer([input_text], return_tensors='np')\n",
    "# In the line below use the variable name of the model you want to test\n",
    "out = model.generate(**tokenized, max_length=128)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's some tokens and a lot of padding! Let's decode those to see what it says, using the `skip_special_tokens` argument to skip those padding tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans la peinture chinoise, l'abstraction peut √™tre trac√©e √† la dynastie Tang peintre Wang Mo, qui est cr√©dit√© d'avoir invent√© le style de peinture √† jet d'√©claboussures.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\MTproject\\MTvenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application to the Vital articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article                                          sentences\n",
       "0  Abstract art  In Chinese painting, abstraction can be traced...\n",
       "1  Abstract art  While none of his paintings remain, this style...\n",
       "2  Abstract art  The Chan buddhist painter Liang Kai (??, c. 11...\n",
       "3  Abstract art  A late Song painter named Yu Jian, adept to Ti...\n",
       "4   Alan Turing  When Turing was 39 years old in 1951, he turne..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital = pd.read_csv(\"C:\\\\Users\\\\Utilisateur\\\\Documents\\\\wiki\\\\scrapping\\\\ver2\\\\articles_clean_ver2\\\\en_only_fr.csv\", sep=\";\", encoding=\"iso-8859-1\")\n",
    "vital.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model without any fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\MTproject\\MTvenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans la peinture chinoise, l'abstraction peut √™tre trac√©e √† la dynastie Tang peintre Wang Mo (??), qui est cr√©dit√© d'avoir invent√© le style de peinture √©clabouss√©e.\n",
      "Bien qu'aucune de ses peintures ne reste, ce style est clairement vu dans certains Song Dynasty Paintings.\n",
      "Le peintre bouddhiste Chan Liang Kai (??, vers 1140=1210) a appliqu√© le style √† la peinture figurative dans son \"Immortal in splashed enk\" dans lequel une repr√©sentation pr√©cise est sacrifi√©e pour am√©liorer la spontan√©it√© li√©e √† l'esprit non-rationnel de l'√©clair√©.\n",
      "Un peintre de feu Song nomm√© Yu Jian, adepte du bouddhisme de Tiantai, a cr√©√© une s√©rie de paysages d'encre √©clabouss√©es qui a finalement inspir√© de nombreux peintres japonais Zen.\n",
      "Quand Turing avait 39 ans en 1951, il se tourna vers la biologie math√©matique, publiant finalement son chef-d'≈ìuvre \"The Chemical Bases of Morphogen√®se\" en janvier 1952.\n",
      "Il s'int√©ressait √† la morphogen√®se, au d√©veloppement de mod√®les et de formes dans les organismes biologiques.\n",
      "Il a sugg√©r√© qu'un syst√®me de produits chimiques qui r√©agissent les uns avec les autres et qui se diffusent dans l'espace, appel√© syst√®me de diffusion des r√©actions, pourrait expliquer ¬´ les principaux ph√©nom√®nes de morphogen√®se ¬ª.\n",
      "Il a utilis√© des syst√®mes d'√©quations diff√©rentielles partielles pour mod√©liser les r√©actions chimiques catalytiques.\n",
      "Par exemple, si un catalyseur A est n√©cessaire pour qu'une certaine r√©action chimique ait lieu, et si la r√©action produite plus du catalyseur A, alors nous disons que la r√©action est autocatalytique, et il y a une r√©troaction positive qui peut √™tre mod√©lis√©e par des √©quations diff√©rentielles non lin√©aires.\n",
      "Turing a d√©couvert que des patrons pouvaient √™tre cr√©√©s si la r√©action chimique non seulement produisait le catalyseur A, mais √©galement un inhibiteur B qui ralentissait la production de A.\n",
      "Si A et B ont ensuite diffus√© √† travers le conteneur √† des vitesses diff√©rentes, alors vous pourriez avoir certaines r√©gions o√π A a domin√© et certaines o√π B l'a fait.\n",
      "Pour en calculer l'ampleur, Turing aurait eu besoin d'un ordinateur puissant, mais ceux-ci n'√©taient pas si librement disponibles en 1951, donc il a d√ª utiliser des approximations lin√©aires pour r√©soudre les √©quations √† la main.\n",
      "Ces calculs ont donn√© les bons r√©sultats qualitatifs et ont produit, par exemple, un m√©lange uniforme qui, bizarrement, avait r√©guli√®rement espac√© des taches rouges fixes.\n",
      "Le biochimiste russe Boris Belousov avait r√©alis√© des exp√©riences avec des r√©sultats similaires, mais n'a pas pu faire publier ses articles √† cause du pr√©jug√© contemporain selon lequel une telle chose violait la deuxi√®me loi de la thermodynamique.\n",
      "Belousov n'√©tait pas au courant du papier de Turing dans les Transactions Philosophiques de la Soci√©t√© Royale.\n",
      "Bien que publi√© avant que la structure et le r√¥le de l'ADN ait √©t√© compris, le travail de Turing sur la morphogen√®se reste pertinent aujourd'hui et est consid√©r√© comme un travail s√©minal en biologie math√©matique.\n",
      "L'une des premi√®res applications du papier de Turing a √©t√© le travail de James Murray expliquant les taches et les rayures sur la fourrure des chats, grands et petits.\n",
      "D'autres recherches dans le domaine sugg√®rent que le travail de Turing peut expliquer en partie la croissance des ¬´ plumes, des follicules pileux, le mod√®le de ramification des poumons, et m√™me l'asym√©trie gauche-droite qui place le c≈ìur sur le c√¥t√© gauche de la poitrine ¬ª.\n",
      "En 2012, Sheth et al.\n",
      "Chez la souris, l'√©limination des g√®nes Hox entra√Æne une augmentation du nombre de chiffres sans augmentation de la taille globale du membre, ce qui sugg√®re que les g√®nes Hox contr√¥lent la formation de chiffres en harmonisant la longueur d'onde d'un m√©canisme de type Turing.\n",
      "Par la suite, les documents n'ont pas √©t√© disponibles avant la publication de Collected Works of A. M. Turing en 1992.\n",
      "Dans le juda√Øsme humaniste, Talmud est √©tudi√© comme un texte historique, afin de d√©couvrir comment il peut d√©montrer sa pertinence pratique pour vivre aujourd'hui.\n",
      "Une c√©r√©monie religieuse pratiqu√©e au Gabon et au Cameroun est celle des Okuyi, pratiqu√©e par plusieurs groupes ethniques bantous.\n",
      "Dans cet √©tat, selon la r√©gion, des rythmes de tambours ou instrumentaux jou√©s par des musiciens respect√©s (chacun √©tant unique √† une divinit√© ou √† un anc√™tre donn√©), les participants incarnent une divinit√© ou un anc√™tre, une √©nergie ou un √©tat d'esprit en effectuant des mouvements rituels ou des danses distincts qui renforcent encore leur conscience √©lev√©e.\n",
      "Lorsque cet √©tat semblable √† la transe est vu et compris, les adh√©rents sont mis au courant d'une fa√ßon de contempler l'incarnation pure ou symbolique d'un √©tat d'esprit ou d'un cadre de r√©f√©rence particulier.\n",
      "Cela renforce les comp√©tences pour s√©parer les sentiments suscit√©s par cet √©tat d'esprit de leurs manifestations situationnelles dans la vie quotidienne.\n",
      "Une telle s√©paration et la contemplation subs√©quente de la nature et des sources d'√©nergie ou de sentiments purs aident les participants √† les g√©rer et √† les accepter lorsqu'ils surviennent dans des contextes banals.\n",
      "En linguistique, la phon√©tique articulaire est l'√©tude de la fa√ßon dont la langue, les l√®vres, la m√¢choire, les cordes vocales et d'autres organes de la parole sont utilis√©s pour faire des sons.\n",
      "Les sons de la parole sont class√©s par mode d'articulation et lieu d'articulation.\n",
      "Le lieu d'articulation d√©signe l'endroit o√π, dans le cou ou la bouche, le courant d'air est restreint.\n",
      "Le mode d'articulation d√©signe la fa√ßon dont les organes de la parole interagissent, comme la fa√ßon dont l'air est restreint, quelle forme de courant d'air est utilis√©e (p. ex.\n",
      "pulmonique, implosif, √©jectifs et clics), que les cordes vocales vibrent ou non, et que la cavit√© nasale soit ouverte au courant d'air.\n",
      "Le concept est principalement utilis√© pour la production de consonnes, mais peut √™tre utilis√© pour les voyelles dans des qualit√©s telles que la voix et la nasalisation.\n",
      "Pour n'importe quel lieu d'articulation, il peut y avoir plusieurs mani√®res d'articulation, et donc plusieurs consonnes homorganiques.\n",
      "La parole humaine normale est pulmonique, produite avec la pression des poumons, qui cr√©e la phonation dans le glattis dans le larynx, qui est ensuite modifi√© par le tractus vocal et la bouche en diff√©rentes voyelles et consonnes.\n",
      "Cependant, les humains peuvent prononcer des mots sans utiliser les poumons et les glattis dans le discours alaryng√©al, dont il existe trois types : le discours oesophagien, le discours pharyng√©al et le discours buccal (plus connu sous le nom de Donald Duck).\n",
      "La production de discours est une activit√© complexe et, par cons√©quent, les erreurs sont fr√©quentes, surtout chez les enfants.\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for input_sentence in vital[\"sentences\"]:\n",
    "    tokenized_sentence = tokenizer([input_sentence], return_tensors='np')\n",
    "    out = model.generate(**tokenized_sentence, max_length=128)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        output_sentence = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        print(output_sentence)\n",
    "        outputs.append(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital[\"initial_model\"] = outputs\n",
    "vital.to_csv(\"translations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "      <th>initial_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "      <td>Dans la peinture chinoise, l'abstraction peut ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "      <td>Un peintre de feu Song nomm√© Yu Jian, adepte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951, il se tourn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article                                          sentences  \\\n",
       "0  Abstract art  In Chinese painting, abstraction can be traced...   \n",
       "1  Abstract art  While none of his paintings remain, this style...   \n",
       "2  Abstract art  The Chan buddhist painter Liang Kai (??, c. 11...   \n",
       "3  Abstract art  A late Song painter named Yu Jian, adept to Ti...   \n",
       "4   Alan Turing  When Turing was 39 years old in 1951, he turne...   \n",
       "\n",
       "                                       initial_model  \n",
       "0  Dans la peinture chinoise, l'abstraction peut ...  \n",
       "1  Bien qu'aucune de ses peintures ne reste, ce s...  \n",
       "2  Le peintre bouddhiste Chan Liang Kai (??, vers...  \n",
       "3  Un peintre de feu Song nomm√© Yu Jian, adepte d...  \n",
       "4  Quand Turing avait 39 ans en 1951, il se tourn...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuned model: no annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "      <th>initial_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "      <td>Dans la peinture chinoise, l'abstraction peut ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "      <td>Un peintre de feu Song nomm√© Yu Jian, adepte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951, il se tourn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article                                          sentences  \\\n",
       "0  Abstract art  In Chinese painting, abstraction can be traced...   \n",
       "1  Abstract art  While none of his paintings remain, this style...   \n",
       "2  Abstract art  The Chan buddhist painter Liang Kai (??, c. 11...   \n",
       "3  Abstract art  A late Song painter named Yu Jian, adept to Ti...   \n",
       "4   Alan Turing  When Turing was 39 years old in 1951, he turne...   \n",
       "\n",
       "                                       initial_model  \n",
       "0  Dans la peinture chinoise, l'abstraction peut ...  \n",
       "1  Bien qu'aucune de ses peintures ne reste, ce s...  \n",
       "2  Le peintre bouddhiste Chan Liang Kai (??, vers...  \n",
       "3  Un peintre de feu Song nomm√© Yu Jian, adepte d...  \n",
       "4  Quand Turing avait 39 ans en 1951, il se tourn...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital = pd.read_csv(\"translations.csv\")\n",
    "vital.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\MTproject\\MTvenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans la peinture chinoise l'abstraction peut √™tre trac√©e par le peintre de la dynastie Tang Wang Mo (??) qui est cr√©dit√© d'avoir invent√© le style de peinture √† jets d'√©claboussures.\n",
      "Bien qu'aucune de ses peintures ne subsiste, ce style est clairement vu dans certains Song Dynasty Paintings.\n",
      "Le peintre bouddhiste Chan Liang Kai (??, vers 1140=1210) a appliqu√© le style √† la peinture figurative dans son ¬´ Immortal in splashed enk ¬ª dans lequel une repr√©sentation pr√©cise est sacrifi√©e pour renforcer la spontan√©it√© li√©e √† l'esprit non rationnel de l'√©clair√©.\n",
      "Un peintre de feu Song nomm√© Yu Jian, adepte du Bouddhisme de Tiantai, cr√©e une s√©rie de paysages √† l'encre √©clabouss√©es qui inspirent finalement de nombreux peintres japonais Zens............................................................................\n",
      "Quand Turing avait 39 ans en 1951 il se tourna vers la biologie math√©matique et publia finalement son chef-d'≈ìuvre ¬´ The Chemical Bases of Morphogen√®se ¬ª en janvier 1952.\n",
      "Il s'int√©ressait √† la morphogen√®se, au d√©veloppement de motifs et de formes dans les organismes biologiques......................................................................................................\n",
      "Il sugg√®re qu'un syst√®me de produits chimiques r√©agissant l'un avec l'autre et diffusant √† travers l'espace appel√© syst√®me de diffusion de r√©action pourrait expliquer ¬´ les principaux ph√©nom√®nes de morphogen√®se ¬ª.\n",
      "Il utilise des syst√®mes d'√©quations diff√©rentielles partielles pour mod√©liser les r√©actions chimiques catalytiques.\n",
      "Par exemple, si un catalyseur A est n√©cessaire pour qu'une certaine r√©action chimique ait lieu et si la r√©action produite plus du catalyseur A est alors nous disons que la r√©action est autocatalytique et qu'il y a un retour positif qui peut √™tre mod√©lis√© par des √©quations diff√©rentielles non lin√©aires.\n",
      "Turing d√©couvre que des motifs peuvent √™tre cr√©√©s si la r√©action chimique produit non seulement le catalyseur A mais aussi un inhibiteur B qui ralentit la production de A.\n",
      "Si A et B se diffusent alors √† travers le conteneur √† des vitesses diff√©rentes, alors vous pouvez avoir certaines r√©gions o√π A domine et d'autres o√π B fait.\n",
      "Pour calculer l'√©tendue de cela, Turing aurait eu besoin d'un ordinateur puissant, mais ceux-ci n'√©taient pas si librement disponibles en 1951, il a donc d√ª utiliser des approximations lin√©aires pour r√©soudre les √©quations √† la main.\n",
      "Ces calculs ont donn√© les bons r√©sultats qualitatifs et ont produit par exemple un m√©lange uniforme qui a √©t√© assez √©trangement espac√© r√©guli√®rement des taches rouges fixes.\n",
      "Le biochimiste russe Boris Belousov avait r√©alis√© des exp√©riences avec des r√©sultats similaires mais ne pouvait pas faire publier ses articles √† cause du pr√©jug√© contemporain selon lequel une telle chose violait la deuxi√®me loi de la thermodynamique.\n",
      "Belousov n'√©tait pas au courant du journal de Turing dans les Transactions Philosophiques de la Royal Society.\n",
      "Bien que publi√© avant que la structure et le r√¥le de l'ADN n'aient √©t√© compris, les travaux de Turing sur la morphogen√®se restent pertinents aujourd'hui et sont consid√©r√©s comme un travail s√©minal en biologie math√©matique..............................................................................\n",
      "L'une des premi√®res applications du papier de Turing est le travail de James Murray expliquant les taches et les rayures sur la fourrure des chats, grands et petits.\n",
      "D'autres recherches dans ce domaine sugg√®rent que les travaux de Turing peuvent expliquer en partie la croissance des ¬´ plumes, des follicules pileux, le mod√®le de ramification des poumons et m√™me l'asym√©trie gauche-droite qui place le c≈ìur sur le c√¥t√© gauche de la poitrine ¬ª.\n",
      "En 2012, Sheth et al.\n",
      "Chez la souris, l'√©limination des g√®nes Hox provoque une augmentation du nombre de chiffres sans augmentation de la taille globale du membre, ce qui sugg√®re que les g√®nes Hox contr√¥lent la formation de chiffres en harmonisant la longueur d'onde d'un m√©canisme de type Turing.\n",
      "Des articles plus tard n'ont pas √©t√© disponibles avant que Collected Works of A. M. Turing n'ait √©t√© publi√© en 1992.\n",
      "Au sein du juda√Øsme humaniste, Talmud est √©tudi√© comme un texte historique afin de d√©couvrir comment il peut d√©montrer sa pertinence pratique pour la vie d'aujourd'hui............................................................................................\n",
      "Une c√©r√©monie religieuse pratiqu√©e au Gabon et au Cameroun est l'Okuyi, pratiqu√©e par plusieurs groupes ethniques bantous.\n",
      "Dans cet √©tat, selon la r√©gion des rythmes de tambours ou instrumentaux jou√©s par des musiciens respect√©s (chacun √©tant unique √† une divinit√© ou un anc√™tre donn√©), les participants incarnent une divinit√© ou un anc√™tre, une √©nergie ou un √©tat d'esprit en effectuant des mouvements rituels distincts ou des danses qui renforcent encore leur conscience √©lev√©e.\n",
      "Quand cet √©tat de transe est vu et compris, les adh√©rents sont en mesure de contempler l'incarnation pure ou symbolique d'un √©tat d'esprit particulier ou d'un cadre de r√©f√©rence......................................................................................\n",
      "Cela cr√©e des comp√©tences pour s√©parer les sentiments suscit√©s par cet √©tat d'esprit de leurs manifestations situationnelles dans la vie de tous les jours..................................................................................................\n",
      "Une telle s√©paration et la contemplation subs√©quente de la nature et des sources d'√©nergie ou de sentiments purs aident les participants √† les g√©rer et √† les accepter lorsqu'ils apparaissent dans des contextes mondains.\n",
      "En linguistique, la phon√©tique articulaire est l'√©tude de la fa√ßon dont la langue, les l√®vres, la m√¢choire, les cordes vocales et d'autres organes de parole sont utilis√©s pour faire des sons.\n",
      "Les sons de la parole sont class√©s par mode d'articulation et lieu d'articulation.\n",
      "Le lieu d'articulation d√©signe l'endroit o√π dans le cou ou la bouche le courant d'air est restreint.\n",
      "Le mode d'articulation d√©signe la fa√ßon dont les organes de parole interagissent, comme la fa√ßon dont l'air est restreint et quelle forme de courant d'air est utilis√©e (p. ex.\n",
      "pulmonique, implosif, √©jectifs et clics), que les cordes vocales vibrent ou non et que la cavit√© nasale soit ouverte au courant d'air.\n",
      "Le concept est principalement utilis√© pour la production de consonnes mais peut √™tre utilis√© pour les voyelles dans des qualit√©s telles que la vocation et la nasalisation.\n",
      "Pour n'importe quel lieu d'articulation il peut y avoir plusieurs mani√®res d'articulation et donc plusieurs consonnes homorganiques.\n",
      "La parole humaine normale est pulmonique, produite avec la pression des poumons qui cr√©e la phonation dans les glattis dans le larynx qui est ensuite modifi√©e par le chant et la bouche en diff√©rentes voyelles et consonnes.\n",
      "Cependant les humains peuvent prononcer des paroles sans utiliser les poumons et les glattis dans le discours alaryng√©al, dont il existe trois types : le discours √©sophageal, le discours pharyng√©al et le discours buccal (plus connu sous le nom de Donald Duck).\n",
      "La production de la parole est une activit√© complexe et par cons√©quent les erreurs sont fr√©quentes, en particulier chez les enfants.\n"
     ]
    }
   ],
   "source": [
    "outputs_no_anno = []\n",
    "\n",
    "for input_sentence in vital[\"sentences\"]:\n",
    "    tokenized_sentence = tokenizer([input_sentence], return_tensors='np')\n",
    "    out = model.generate(**tokenized_sentence, max_length=128)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        output_sentence = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        print(output_sentence)\n",
    "        outputs_no_anno.append(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "      <th>initial_model</th>\n",
       "      <th>ft_no_anno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "      <td>Dans la peinture chinoise, l'abstraction peut ...</td>\n",
       "      <td>Dans la peinture chinoise l'abstraction peut √™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne subsiste, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "      <td>Un peintre de feu Song nomm√© Yu Jian, adepte d...</td>\n",
       "      <td>Un peintre de feu Song nomm√© Yu Jian, adepte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951, il se tourn...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951 il se tourna...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article                                          sentences  \\\n",
       "0  Abstract art  In Chinese painting, abstraction can be traced...   \n",
       "1  Abstract art  While none of his paintings remain, this style...   \n",
       "2  Abstract art  The Chan buddhist painter Liang Kai (??, c. 11...   \n",
       "3  Abstract art  A late Song painter named Yu Jian, adept to Ti...   \n",
       "4   Alan Turing  When Turing was 39 years old in 1951, he turne...   \n",
       "\n",
       "                                       initial_model  \\\n",
       "0  Dans la peinture chinoise, l'abstraction peut ...   \n",
       "1  Bien qu'aucune de ses peintures ne reste, ce s...   \n",
       "2  Le peintre bouddhiste Chan Liang Kai (??, vers...   \n",
       "3  Un peintre de feu Song nomm√© Yu Jian, adepte d...   \n",
       "4  Quand Turing avait 39 ans en 1951, il se tourn...   \n",
       "\n",
       "                                          ft_no_anno  \n",
       "0  Dans la peinture chinoise l'abstraction peut √™...  \n",
       "1  Bien qu'aucune de ses peintures ne subsiste, c...  \n",
       "2  Le peintre bouddhiste Chan Liang Kai (??, vers...  \n",
       "3  Un peintre de feu Song nomm√© Yu Jian, adepte d...  \n",
       "4  Quand Turing avait 39 ans en 1951 il se tourna...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital[\"ft_no_anno\"] = outputs_no_anno\n",
    "vital.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "      <th>initial_model</th>\n",
       "      <th>ft_no_anno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "      <td>Dans la peinture chinoise, l'abstraction peut ...</td>\n",
       "      <td>Dans la peinture chinoise l'abstraction peut √™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne subsiste, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "      <td>Un peintre de feu Song nomm√© Yu Jian, adepte d...</td>\n",
       "      <td>Un peintre de feu Song nomm√© Yu Jian, adepte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951, il se tourn...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951 il se tourna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>He was interested in morphogenesis, the develo...</td>\n",
       "      <td>Il s'int√©ressait √† la morphogen√®se, au d√©velop...</td>\n",
       "      <td>Il s'int√©ressait √† la morphogen√®se, au d√©velop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>He suggested that a system of chemicals reacti...</td>\n",
       "      <td>Il a sugg√©r√© qu'un syst√®me de produits chimiqu...</td>\n",
       "      <td>Il sugg√®re qu'un syst√®me de produits chimiques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>He used systems of partial differential equati...</td>\n",
       "      <td>Il a utilis√© des syst√®mes d'√©quations diff√©ren...</td>\n",
       "      <td>Il utilise des syst√®mes d'√©quations diff√©renti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>For example, if a catalyst A is required for a...</td>\n",
       "      <td>Par exemple, si un catalyseur A est n√©cessaire...</td>\n",
       "      <td>Par exemple, si un catalyseur A est n√©cessaire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Turing discovered that patterns could be creat...</td>\n",
       "      <td>Turing a d√©couvert que des patrons pouvaient √™...</td>\n",
       "      <td>Turing d√©couvre que des motifs peuvent √™tre cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>If A and B then diffused through the container...</td>\n",
       "      <td>Si A et B ont ensuite diffus√© √† travers le con...</td>\n",
       "      <td>Si A et B se diffusent alors √† travers le cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>To calculate the extent of this, Turing would ...</td>\n",
       "      <td>Pour en calculer l'ampleur, Turing aurait eu b...</td>\n",
       "      <td>Pour calculer l'√©tendue de cela, Turing aurait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>These calculations gave the right qualitative ...</td>\n",
       "      <td>Ces calculs ont donn√© les bons r√©sultats quali...</td>\n",
       "      <td>Ces calculs ont donn√© les bons r√©sultats quali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>The Russian biochemist Boris Belousov had perf...</td>\n",
       "      <td>Le biochimiste russe Boris Belousov avait r√©al...</td>\n",
       "      <td>Le biochimiste russe Boris Belousov avait r√©al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Belousov was not aware of Turing's paper in th...</td>\n",
       "      <td>Belousov n'√©tait pas au courant du papier de T...</td>\n",
       "      <td>Belousov n'√©tait pas au courant du journal de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Although published before the structure and ro...</td>\n",
       "      <td>Bien que publi√© avant que la structure et le r...</td>\n",
       "      <td>Bien que publi√© avant que la structure et le r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>One of the early applications of Turing's pape...</td>\n",
       "      <td>L'une des premi√®res applications du papier de ...</td>\n",
       "      <td>L'une des premi√®res applications du papier de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Further research in the area suggests that Tur...</td>\n",
       "      <td>D'autres recherches dans le domaine sugg√®rent ...</td>\n",
       "      <td>D'autres recherches dans ce domaine sugg√®rent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>In 2012, Sheth, et al.</td>\n",
       "      <td>En 2012, Sheth et al.</td>\n",
       "      <td>En 2012, Sheth et al.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>found that in mice, removal of Hox genes cause...</td>\n",
       "      <td>Chez la souris, l'√©limination des g√®nes Hox en...</td>\n",
       "      <td>Chez la souris, l'√©limination des g√®nes Hox pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Later papers were not available until Collecte...</td>\n",
       "      <td>Par la suite, les documents n'ont pas √©t√© disp...</td>\n",
       "      <td>Des articles plus tard n'ont pas √©t√© disponibl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Talmud</td>\n",
       "      <td>Within Humanistic Judaism, Talmud is studied a...</td>\n",
       "      <td>Dans le juda√Øsme humaniste, Talmud est √©tudi√© ...</td>\n",
       "      <td>Au sein du juda√Øsme humaniste, Talmud est √©tud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>One religious ceremony practiced in Gabon and ...</td>\n",
       "      <td>Une c√©r√©monie religieuse pratiqu√©e au Gabon et...</td>\n",
       "      <td>Une c√©r√©monie religieuse pratiqu√©e au Gabon et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>In this state, depending upon the region, drum...</td>\n",
       "      <td>Dans cet √©tat, selon la r√©gion, des rythmes de...</td>\n",
       "      <td>Dans cet √©tat, selon la r√©gion des rythmes de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>When this trance-like state is witnessed and u...</td>\n",
       "      <td>Lorsque cet √©tat semblable √† la transe est vu ...</td>\n",
       "      <td>Quand cet √©tat de transe est vu et compris, le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>This builds skills at separating the feelings ...</td>\n",
       "      <td>Cela renforce les comp√©tences pour s√©parer les...</td>\n",
       "      <td>Cela cr√©e des comp√©tences pour s√©parer les sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>Such separation and subsequent contemplation o...</td>\n",
       "      <td>Une telle s√©paration et la contemplation subs√©...</td>\n",
       "      <td>Une telle s√©paration et la contemplation subs√©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Speech</td>\n",
       "      <td>In linguistics, articulatory phonetics is the ...</td>\n",
       "      <td>En linguistique, la phon√©tique articulaire est...</td>\n",
       "      <td>En linguistique, la phon√©tique articulaire est...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Speech sounds are categorized by manner of art...</td>\n",
       "      <td>Les sons de la parole sont class√©s par mode d'...</td>\n",
       "      <td>Les sons de la parole sont class√©s par mode d'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Place of articulation refers to where in the n...</td>\n",
       "      <td>Le lieu d'articulation d√©signe l'endroit o√π, d...</td>\n",
       "      <td>Le lieu d'articulation d√©signe l'endroit o√π da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Manner of articulation refers to the manner in...</td>\n",
       "      <td>Le mode d'articulation d√©signe la fa√ßon dont l...</td>\n",
       "      <td>Le mode d'articulation d√©signe la fa√ßon dont l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Speech</td>\n",
       "      <td>pulmonic, implosive, ejectives, and clicks), w...</td>\n",
       "      <td>pulmonique, implosif, √©jectifs et clics), que ...</td>\n",
       "      <td>pulmonique, implosif, √©jectifs et clics), que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Speech</td>\n",
       "      <td>The concept is primarily used for the producti...</td>\n",
       "      <td>Le concept est principalement utilis√© pour la ...</td>\n",
       "      <td>Le concept est principalement utilis√© pour la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Speech</td>\n",
       "      <td>For any place of articulation, there may be se...</td>\n",
       "      <td>Pour n'importe quel lieu d'articulation, il pe...</td>\n",
       "      <td>Pour n'importe quel lieu d'articulation il peu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Normal human speech is pulmonic, produced with...</td>\n",
       "      <td>La parole humaine normale est pulmonique, prod...</td>\n",
       "      <td>La parole humaine normale est pulmonique, prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Speech</td>\n",
       "      <td>However humans can pronounce words without the...</td>\n",
       "      <td>Cependant, les humains peuvent prononcer des m...</td>\n",
       "      <td>Cependant les humains peuvent prononcer des pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Speech production is a complex activity, and a...</td>\n",
       "      <td>La production de discours est une activit√© com...</td>\n",
       "      <td>La production de la parole est une activit√© co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          article  \\\n",
       "0                    Abstract art   \n",
       "1                    Abstract art   \n",
       "2                    Abstract art   \n",
       "3                    Abstract art   \n",
       "4                     Alan Turing   \n",
       "5                     Alan Turing   \n",
       "6                     Alan Turing   \n",
       "7                     Alan Turing   \n",
       "8                     Alan Turing   \n",
       "9                     Alan Turing   \n",
       "10                    Alan Turing   \n",
       "11                    Alan Turing   \n",
       "12                    Alan Turing   \n",
       "13                    Alan Turing   \n",
       "14                    Alan Turing   \n",
       "15                    Alan Turing   \n",
       "16                    Alan Turing   \n",
       "17                    Alan Turing   \n",
       "18                    Alan Turing   \n",
       "19                    Alan Turing   \n",
       "20                    Alan Turing   \n",
       "21                         Talmud   \n",
       "22  Traditional African religions   \n",
       "23  Traditional African religions   \n",
       "24  Traditional African religions   \n",
       "25  Traditional African religions   \n",
       "26  Traditional African religions   \n",
       "27                         Speech   \n",
       "28                         Speech   \n",
       "29                         Speech   \n",
       "30                         Speech   \n",
       "31                         Speech   \n",
       "32                         Speech   \n",
       "33                         Speech   \n",
       "34                         Speech   \n",
       "35                         Speech   \n",
       "36                         Speech   \n",
       "\n",
       "                                            sentences  \\\n",
       "0   In Chinese painting, abstraction can be traced...   \n",
       "1   While none of his paintings remain, this style...   \n",
       "2   The Chan buddhist painter Liang Kai (??, c. 11...   \n",
       "3   A late Song painter named Yu Jian, adept to Ti...   \n",
       "4   When Turing was 39 years old in 1951, he turne...   \n",
       "5   He was interested in morphogenesis, the develo...   \n",
       "6   He suggested that a system of chemicals reacti...   \n",
       "7   He used systems of partial differential equati...   \n",
       "8   For example, if a catalyst A is required for a...   \n",
       "9   Turing discovered that patterns could be creat...   \n",
       "10  If A and B then diffused through the container...   \n",
       "11  To calculate the extent of this, Turing would ...   \n",
       "12  These calculations gave the right qualitative ...   \n",
       "13  The Russian biochemist Boris Belousov had perf...   \n",
       "14  Belousov was not aware of Turing's paper in th...   \n",
       "15  Although published before the structure and ro...   \n",
       "16  One of the early applications of Turing's pape...   \n",
       "17  Further research in the area suggests that Tur...   \n",
       "18                             In 2012, Sheth, et al.   \n",
       "19  found that in mice, removal of Hox genes cause...   \n",
       "20  Later papers were not available until Collecte...   \n",
       "21  Within Humanistic Judaism, Talmud is studied a...   \n",
       "22  One religious ceremony practiced in Gabon and ...   \n",
       "23  In this state, depending upon the region, drum...   \n",
       "24  When this trance-like state is witnessed and u...   \n",
       "25  This builds skills at separating the feelings ...   \n",
       "26  Such separation and subsequent contemplation o...   \n",
       "27  In linguistics, articulatory phonetics is the ...   \n",
       "28  Speech sounds are categorized by manner of art...   \n",
       "29  Place of articulation refers to where in the n...   \n",
       "30  Manner of articulation refers to the manner in...   \n",
       "31  pulmonic, implosive, ejectives, and clicks), w...   \n",
       "32  The concept is primarily used for the producti...   \n",
       "33  For any place of articulation, there may be se...   \n",
       "34  Normal human speech is pulmonic, produced with...   \n",
       "35  However humans can pronounce words without the...   \n",
       "36  Speech production is a complex activity, and a...   \n",
       "\n",
       "                                        initial_model  \\\n",
       "0   Dans la peinture chinoise, l'abstraction peut ...   \n",
       "1   Bien qu'aucune de ses peintures ne reste, ce s...   \n",
       "2   Le peintre bouddhiste Chan Liang Kai (??, vers...   \n",
       "3   Un peintre de feu Song nomm√© Yu Jian, adepte d...   \n",
       "4   Quand Turing avait 39 ans en 1951, il se tourn...   \n",
       "5   Il s'int√©ressait √† la morphogen√®se, au d√©velop...   \n",
       "6   Il a sugg√©r√© qu'un syst√®me de produits chimiqu...   \n",
       "7   Il a utilis√© des syst√®mes d'√©quations diff√©ren...   \n",
       "8   Par exemple, si un catalyseur A est n√©cessaire...   \n",
       "9   Turing a d√©couvert que des patrons pouvaient √™...   \n",
       "10  Si A et B ont ensuite diffus√© √† travers le con...   \n",
       "11  Pour en calculer l'ampleur, Turing aurait eu b...   \n",
       "12  Ces calculs ont donn√© les bons r√©sultats quali...   \n",
       "13  Le biochimiste russe Boris Belousov avait r√©al...   \n",
       "14  Belousov n'√©tait pas au courant du papier de T...   \n",
       "15  Bien que publi√© avant que la structure et le r...   \n",
       "16  L'une des premi√®res applications du papier de ...   \n",
       "17  D'autres recherches dans le domaine sugg√®rent ...   \n",
       "18                              En 2012, Sheth et al.   \n",
       "19  Chez la souris, l'√©limination des g√®nes Hox en...   \n",
       "20  Par la suite, les documents n'ont pas √©t√© disp...   \n",
       "21  Dans le juda√Øsme humaniste, Talmud est √©tudi√© ...   \n",
       "22  Une c√©r√©monie religieuse pratiqu√©e au Gabon et...   \n",
       "23  Dans cet √©tat, selon la r√©gion, des rythmes de...   \n",
       "24  Lorsque cet √©tat semblable √† la transe est vu ...   \n",
       "25  Cela renforce les comp√©tences pour s√©parer les...   \n",
       "26  Une telle s√©paration et la contemplation subs√©...   \n",
       "27  En linguistique, la phon√©tique articulaire est...   \n",
       "28  Les sons de la parole sont class√©s par mode d'...   \n",
       "29  Le lieu d'articulation d√©signe l'endroit o√π, d...   \n",
       "30  Le mode d'articulation d√©signe la fa√ßon dont l...   \n",
       "31  pulmonique, implosif, √©jectifs et clics), que ...   \n",
       "32  Le concept est principalement utilis√© pour la ...   \n",
       "33  Pour n'importe quel lieu d'articulation, il pe...   \n",
       "34  La parole humaine normale est pulmonique, prod...   \n",
       "35  Cependant, les humains peuvent prononcer des m...   \n",
       "36  La production de discours est une activit√© com...   \n",
       "\n",
       "                                           ft_no_anno  \n",
       "0   Dans la peinture chinoise l'abstraction peut √™...  \n",
       "1   Bien qu'aucune de ses peintures ne subsiste, c...  \n",
       "2   Le peintre bouddhiste Chan Liang Kai (??, vers...  \n",
       "3   Un peintre de feu Song nomm√© Yu Jian, adepte d...  \n",
       "4   Quand Turing avait 39 ans en 1951 il se tourna...  \n",
       "5   Il s'int√©ressait √† la morphogen√®se, au d√©velop...  \n",
       "6   Il sugg√®re qu'un syst√®me de produits chimiques...  \n",
       "7   Il utilise des syst√®mes d'√©quations diff√©renti...  \n",
       "8   Par exemple, si un catalyseur A est n√©cessaire...  \n",
       "9   Turing d√©couvre que des motifs peuvent √™tre cr...  \n",
       "10  Si A et B se diffusent alors √† travers le cont...  \n",
       "11  Pour calculer l'√©tendue de cela, Turing aurait...  \n",
       "12  Ces calculs ont donn√© les bons r√©sultats quali...  \n",
       "13  Le biochimiste russe Boris Belousov avait r√©al...  \n",
       "14  Belousov n'√©tait pas au courant du journal de ...  \n",
       "15  Bien que publi√© avant que la structure et le r...  \n",
       "16  L'une des premi√®res applications du papier de ...  \n",
       "17  D'autres recherches dans ce domaine sugg√®rent ...  \n",
       "18                              En 2012, Sheth et al.  \n",
       "19  Chez la souris, l'√©limination des g√®nes Hox pr...  \n",
       "20  Des articles plus tard n'ont pas √©t√© disponibl...  \n",
       "21  Au sein du juda√Øsme humaniste, Talmud est √©tud...  \n",
       "22  Une c√©r√©monie religieuse pratiqu√©e au Gabon et...  \n",
       "23  Dans cet √©tat, selon la r√©gion des rythmes de ...  \n",
       "24  Quand cet √©tat de transe est vu et compris, le...  \n",
       "25  Cela cr√©e des comp√©tences pour s√©parer les sen...  \n",
       "26  Une telle s√©paration et la contemplation subs√©...  \n",
       "27  En linguistique, la phon√©tique articulaire est...  \n",
       "28  Les sons de la parole sont class√©s par mode d'...  \n",
       "29  Le lieu d'articulation d√©signe l'endroit o√π da...  \n",
       "30  Le mode d'articulation d√©signe la fa√ßon dont l...  \n",
       "31  pulmonique, implosif, √©jectifs et clics), que ...  \n",
       "32  Le concept est principalement utilis√© pour la ...  \n",
       "33  Pour n'importe quel lieu d'articulation il peu...  \n",
       "34  La parole humaine normale est pulmonique, prod...  \n",
       "35  Cependant les humains peuvent prononcer des pa...  \n",
       "36  La production de la parole est une activit√© co...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital.to_csv(\"translations_ft_no.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuned model: POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "      <th>initial_model</th>\n",
       "      <th>ft_no_anno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "      <td>Dans la peinture chinoise, l'abstraction peut ...</td>\n",
       "      <td>Dans la peinture chinoise l'abstraction peut √™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne subsiste, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "      <td>Un peintre de feu Song nomm√© Yu Jian, adepte d...</td>\n",
       "      <td>Un peintre de feu Song nomm√© Yu Jian, adepte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951, il se tourn...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951 il se tourna...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article                                          sentences  \\\n",
       "0  Abstract art  In Chinese painting, abstraction can be traced...   \n",
       "1  Abstract art  While none of his paintings remain, this style...   \n",
       "2  Abstract art  The Chan buddhist painter Liang Kai (??, c. 11...   \n",
       "3  Abstract art  A late Song painter named Yu Jian, adept to Ti...   \n",
       "4   Alan Turing  When Turing was 39 years old in 1951, he turne...   \n",
       "\n",
       "                                       initial_model  \\\n",
       "0  Dans la peinture chinoise, l'abstraction peut ...   \n",
       "1  Bien qu'aucune de ses peintures ne reste, ce s...   \n",
       "2  Le peintre bouddhiste Chan Liang Kai (??, vers...   \n",
       "3  Un peintre de feu Song nomm√© Yu Jian, adepte d...   \n",
       "4  Quand Turing avait 39 ans en 1951, il se tourn...   \n",
       "\n",
       "                                          ft_no_anno  \n",
       "0  Dans la peinture chinoise l'abstraction peut √™...  \n",
       "1  Bien qu'aucune de ses peintures ne subsiste, c...  \n",
       "2  Le peintre bouddhiste Chan Liang Kai (??, vers...  \n",
       "3  Un peintre de feu Song nomm√© Yu Jian, adepte d...  \n",
       "4  Quand Turing avait 39 ans en 1951 il se tourna...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuned model: WA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital = pd.read_csv(\"translations_ft_no.csv\")\n",
    "vital.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\MTproject\\MTvenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans la peinture chinoise l'abstraction peut √™tre trac√©e √† la dynastie Tang peintre Wang Mo (??), qui est cr√©dit√© d'avoir invent√© le style de peinture √©claboussure-puce.................................................................................\n",
      "Bien qu'aucune de ses peintures ne reste, ce style est clairement vu dans certains Song Dynasty Paintings.....................................................................................................\n",
      "Le peintre bouddhiste de Chan Liang Kai (?? vers 1140=1210) a appliqu√© le style √† la peinture figurative dans son ¬´ Immortal in splashed enk ¬ª dans lequel une repr√©sentation pr√©cise est sacrifi√©e pour am√©liorer la spontan√©it√© li√©e √† l'esprit non rationnel de l'√©clair√©.\n",
      "Un peintre de feu Song nomm√© Yu Jian, adepte du Bouddhisme de Tiantai a cr√©√© une s√©rie de paysages d'encre √©clabouss√©s qui ont finalement inspir√© de nombreux peintres japonais Zen.\n",
      "Quand Turing avait 39 ans en 1951 il se tourna vers la biologie math√©matique et publia finalement son chef-d'≈ìuvre ¬´ The Chemical Bases of Morphogen√®se ¬ª en janvier 1952.\n",
      "Il s'int√©ressait √† la morphogen√®se, au d√©veloppement de motifs et de formes dans les organismes biologiques.\n",
      "Il sugg√®re qu'un syst√®me de produits chimiques r√©agissant l'un avec l'autre et diffusant dans l'espace appel√© syst√®me de diffusion de r√©action pourrait expliquer ¬´ les principaux ph√©nom√®nes de morphogen√®se ¬ª.\n",
      "Il utilise des syst√®mes d'√©quations diff√©rentielles partielles pour mod√©liser les r√©actions chimiques catalytiques.\n",
      "Par exemple, si un catalyseur A est n√©cessaire pour qu'une certaine r√©action chimique ait lieu et si la r√©action produite plus du catalyseur A est alors nous disons que la r√©action est autocatalytique et qu'il y a un retour positif qui peut √™tre mod√©lis√© par des √©quations diff√©rentielles non lin√©aires.\n",
      "Turing d√©couvre que des motifs peuvent √™tre cr√©√©s si la r√©action chimique produit non seulement le catalyseur A mais aussi un inhibiteur B qui ralentit la production de A.\n",
      "Si A et B ont ensuite diffus√© √† travers le conteneur √† des vitesses diff√©rentes, alors vous pourriez avoir certaines r√©gions o√π A a domin√© et certaines o√π B l'a fait.\n",
      "Pour calculer l'√©tendue de cela, Turing aurait eu besoin d'un ordinateur puissant, mais ceux-ci n'√©taient pas si librement disponibles en 1951, il a donc d√ª utiliser des approximations lin√©aires pour r√©soudre les √©quations √† la main.\n",
      "Ces calculs donnent les bons r√©sultats qualitatifs et produisent par exemple un m√©lange uniforme qui est √©trangement assez espac√© r√©guli√®rement des taches rouges fixes.\n",
      "Le biochimiste russe Boris Belousov avait r√©alis√© des exp√©riences avec des r√©sultats similaires mais ne pouvait pas faire publier ses articles √† cause du pr√©jug√© contemporain selon lequel une telle chose violait la deuxi√®me loi de la thermodynamique.\n",
      "Belousov n'√©tait pas au courant du journal de Turing dans les Transactions Philosophiques de la Royal Society.\n",
      "Bien que publi√© avant que la structure et le r√¥le de l'ADN aient √©t√© compris, le travail de Turing sur la morphogen√®se reste pertinent aujourd'hui et est consid√©r√© comme un travail s√©minal en biologie math√©matique.\n",
      "L'une des premi√®res applications du papier de Turing fut le travail de James Murray expliquant les taches et les rayures sur la fourrure des chats, grands et petits.\n",
      "D'autres recherches dans ce domaine sugg√®rent que les travaux de Turing peuvent expliquer en partie la croissance des ¬´ plumes, des follicules pileux, le mod√®le de ramification des poumons et m√™me l'asym√©trie gauche-droite qui place le c≈ìur sur le c√¥t√© gauche de la poitrine ¬ª.\n",
      "En 2012, Sheth et al.\n",
      "Chez la souris, l'√©limination des g√®nes Hox provoque une augmentation du nombre de chiffres sans augmentation de la taille globale du membre, ce qui sugg√®re que les g√®nes Hox contr√¥lent la formation de chiffres en ajustant la longueur d'onde d'un m√©canisme de type Turing.\n",
      "Plus tard les articles n'ont pas √©t√© disponibles avant que Collected Works of A. M. Turing n'ait √©t√© publi√© en 1992.\n",
      "Au sein du juda√Øsme humaniste le Talmud est √©tudi√© comme un texte historique afin de d√©couvrir comment il peut d√©montrer sa pertinence pratique pour la vie d'aujourd'hui............................................................................................\n",
      "Une c√©r√©monie religieuse pratiqu√©e au Gabon et au Cameroun est l'Okuyi pratiqu√© par plusieurs groupes ethniques bantous.\n",
      "Dans cet √©tat, selon la r√©gion, les rythmes de tambour ou instrumentaux jou√©s par des musiciens respect√©s (chacun √©tant unique √† une divinit√© ou un anc√™tre donn√©), les participants incarnent une divinit√© ou un anc√™tre, une √©nergie ou un √©tat d'esprit en effectuant des mouvements rituels distincts ou des danses qui renforcent encore leur conscience √©lev√©e.\n",
      "Lorsque cet √©tat de transe est vu et compris, les adh√©rents sont mis au courant d'une fa√ßon de contempler l'incarnation pure ou symbolique d'un √©tat d'esprit particulier ou d'un cadre de r√©f√©rence particulier................................................................................\n",
      "Cela cr√©e des comp√©tences pour s√©parer les sentiments suscit√©s par cet √©tat d'esprit de leurs manifestations situationnelles dans la vie de tous les jours..................................................................................................\n",
      "Une telle s√©paration et la contemplation subs√©quente de la nature et des sources d'√©nergie pure ou de sentiments aide les participants √† les g√©rer et √† les accepter lorsqu'ils apparaissent dans des contextes banals.\n",
      "En linguistique, la phon√©tique articulaire est l'√©tude de la fa√ßon dont la langue, les l√®vres, la m√¢choire, les cordes vocales et d'autres organes de parole sont utilis√©s pour faire des sons.\n",
      "Les sons de la parole sont class√©s par mode d'articulation et lieu d'articulation.\n",
      "Le lieu d'articulation d√©signe l'endroit o√π dans le cou ou la bouche le courant d'air est constrictionn√©....................................................................................................\n",
      "Le mode d'articulation d√©signe la mani√®re dont les organes de parole interagissent, comme la mani√®re dont l'air est restreint et quelle forme de courant d'air est utilis√©e (p. ex.\n",
      "pulmonique, implosif, √©jectifs et clics), que les cordes vocales vibrent ou non et que la cavit√© nasale soit ouverte au courant d'air.\n",
      "Le concept est principalement utilis√© pour la production de consonnes mais peut √™tre utilis√© pour les voyelles dans des qualit√©s telles que la vocation et la nasalisation.\n",
      "Pour n'importe quel lieu d'articulation il peut y avoir plusieurs mani√®res d'articulation et donc plusieurs consonnes homorganiques.\n",
      "La parole humaine normale est pulmonique, produite avec la pression des poumons qui cr√©e la phonation dans les glattis dans le larynx qui est ensuite modifi√©e par le chant et la bouche en diff√©rentes voyelles et consonnes..........................................................................\n",
      "Cependant les humains peuvent prononcer des paroles sans utiliser les poumons et les glattis dans le discours alaryng√©al, dont il existe trois types : le discours √©sophageal, le discours pharyng√©al et le discours buccal (plus connu sous le nom de Donald Duck).\n",
      "La production de discours est une activit√© complexe et par cons√©quent les erreurs sont fr√©quentes, en particulier chez les enfants et les adolescents......................................................................................................\n"
     ]
    }
   ],
   "source": [
    "outputs_wa = []\n",
    "\n",
    "for input_sentence in vital[\"sentences\"]:\n",
    "    tokenized_sentence = tokenizer([input_sentence], return_tensors='np')\n",
    "    out = model_with_wa_trg_ids.generate(**tokenized_sentence, max_length=128)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        output_sentence = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        print(output_sentence)\n",
    "        outputs_wa.append(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "      <th>initial_model</th>\n",
       "      <th>ft_no_anno</th>\n",
       "      <th>ft_wa_trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "      <td>Dans la peinture chinoise, l'abstraction peut ...</td>\n",
       "      <td>Dans la peinture chinoise l'abstraction peut √™...</td>\n",
       "      <td>Dans la peinture chinoise l'abstraction peut √™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne subsiste, c...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "      <td>Le peintre bouddhiste de Chan Liang Kai (?? ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "      <td>Un peintre de feu Song nomm√© Yu Jian, adepte d...</td>\n",
       "      <td>Un peintre de feu Song nomm√© Yu Jian, adepte d...</td>\n",
       "      <td>Un peintre de feu Song nomm√© Yu Jian, adepte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951, il se tourn...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951 il se tourna...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951 il se tourna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>He was interested in morphogenesis, the develo...</td>\n",
       "      <td>Il s'int√©ressait √† la morphogen√®se, au d√©velop...</td>\n",
       "      <td>Il s'int√©ressait √† la morphogen√®se, au d√©velop...</td>\n",
       "      <td>Il s'int√©ressait √† la morphogen√®se, au d√©velop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>He suggested that a system of chemicals reacti...</td>\n",
       "      <td>Il a sugg√©r√© qu'un syst√®me de produits chimiqu...</td>\n",
       "      <td>Il sugg√®re qu'un syst√®me de produits chimiques...</td>\n",
       "      <td>Il sugg√®re qu'un syst√®me de produits chimiques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>He used systems of partial differential equati...</td>\n",
       "      <td>Il a utilis√© des syst√®mes d'√©quations diff√©ren...</td>\n",
       "      <td>Il utilise des syst√®mes d'√©quations diff√©renti...</td>\n",
       "      <td>Il utilise des syst√®mes d'√©quations diff√©renti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>For example, if a catalyst A is required for a...</td>\n",
       "      <td>Par exemple, si un catalyseur A est n√©cessaire...</td>\n",
       "      <td>Par exemple, si un catalyseur A est n√©cessaire...</td>\n",
       "      <td>Par exemple, si un catalyseur A est n√©cessaire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Turing discovered that patterns could be creat...</td>\n",
       "      <td>Turing a d√©couvert que des patrons pouvaient √™...</td>\n",
       "      <td>Turing d√©couvre que des motifs peuvent √™tre cr...</td>\n",
       "      <td>Turing d√©couvre que des motifs peuvent √™tre cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>If A and B then diffused through the container...</td>\n",
       "      <td>Si A et B ont ensuite diffus√© √† travers le con...</td>\n",
       "      <td>Si A et B se diffusent alors √† travers le cont...</td>\n",
       "      <td>Si A et B ont ensuite diffus√© √† travers le con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>To calculate the extent of this, Turing would ...</td>\n",
       "      <td>Pour en calculer l'ampleur, Turing aurait eu b...</td>\n",
       "      <td>Pour calculer l'√©tendue de cela, Turing aurait...</td>\n",
       "      <td>Pour calculer l'√©tendue de cela, Turing aurait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>These calculations gave the right qualitative ...</td>\n",
       "      <td>Ces calculs ont donn√© les bons r√©sultats quali...</td>\n",
       "      <td>Ces calculs ont donn√© les bons r√©sultats quali...</td>\n",
       "      <td>Ces calculs donnent les bons r√©sultats qualita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>The Russian biochemist Boris Belousov had perf...</td>\n",
       "      <td>Le biochimiste russe Boris Belousov avait r√©al...</td>\n",
       "      <td>Le biochimiste russe Boris Belousov avait r√©al...</td>\n",
       "      <td>Le biochimiste russe Boris Belousov avait r√©al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Belousov was not aware of Turing's paper in th...</td>\n",
       "      <td>Belousov n'√©tait pas au courant du papier de T...</td>\n",
       "      <td>Belousov n'√©tait pas au courant du journal de ...</td>\n",
       "      <td>Belousov n'√©tait pas au courant du journal de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Although published before the structure and ro...</td>\n",
       "      <td>Bien que publi√© avant que la structure et le r...</td>\n",
       "      <td>Bien que publi√© avant que la structure et le r...</td>\n",
       "      <td>Bien que publi√© avant que la structure et le r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>One of the early applications of Turing's pape...</td>\n",
       "      <td>L'une des premi√®res applications du papier de ...</td>\n",
       "      <td>L'une des premi√®res applications du papier de ...</td>\n",
       "      <td>L'une des premi√®res applications du papier de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Further research in the area suggests that Tur...</td>\n",
       "      <td>D'autres recherches dans le domaine sugg√®rent ...</td>\n",
       "      <td>D'autres recherches dans ce domaine sugg√®rent ...</td>\n",
       "      <td>D'autres recherches dans ce domaine sugg√®rent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>In 2012, Sheth, et al.</td>\n",
       "      <td>En 2012, Sheth et al.</td>\n",
       "      <td>En 2012, Sheth et al.</td>\n",
       "      <td>En 2012, Sheth et al.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>found that in mice, removal of Hox genes cause...</td>\n",
       "      <td>Chez la souris, l'√©limination des g√®nes Hox en...</td>\n",
       "      <td>Chez la souris, l'√©limination des g√®nes Hox pr...</td>\n",
       "      <td>Chez la souris, l'√©limination des g√®nes Hox pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>Later papers were not available until Collecte...</td>\n",
       "      <td>Par la suite, les documents n'ont pas √©t√© disp...</td>\n",
       "      <td>Des articles plus tard n'ont pas √©t√© disponibl...</td>\n",
       "      <td>Plus tard les articles n'ont pas √©t√© disponibl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Talmud</td>\n",
       "      <td>Within Humanistic Judaism, Talmud is studied a...</td>\n",
       "      <td>Dans le juda√Øsme humaniste, Talmud est √©tudi√© ...</td>\n",
       "      <td>Au sein du juda√Øsme humaniste, Talmud est √©tud...</td>\n",
       "      <td>Au sein du juda√Øsme humaniste le Talmud est √©t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>One religious ceremony practiced in Gabon and ...</td>\n",
       "      <td>Une c√©r√©monie religieuse pratiqu√©e au Gabon et...</td>\n",
       "      <td>Une c√©r√©monie religieuse pratiqu√©e au Gabon et...</td>\n",
       "      <td>Une c√©r√©monie religieuse pratiqu√©e au Gabon et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>In this state, depending upon the region, drum...</td>\n",
       "      <td>Dans cet √©tat, selon la r√©gion, des rythmes de...</td>\n",
       "      <td>Dans cet √©tat, selon la r√©gion des rythmes de ...</td>\n",
       "      <td>Dans cet √©tat, selon la r√©gion, les rythmes de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>When this trance-like state is witnessed and u...</td>\n",
       "      <td>Lorsque cet √©tat semblable √† la transe est vu ...</td>\n",
       "      <td>Quand cet √©tat de transe est vu et compris, le...</td>\n",
       "      <td>Lorsque cet √©tat de transe est vu et compris, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>This builds skills at separating the feelings ...</td>\n",
       "      <td>Cela renforce les comp√©tences pour s√©parer les...</td>\n",
       "      <td>Cela cr√©e des comp√©tences pour s√©parer les sen...</td>\n",
       "      <td>Cela cr√©e des comp√©tences pour s√©parer les sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Traditional African religions</td>\n",
       "      <td>Such separation and subsequent contemplation o...</td>\n",
       "      <td>Une telle s√©paration et la contemplation subs√©...</td>\n",
       "      <td>Une telle s√©paration et la contemplation subs√©...</td>\n",
       "      <td>Une telle s√©paration et la contemplation subs√©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Speech</td>\n",
       "      <td>In linguistics, articulatory phonetics is the ...</td>\n",
       "      <td>En linguistique, la phon√©tique articulaire est...</td>\n",
       "      <td>En linguistique, la phon√©tique articulaire est...</td>\n",
       "      <td>En linguistique, la phon√©tique articulaire est...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Speech sounds are categorized by manner of art...</td>\n",
       "      <td>Les sons de la parole sont class√©s par mode d'...</td>\n",
       "      <td>Les sons de la parole sont class√©s par mode d'...</td>\n",
       "      <td>Les sons de la parole sont class√©s par mode d'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Place of articulation refers to where in the n...</td>\n",
       "      <td>Le lieu d'articulation d√©signe l'endroit o√π, d...</td>\n",
       "      <td>Le lieu d'articulation d√©signe l'endroit o√π da...</td>\n",
       "      <td>Le lieu d'articulation d√©signe l'endroit o√π da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Manner of articulation refers to the manner in...</td>\n",
       "      <td>Le mode d'articulation d√©signe la fa√ßon dont l...</td>\n",
       "      <td>Le mode d'articulation d√©signe la fa√ßon dont l...</td>\n",
       "      <td>Le mode d'articulation d√©signe la mani√®re dont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Speech</td>\n",
       "      <td>pulmonic, implosive, ejectives, and clicks), w...</td>\n",
       "      <td>pulmonique, implosif, √©jectifs et clics), que ...</td>\n",
       "      <td>pulmonique, implosif, √©jectifs et clics), que ...</td>\n",
       "      <td>pulmonique, implosif, √©jectifs et clics), que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Speech</td>\n",
       "      <td>The concept is primarily used for the producti...</td>\n",
       "      <td>Le concept est principalement utilis√© pour la ...</td>\n",
       "      <td>Le concept est principalement utilis√© pour la ...</td>\n",
       "      <td>Le concept est principalement utilis√© pour la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Speech</td>\n",
       "      <td>For any place of articulation, there may be se...</td>\n",
       "      <td>Pour n'importe quel lieu d'articulation, il pe...</td>\n",
       "      <td>Pour n'importe quel lieu d'articulation il peu...</td>\n",
       "      <td>Pour n'importe quel lieu d'articulation il peu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Normal human speech is pulmonic, produced with...</td>\n",
       "      <td>La parole humaine normale est pulmonique, prod...</td>\n",
       "      <td>La parole humaine normale est pulmonique, prod...</td>\n",
       "      <td>La parole humaine normale est pulmonique, prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Speech</td>\n",
       "      <td>However humans can pronounce words without the...</td>\n",
       "      <td>Cependant, les humains peuvent prononcer des m...</td>\n",
       "      <td>Cependant les humains peuvent prononcer des pa...</td>\n",
       "      <td>Cependant les humains peuvent prononcer des pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Speech</td>\n",
       "      <td>Speech production is a complex activity, and a...</td>\n",
       "      <td>La production de discours est une activit√© com...</td>\n",
       "      <td>La production de la parole est une activit√© co...</td>\n",
       "      <td>La production de discours est une activit√© com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          article  \\\n",
       "0                    Abstract art   \n",
       "1                    Abstract art   \n",
       "2                    Abstract art   \n",
       "3                    Abstract art   \n",
       "4                     Alan Turing   \n",
       "5                     Alan Turing   \n",
       "6                     Alan Turing   \n",
       "7                     Alan Turing   \n",
       "8                     Alan Turing   \n",
       "9                     Alan Turing   \n",
       "10                    Alan Turing   \n",
       "11                    Alan Turing   \n",
       "12                    Alan Turing   \n",
       "13                    Alan Turing   \n",
       "14                    Alan Turing   \n",
       "15                    Alan Turing   \n",
       "16                    Alan Turing   \n",
       "17                    Alan Turing   \n",
       "18                    Alan Turing   \n",
       "19                    Alan Turing   \n",
       "20                    Alan Turing   \n",
       "21                         Talmud   \n",
       "22  Traditional African religions   \n",
       "23  Traditional African religions   \n",
       "24  Traditional African religions   \n",
       "25  Traditional African religions   \n",
       "26  Traditional African religions   \n",
       "27                         Speech   \n",
       "28                         Speech   \n",
       "29                         Speech   \n",
       "30                         Speech   \n",
       "31                         Speech   \n",
       "32                         Speech   \n",
       "33                         Speech   \n",
       "34                         Speech   \n",
       "35                         Speech   \n",
       "36                         Speech   \n",
       "\n",
       "                                            sentences  \\\n",
       "0   In Chinese painting, abstraction can be traced...   \n",
       "1   While none of his paintings remain, this style...   \n",
       "2   The Chan buddhist painter Liang Kai (??, c. 11...   \n",
       "3   A late Song painter named Yu Jian, adept to Ti...   \n",
       "4   When Turing was 39 years old in 1951, he turne...   \n",
       "5   He was interested in morphogenesis, the develo...   \n",
       "6   He suggested that a system of chemicals reacti...   \n",
       "7   He used systems of partial differential equati...   \n",
       "8   For example, if a catalyst A is required for a...   \n",
       "9   Turing discovered that patterns could be creat...   \n",
       "10  If A and B then diffused through the container...   \n",
       "11  To calculate the extent of this, Turing would ...   \n",
       "12  These calculations gave the right qualitative ...   \n",
       "13  The Russian biochemist Boris Belousov had perf...   \n",
       "14  Belousov was not aware of Turing's paper in th...   \n",
       "15  Although published before the structure and ro...   \n",
       "16  One of the early applications of Turing's pape...   \n",
       "17  Further research in the area suggests that Tur...   \n",
       "18                             In 2012, Sheth, et al.   \n",
       "19  found that in mice, removal of Hox genes cause...   \n",
       "20  Later papers were not available until Collecte...   \n",
       "21  Within Humanistic Judaism, Talmud is studied a...   \n",
       "22  One religious ceremony practiced in Gabon and ...   \n",
       "23  In this state, depending upon the region, drum...   \n",
       "24  When this trance-like state is witnessed and u...   \n",
       "25  This builds skills at separating the feelings ...   \n",
       "26  Such separation and subsequent contemplation o...   \n",
       "27  In linguistics, articulatory phonetics is the ...   \n",
       "28  Speech sounds are categorized by manner of art...   \n",
       "29  Place of articulation refers to where in the n...   \n",
       "30  Manner of articulation refers to the manner in...   \n",
       "31  pulmonic, implosive, ejectives, and clicks), w...   \n",
       "32  The concept is primarily used for the producti...   \n",
       "33  For any place of articulation, there may be se...   \n",
       "34  Normal human speech is pulmonic, produced with...   \n",
       "35  However humans can pronounce words without the...   \n",
       "36  Speech production is a complex activity, and a...   \n",
       "\n",
       "                                        initial_model  \\\n",
       "0   Dans la peinture chinoise, l'abstraction peut ...   \n",
       "1   Bien qu'aucune de ses peintures ne reste, ce s...   \n",
       "2   Le peintre bouddhiste Chan Liang Kai (??, vers...   \n",
       "3   Un peintre de feu Song nomm√© Yu Jian, adepte d...   \n",
       "4   Quand Turing avait 39 ans en 1951, il se tourn...   \n",
       "5   Il s'int√©ressait √† la morphogen√®se, au d√©velop...   \n",
       "6   Il a sugg√©r√© qu'un syst√®me de produits chimiqu...   \n",
       "7   Il a utilis√© des syst√®mes d'√©quations diff√©ren...   \n",
       "8   Par exemple, si un catalyseur A est n√©cessaire...   \n",
       "9   Turing a d√©couvert que des patrons pouvaient √™...   \n",
       "10  Si A et B ont ensuite diffus√© √† travers le con...   \n",
       "11  Pour en calculer l'ampleur, Turing aurait eu b...   \n",
       "12  Ces calculs ont donn√© les bons r√©sultats quali...   \n",
       "13  Le biochimiste russe Boris Belousov avait r√©al...   \n",
       "14  Belousov n'√©tait pas au courant du papier de T...   \n",
       "15  Bien que publi√© avant que la structure et le r...   \n",
       "16  L'une des premi√®res applications du papier de ...   \n",
       "17  D'autres recherches dans le domaine sugg√®rent ...   \n",
       "18                              En 2012, Sheth et al.   \n",
       "19  Chez la souris, l'√©limination des g√®nes Hox en...   \n",
       "20  Par la suite, les documents n'ont pas √©t√© disp...   \n",
       "21  Dans le juda√Øsme humaniste, Talmud est √©tudi√© ...   \n",
       "22  Une c√©r√©monie religieuse pratiqu√©e au Gabon et...   \n",
       "23  Dans cet √©tat, selon la r√©gion, des rythmes de...   \n",
       "24  Lorsque cet √©tat semblable √† la transe est vu ...   \n",
       "25  Cela renforce les comp√©tences pour s√©parer les...   \n",
       "26  Une telle s√©paration et la contemplation subs√©...   \n",
       "27  En linguistique, la phon√©tique articulaire est...   \n",
       "28  Les sons de la parole sont class√©s par mode d'...   \n",
       "29  Le lieu d'articulation d√©signe l'endroit o√π, d...   \n",
       "30  Le mode d'articulation d√©signe la fa√ßon dont l...   \n",
       "31  pulmonique, implosif, √©jectifs et clics), que ...   \n",
       "32  Le concept est principalement utilis√© pour la ...   \n",
       "33  Pour n'importe quel lieu d'articulation, il pe...   \n",
       "34  La parole humaine normale est pulmonique, prod...   \n",
       "35  Cependant, les humains peuvent prononcer des m...   \n",
       "36  La production de discours est une activit√© com...   \n",
       "\n",
       "                                           ft_no_anno  \\\n",
       "0   Dans la peinture chinoise l'abstraction peut √™...   \n",
       "1   Bien qu'aucune de ses peintures ne subsiste, c...   \n",
       "2   Le peintre bouddhiste Chan Liang Kai (??, vers...   \n",
       "3   Un peintre de feu Song nomm√© Yu Jian, adepte d...   \n",
       "4   Quand Turing avait 39 ans en 1951 il se tourna...   \n",
       "5   Il s'int√©ressait √† la morphogen√®se, au d√©velop...   \n",
       "6   Il sugg√®re qu'un syst√®me de produits chimiques...   \n",
       "7   Il utilise des syst√®mes d'√©quations diff√©renti...   \n",
       "8   Par exemple, si un catalyseur A est n√©cessaire...   \n",
       "9   Turing d√©couvre que des motifs peuvent √™tre cr...   \n",
       "10  Si A et B se diffusent alors √† travers le cont...   \n",
       "11  Pour calculer l'√©tendue de cela, Turing aurait...   \n",
       "12  Ces calculs ont donn√© les bons r√©sultats quali...   \n",
       "13  Le biochimiste russe Boris Belousov avait r√©al...   \n",
       "14  Belousov n'√©tait pas au courant du journal de ...   \n",
       "15  Bien que publi√© avant que la structure et le r...   \n",
       "16  L'une des premi√®res applications du papier de ...   \n",
       "17  D'autres recherches dans ce domaine sugg√®rent ...   \n",
       "18                              En 2012, Sheth et al.   \n",
       "19  Chez la souris, l'√©limination des g√®nes Hox pr...   \n",
       "20  Des articles plus tard n'ont pas √©t√© disponibl...   \n",
       "21  Au sein du juda√Øsme humaniste, Talmud est √©tud...   \n",
       "22  Une c√©r√©monie religieuse pratiqu√©e au Gabon et...   \n",
       "23  Dans cet √©tat, selon la r√©gion des rythmes de ...   \n",
       "24  Quand cet √©tat de transe est vu et compris, le...   \n",
       "25  Cela cr√©e des comp√©tences pour s√©parer les sen...   \n",
       "26  Une telle s√©paration et la contemplation subs√©...   \n",
       "27  En linguistique, la phon√©tique articulaire est...   \n",
       "28  Les sons de la parole sont class√©s par mode d'...   \n",
       "29  Le lieu d'articulation d√©signe l'endroit o√π da...   \n",
       "30  Le mode d'articulation d√©signe la fa√ßon dont l...   \n",
       "31  pulmonique, implosif, √©jectifs et clics), que ...   \n",
       "32  Le concept est principalement utilis√© pour la ...   \n",
       "33  Pour n'importe quel lieu d'articulation il peu...   \n",
       "34  La parole humaine normale est pulmonique, prod...   \n",
       "35  Cependant les humains peuvent prononcer des pa...   \n",
       "36  La production de la parole est une activit√© co...   \n",
       "\n",
       "                                            ft_wa_trg  \n",
       "0   Dans la peinture chinoise l'abstraction peut √™...  \n",
       "1   Bien qu'aucune de ses peintures ne reste, ce s...  \n",
       "2   Le peintre bouddhiste de Chan Liang Kai (?? ve...  \n",
       "3   Un peintre de feu Song nomm√© Yu Jian, adepte d...  \n",
       "4   Quand Turing avait 39 ans en 1951 il se tourna...  \n",
       "5   Il s'int√©ressait √† la morphogen√®se, au d√©velop...  \n",
       "6   Il sugg√®re qu'un syst√®me de produits chimiques...  \n",
       "7   Il utilise des syst√®mes d'√©quations diff√©renti...  \n",
       "8   Par exemple, si un catalyseur A est n√©cessaire...  \n",
       "9   Turing d√©couvre que des motifs peuvent √™tre cr...  \n",
       "10  Si A et B ont ensuite diffus√© √† travers le con...  \n",
       "11  Pour calculer l'√©tendue de cela, Turing aurait...  \n",
       "12  Ces calculs donnent les bons r√©sultats qualita...  \n",
       "13  Le biochimiste russe Boris Belousov avait r√©al...  \n",
       "14  Belousov n'√©tait pas au courant du journal de ...  \n",
       "15  Bien que publi√© avant que la structure et le r...  \n",
       "16  L'une des premi√®res applications du papier de ...  \n",
       "17  D'autres recherches dans ce domaine sugg√®rent ...  \n",
       "18                              En 2012, Sheth et al.  \n",
       "19  Chez la souris, l'√©limination des g√®nes Hox pr...  \n",
       "20  Plus tard les articles n'ont pas √©t√© disponibl...  \n",
       "21  Au sein du juda√Øsme humaniste le Talmud est √©t...  \n",
       "22  Une c√©r√©monie religieuse pratiqu√©e au Gabon et...  \n",
       "23  Dans cet √©tat, selon la r√©gion, les rythmes de...  \n",
       "24  Lorsque cet √©tat de transe est vu et compris, ...  \n",
       "25  Cela cr√©e des comp√©tences pour s√©parer les sen...  \n",
       "26  Une telle s√©paration et la contemplation subs√©...  \n",
       "27  En linguistique, la phon√©tique articulaire est...  \n",
       "28  Les sons de la parole sont class√©s par mode d'...  \n",
       "29  Le lieu d'articulation d√©signe l'endroit o√π da...  \n",
       "30  Le mode d'articulation d√©signe la mani√®re dont...  \n",
       "31  pulmonique, implosif, √©jectifs et clics), que ...  \n",
       "32  Le concept est principalement utilis√© pour la ...  \n",
       "33  Pour n'importe quel lieu d'articulation il peu...  \n",
       "34  La parole humaine normale est pulmonique, prod...  \n",
       "35  Cependant les humains peuvent prononcer des pa...  \n",
       "36  La production de discours est une activit√© com...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital[\"ft_wa_trg\"] = outputs_wa\n",
    "vital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital.to_csv(\"translations_ft_wa.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Translation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('MTvenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc73298858d0756eee327543f37df2c3feab8645937b6dca4052bb1287dcab38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
