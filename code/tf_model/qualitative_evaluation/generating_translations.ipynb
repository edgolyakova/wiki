{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install transformers[sentencepiece] datasets\n",
    "#! pip install sacrebleu sentencepiece\n",
    "#! pip install huggingface_hub\n",
    "# pip install tensorflow==2.9\n",
    "# pip freeze > requirements.txt\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "# !apt install git-lfs\n",
    "# !git config --global user.email \"you@example.com\"\n",
    "# !git config --global user.name \"Your Name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ff433e52dc46989227148086002ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13cbca759e04de4b35e808c7304d9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Helsinki-NLP/opus-mt-en-fr were not used when initializing TFMarianMTModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing TFMarianMTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMarianMTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - ETA: 0s - loss: 1.5174WARNING:tensorflow:5 out of the last 5 calls to <function KerasMetricCallback.on_epoch_end.<locals>.generation_function at 0x7fddcd073af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function KerasMetricCallback.on_epoch_end.<locals>.generation_function at 0x7fddcd073af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "94/94 [==============================] - 1862s 20s/step - loss: 1.5174 - val_loss: 1.4000 - bleu: 16.3806 - gen_len: 60.2037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdde34375e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk, load_dataset\n",
    "from evaluate import load\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from transformers import AutoTokenizer\n",
    "import spacy\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "import numpy as np\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import AdamWeightDecay\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "def csv_to_dataset(filename, source_lang, target_lang, pos_tags=False, wa_tags=False, store=False):\n",
    "    data = pd.read_csv(filename)\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['translation'] = [{source_lang: x, target_lang: y} for x, y in zip(data[source_lang], data[target_lang])]\n",
    "    if pos_tags:\n",
    "        new_df['pos'] = [{source_lang: x, target_lang: y} for x, y in zip(data[f'pos_{source_lang}'], data[f'pos_{target_lang}'])]\n",
    "    if wa_tags:\n",
    "        new_df['wa'] = data['wa']\n",
    "    return Dataset.from_pandas(new_df).train_test_split(test_size=0.2)\n",
    "\n",
    "loaded_dataset = load_from_disk('../fr_dataset_split.hf')\n",
    "loaded_dataset.remove_columns(['pos', 'wa'])\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "metric = load(\"sacrebleu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "if \"mbart\" in model_checkpoint:\n",
    "    tokenizer.src_lang = \"en-XX\"\n",
    "    tokenizer.tgt_lang = \"fr-FR\"\n",
    "\n",
    "\n",
    "en_pos_sp = spacy.load(\"en_core_web_sm\")\n",
    "fr_pos_sp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"translate English to French: \"\n",
    "else:\n",
    "    prefix = \"\"\n",
    "\n",
    "def token_to_pos(token, lang):\n",
    "    if lang == 'en':\n",
    "        decoded = list(en_pos_sp(tokenizer.decode(token)))\n",
    "    elif lang == 'fr':\n",
    "        decoded = list(fr_pos_sp(tokenizer.decode(token)))\n",
    "    return decoded[-1].pos if decoded else -1\n",
    "\n",
    "def get_pos_tags(tokenized_sent, lang):\n",
    "    return list(map(lambda x: token_to_pos(x, lang), tokenized_sent))\n",
    "\n",
    "def metric_fn(eval_predictions):\n",
    "    preds, labels = eval_predictions\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # We use -100 to mask labels - replace it with the tokenizer pad token when decoding\n",
    "    # so that no output is emitted for these\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "# Pos_tags need to be set to True in the cell\n",
    "pos_tags = True\n",
    "wa_tags = False\n",
    "wa_type = None\n",
    "\n",
    "def encode_wa(tokenized_input, tokenized_target, wa, wa_type):\n",
    "    wa_dict = {int(src): int(trg) for src, trg in map(lambda x: x.split('-'), wa.split())}\n",
    "    n = len(tokenized_input)\n",
    "    m = len(tokenized_target)\n",
    "    if wa_type == 'trg-ids':\n",
    "        wa_emb = [0]*n\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] = tokenized_target[v]\n",
    "        return wa_emb\n",
    "    elif wa_type == 'sums':\n",
    "        wa_emb = deepcopy(tokenized_input)\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] += tokenized_target[v]\n",
    "        return wa_emb\n",
    "    \n",
    "    elif wa_type == 'mult':\n",
    "        wa_emb = deepcopy(tokenized_input)\n",
    "        for k, v in wa_dict.items():\n",
    "            if k >= n or v >= m:\n",
    "                break\n",
    "            wa_emb[k] *= tokenized_target[v]\n",
    "        return wa_emb\n",
    "\n",
    "def preprocess_function(dataset):\n",
    "    global source_lang, target_lang, pos_tags, wa_type\n",
    "    inputs = [prefix + d[source_lang] for d in dataset[\"translation\"]]\n",
    "    targets = [d[target_lang] for d in dataset[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    if pos_tags:\n",
    "        model_inputs['pos'] = [get_pos_tags(x, 'en') for x in model_inputs['input_ids']]\n",
    "        model_inputs['target_pos'] = [get_pos_tags(y, 'fr') for y in model_inputs['labels']]\n",
    "        \n",
    "    if wa_type:\n",
    "        model_inputs['wa'] = [encode_wa(src, trg, wa, wa_type) for src, trg, wa \\\n",
    "                              in zip(model_inputs['input_ids'],  model_inputs['labels'], dataset[\"wa\"])]\n",
    "    return model_inputs\n",
    "\n",
    "split_dataset = loaded_dataset.remove_columns(['wa'])\n",
    "\n",
    "pos_anno_dataset = split_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "model_with_pos = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "data_collator_pos = DataCollatorForSeq2Seq(tokenizer, model=model_with_pos, return_tensors=\"tf\")\n",
    "generation_data_collator_pos = DataCollatorForSeq2Seq(tokenizer, model=model_with_pos, return_tensors=\"tf\", pad_to_multiple_of=128)\n",
    "\n",
    "train_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator_pos,\n",
    ")\n",
    "\n",
    "validation_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator_pos,\n",
    ")\n",
    "\n",
    "generation_dataset = model_with_pos.prepare_tf_dataset(\n",
    "    pos_anno_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator_pos,\n",
    ")\n",
    "\n",
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model_with_pos.compile(optimizer=optimizer)\n",
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True, \n",
    "    generate_kwargs={\"max_length\": 128}\n",
    ")\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./translation_model_save/logs\")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback]\n",
    "\n",
    "model_with_pos.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>sentences</th>\n",
       "      <th>initial_model</th>\n",
       "      <th>ft_no_anno</th>\n",
       "      <th>ft_wa_trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>In Chinese painting, abstraction can be traced...</td>\n",
       "      <td>Dans la peinture chinoise, l'abstraction peut ...</td>\n",
       "      <td>Dans la peinture chinoise l'abstraction peut ê...</td>\n",
       "      <td>Dans la peinture chinoise l'abstraction peut ê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>While none of his paintings remain, this style...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne subsiste, c...</td>\n",
       "      <td>Bien qu'aucune de ses peintures ne reste, ce s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>The Chan buddhist painter Liang Kai (??, c. 11...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "      <td>Le peintre bouddhiste Chan Liang Kai (??, vers...</td>\n",
       "      <td>Le peintre bouddhiste de Chan Liang Kai (?? ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract art</td>\n",
       "      <td>A late Song painter named Yu Jian, adept to Ti...</td>\n",
       "      <td>Un peintre de feu Song nommé Yu Jian, adepte d...</td>\n",
       "      <td>Un peintre de feu Song nommé Yu Jian, adepte d...</td>\n",
       "      <td>Un peintre de feu Song nommé Yu Jian, adepte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>When Turing was 39 years old in 1951, he turne...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951, il se tourn...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951 il se tourna...</td>\n",
       "      <td>Quand Turing avait 39 ans en 1951 il se tourna...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article                                          sentences  \\\n",
       "0  Abstract art  In Chinese painting, abstraction can be traced...   \n",
       "1  Abstract art  While none of his paintings remain, this style...   \n",
       "2  Abstract art  The Chan buddhist painter Liang Kai (??, c. 11...   \n",
       "3  Abstract art  A late Song painter named Yu Jian, adept to Ti...   \n",
       "4   Alan Turing  When Turing was 39 years old in 1951, he turne...   \n",
       "\n",
       "                                       initial_model  \\\n",
       "0  Dans la peinture chinoise, l'abstraction peut ...   \n",
       "1  Bien qu'aucune de ses peintures ne reste, ce s...   \n",
       "2  Le peintre bouddhiste Chan Liang Kai (??, vers...   \n",
       "3  Un peintre de feu Song nommé Yu Jian, adepte d...   \n",
       "4  Quand Turing avait 39 ans en 1951, il se tourn...   \n",
       "\n",
       "                                          ft_no_anno  \\\n",
       "0  Dans la peinture chinoise l'abstraction peut ê...   \n",
       "1  Bien qu'aucune de ses peintures ne subsiste, c...   \n",
       "2  Le peintre bouddhiste Chan Liang Kai (??, vers...   \n",
       "3  Un peintre de feu Song nommé Yu Jian, adepte d...   \n",
       "4  Quand Turing avait 39 ans en 1951 il se tourna...   \n",
       "\n",
       "                                           ft_wa_trg  \n",
       "0  Dans la peinture chinoise l'abstraction peut ê...  \n",
       "1  Bien qu'aucune de ses peintures ne reste, ce s...  \n",
       "2  Le peintre bouddhiste de Chan Liang Kai (?? ve...  \n",
       "3  Un peintre de feu Song nommé Yu Jian, adepte d...  \n",
       "4  Quand Turing avait 39 ans en 1951 il se tourna...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital = pd.read_csv(\"translations_ft_wa.csv\")\n",
    "vital.head()\n",
    "# Check that the dataframe opens correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans la peinture chinoise, l'abstraction peut être tracée par le peintre de la dynastie Tang Wang Mo (???), qui est crédité d'avoir inventé le style de peinture à jet d'éclaboussures.\n",
      "Bien qu'aucune de ses peintures ne reste, ce style est clairement vu dans certains Song Dynasty Paintings.....................................................................................................\n",
      "Le peintre bouddhiste Chan Liang Kai (??, vers 1140=1210) applique le style à la peinture figurative dans son « Immortal in splashed enk » dans lequel une représentation précise est sacrifiée pour améliorer la spontanéité liée à l'esprit non rationnel de l'éclairé.\n",
      "Un peintre de feu Song nommé Yu Jian, adepte du Bouddhisme de Tiantai, a créé une série de paysages à l'encre éclaboussée qui ont finalement inspiré de nombreux peintres japonais Zen.\n",
      "Quand Turing avait 39 ans en 1951 il se tourna vers la biologie mathématique et publia finalement son chef-d'œuvre « The Chemical Basis of Morphogenèse » en janvier 1952.\n",
      "Il s'intéressait à la morphogenèse, au développement de motifs et de formes dans les organismes biologiques......................................................................................................\n",
      "Il suggère qu'un système de produits chimiques réagissant l'un avec l'autre et diffusant dans l'espace, appelé système de diffusion de réaction, pourrait expliquer « les principaux phénomènes de morphogenèse ».\n",
      "Il utilise des systèmes d'équations différentielles partielles pour modéliser les réactions chimiques catalytiques.\n",
      "Par exemple, si un catalyseur A est nécessaire pour qu'une certaine réaction chimique ait lieu et si la réaction produite est plus du catalyseur A, alors nous disons que la réaction est autocatalytique et qu'il y a un retour positif qui peut être modélisé par des équations différentielles non linéaires.............................................................\n",
      "Turing a découvert que des patrons pouvaient être créés si la réaction chimique ne produisait pas seulement le catalyseur A mais aussi un inhibiteur B qui ralentissait la production de A.\n",
      "Si A et B ont ensuite diffusé à travers le conteneur à des vitesses différentes, alors vous pourriez avoir certaines régions où A a dominé et certaines où B l'a fait............................................................................................\n",
      "Pour calculer l'étendue de cela, Turing aurait eu besoin d'un ordinateur puissant, mais ceux-ci n'étaient pas si librement disponibles en 1951, il a donc dû utiliser des approximations linéaires pour résoudre les équations à la main........................................................................\n",
      "Ces calculs ont donné les bons résultats qualitatifs et ont produit, par exemple, un mélange uniforme qui a été assez étrangement espacé régulièrement des taches rouges fixes.............................................................................................\n",
      "Le biochimiste russe Boris Belousov avait réalisé des expériences avec des résultats similaires mais ne pouvait pas faire publier ses articles à cause du préjugé contemporain selon lequel une telle chose violait la deuxième loi de la thermodynamique.\n",
      "Belousov n'était pas au courant du journal de Turing dans les Transactions Philosophiques de la Royal Society (en anglais seulement)...............................................................................................\n",
      "Bien que publié avant que la structure et le rôle de l'ADN n'aient été compris, le travail de Turing sur la morphogenèse reste pertinent aujourd'hui et est considéré comme un travail séminal en biologie mathématique..............................................................................\n",
      "L'une des premières applications du papier de Turing est le travail de James Murray expliquant les taches et les rayures sur la fourrure des chats, grands et petits.\n",
      "D'autres recherches dans ce domaine suggèrent que les travaux de Turing peuvent expliquer en partie la croissance des « plumes, des follicules pileux, le modèle de ramification des poumons et même l'asymétrie gauche-droite qui place le cœur sur le côté gauche de la poitrine ».\n",
      "En 2012, Sheth et al.\n",
      "Chez la souris, l'élimination des gènes Hox provoque une augmentation du nombre de chiffres sans augmentation de la taille globale du membre, ce qui suggère que les gènes Hox contrôlent la formation de chiffres en réglant la longueur d'onde d'un mécanisme de type Turing.\n",
      "Plus tard les articles n'ont pas été disponibles avant que Collected Works of A. M. Turing n'ait été publié en 1992.\n",
      "Au sein du judaïsme humaniste, Talmud est étudié comme un texte historique afin de découvrir comment il peut démontrer sa pertinence pratique pour la vie d'aujourd'hui............................................................................................\n",
      "Une cérémonie religieuse pratiquée au Gabon et au Cameroun est l'Okuyi, pratiquée par plusieurs groupes ethniques bantous.....................................................................................................\n",
      "Dans cet état, selon la région, les rythmes de tambour ou instrumentaux joués par des musiciens respectés (chacun étant unique à une divinité ou à un ancêtre donné), les participants incarnent une divinité ou un ancêtre, une énergie ou un état d'esprit en effectuant des mouvements rituels distincts ou des danses qui renforcent encore leur conscience élevée.\n",
      "Lorsque cet état de transe est vu et compris, les adhérents ont accès à une façon de contempler l'incarnation pure ou symbolique d'un état d'esprit ou d'un cadre de référence particulier....................................................................................\n",
      "Cela crée des compétences pour séparer les sentiments suscités par cet état d'esprit de leurs manifestations situationnelles dans la vie de tous les jours..................................................................................................\n",
      "Une telle séparation et la contemplation ultérieure de la nature et des sources d'énergie pure ou de sentiments aide les participants à les gérer et à les accepter lorsqu'ils se présentent dans des contextes mondains...................................................................................\n",
      "En linguistique, la phonétique articulaire est l'étude de la façon dont la langue, les lèvres, la mâchoire, les cordes vocales et d'autres organes de la parole sont utilisés pour faire des sons................................................................................\n",
      "Les sons de la parole sont classés par mode d'articulation et lieu d'articulation.\n",
      "Le lieu d'articulation désigne l'endroit où dans le cou ou la bouche le courant d'air est constrictionné....................................................................................................\n",
      "Le mode d'articulation désigne la manière dont les organes de parole interagissent, comme la manière dont l'air est restreint et quelle forme de courant d'air est utilisée (p. ex.\n",
      "pulmonique, implosif, éjectifs et clics), que les cordes vocales vibrent ou non et que la cavité nasale soit ouverte au courant d'air......................................................................................\n",
      "Le concept est principalement utilisé pour la production de consonnes mais peut être utilisé pour les voyelles dans des qualités telles que la vocation et la nasalisation.\n",
      "Pour n'importe quel lieu d'articulation il peut y avoir plusieurs manières d'articuler et donc plusieurs consonnes homorganiques................................................................................................\n",
      "Le langage humain normal est pulmonique, produit avec la pression des poumons, ce qui crée la phonation dans les glattis dans le larynx qui est ensuite modifié par le chant et la bouche en différentes voyelles et consonnes........................................................................\n",
      "Cependant les humains peuvent prononcer des mots sans utiliser les poumons et les glattis dans le discours alaryngéal, dont il existe trois types : le discours ésophageal, le discours pharyngéal et le discours buccal (plus connu sous le nom de Donald Duck).\n",
      "La production de discours est une activité complexe et par conséquent les erreurs sont fréquentes, en particulier chez les enfants et les adolescents en particulier....................................................................................................\n"
     ]
    }
   ],
   "source": [
    "outputs_pos = []\n",
    "\n",
    "for input_sentence in vital[\"sentences\"]:\n",
    "    tokenized_sentence = tokenizer([input_sentence], return_tensors='np')\n",
    "    out = model_with_pos.generate(**tokenized_sentence, max_length=128)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        output_sentence = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        print(output_sentence)\n",
    "        outputs_pos.append(output_sentence)\n",
    "\n",
    "vital[\"ft_pos\"] = outputs_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital.to_csv(\"translations_ft_wa_pos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc73298858d0756eee327543f37df2c3feab8645937b6dca4052bb1287dcab38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
